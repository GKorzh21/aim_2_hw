{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дорогой дневник"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Сначала я решил попробовать без генерации новых фичей и без подкрутки параметров запустить LightGBM и посмотреть, что будет. Скор 0.720\n",
    "При этом пока еще нет ни генерации новых признаков, ни использования таблицы поиска и тд. Попробуем добавить.\n",
    "\n",
    "2) Просто запускаем код из \"baseline_1_pandas.ipynb\" и получаем обещаный скор 0.817\n",
    "\n",
    "3) Оптюнил 10 минут, безрезультатно. Буду придумывать новые признаки. Думаю начать с кластеризации и knn. \n",
    "\n",
    "4) Сначала решил просто расширить плавающее окно с 4 до 5 месяцев, результат 0.8192.\n",
    "\n",
    "    Есть огромное количество идей, только что заменил пандас на поларс, потому что он реально на порядок быстрее. В первую очередь хочется применить знания с семинара по интерпретации бустингов, но сначала заменить катбуст на lgbm, ибо Илья утверждал, что при должном обращении он рвет и мечет.\n",
    "\n",
    "5) Провел маштабное расследование в данных. Сначала просто обучил lgbm со скором примерно 0.817 и применил на нам всю информацию из семинара про интерпретацию бустингов. Пока что не совсем понятно как это оформиать и как интерпретировать некотрые вещи, но что точно ясно это то, что модель досточно устойчива и надежна, она не реазирует на теневые фичи, а это значит что местами можно делать бред не боясь за последствия. Например сделать полиномиальные фичи - всео со всеми. \n",
    "\n",
    "6) Это я и сделал, вмете с новыми фиками добавил количество поисков на последний месяц и неделю. Паралельно с этим убрад один выбрас user_id = 2346229. У человека были абсолютно неадекватные значения. Есть еще большое количество таких людей, но у них значения не такие радикальные и я боюсь что они могут оказаться просто очень активными пользователями, выкидывать их значит лишать себя бесценной информации. После добавления пачуи полиномиальных фичей скор стал 0.8210. Дальше планирую применить кластеризацию на поиске и заказах. Мне кажется очень логичным, что если человек совершал покупки одной категории, то он вероятнее купит еще, чем другой человек с та4ким же числом покупок, но разных категорий. \n",
    "\n",
    "7) Кластеризация. Очень долго ей занимался, сейчас подведу итоги всех трех дней. Главная идея такая - модель уже использует агригаты, так что для того чтобы повысить качество нужно использовать какую то иную информацию. Первое что проходит на ум - запросы человека. Банально провести кластеризацию на запросах и потом построить на этих метках новые агрегаты. Но есть проблема, у нас 77кк строк запросов и даже если написать нормальный алгоритм это будет работать 19 часов. Помимо того на сэмпле и понижении размерности через PCA выяснилось, что данные это полосы, крупные классы запиывают мелкие и те забиваются одной неразделимой кучей в углу, не получается чегото адекватного даже если использовать гауссовскую кластеризациюд или GMM. 300к обьектов обрабатывались несколько минут и я решил сделать кластеризацию полуавтоматически. Я создал столбец 'cluster' и заполнил его -1. Построив кластеризацию на 10_000 обьектах выделил некоторые классы, они ясно выделаются, например 20-30 обьектов с словом \"вода\" в названии, и заполнил их метки руками. Потом снова построил кластеризацию на оставшихся. Леноточная структура осталась, но обьекты былит уже другими. за каждую такую итерацию отсеивалось от 3кк до 10кк обьектов. Итого у меня определилось 18 кластеров - напитки, питомцы, мясо и многое другое. В итоге еще осталось 23кк обьектов, но там очень сложно выделить группы. В принципе я могу свести их до 10кк, но вопрос есть ли в этом смысл. После таких не простых манипуляций мы получаем метки, но это просто инструмент. То что реально даст прирост качества это агрегаты. Не просто было придумать их и не все оказались удачными. Я добавил последний доминирующий кластер, концентрация интересов, переключения между кластерами, стабильность кластеров, время в основном кластере, среднее время между сменами кластеров, средняя длина запроса, разница длины запросов. В итоге скор получился 0.8221."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost\n",
    "import os\n",
    "from datetime import date, timedelta\n",
    "from itertools import combinations\n",
    "\n",
    "from local_utils import *\n",
    "import lightgbm as lgb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_date = date(2024, 8, 1)\n",
    "val_start_date = date(2024, 7, 1)\n",
    "val_end_date = date(2024, 7, 31)\n",
    "train_end_date = date(2024, 6, 30)\n",
    "data_path = \"C:\\\\Users\\\\Admin\\\\Desktop\\\\AIM 2сем\\\\ML2\\\\hw2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_history = pl.scan_parquet(os.path.join(data_path, 'actions_history/*.parquet')).collect()\n",
    "search_history = pl.scan_parquet(os.path.join(data_path, 'search_history/*.parquet')).collect()\n",
    "product_information = pl.read_csv(\n",
    "    os.path.join(data_path, 'product_information.csv'),\n",
    "    ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_target = (\n",
    "    actions_history\n",
    "    .filter(pl.col('timestamp').dt.date() >= val_start_date)\n",
    "    .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "    .select('user_id', (pl.col('action_type_id') == 3).alias('has_order'))\n",
    "    .group_by('user_id')\n",
    "    .agg(pl.max('has_order').cast(pl.Int32).alias('target'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>target</th><th>user_id</th></tr><tr><td>i32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>1227381</td></tr><tr><td>1</td><td>647575</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌────────┬─────────┐\n",
       "│ target ┆ user_id │\n",
       "│ ---    ┆ ---     │\n",
       "│ i32    ┆ u32     │\n",
       "╞════════╪═════════╡\n",
       "│ 0      ┆ 1227381 │\n",
       "│ 1      ┆ 647575  │\n",
       "└────────┴─────────┘"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_target.group_by('target').agg(pl.count('user_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polynomial_features_pd(df, features, degree=2):\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    for feat1, feat2 in combinations(features, 2):\n",
    "        result_df[f'{feat1}_x_{feat2}'] = result_df[feat1] * result_df[feat2]\n",
    "        \n",
    "        if degree > 2:\n",
    "            result_df[f'{feat1}^2_x_{feat2}'] = result_df[feat1]**2 * result_df[feat2]\n",
    "            result_df[f'{feat1}_x_{feat2}^2'] = result_df[feat1] * result_df[feat2]**2\n",
    "    \n",
    "    for feat in features:\n",
    "        result_df[f'{feat}^2'] = result_df[feat]**2\n",
    "        if degree > 2:\n",
    "            result_df[f'{feat}^3'] = result_df[feat]**3\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(df, val_size=0.33, random_state = 42):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    train_idx = np.random.choice(df.index, size=int(df.shape[0]*(1-val_size)), replace=False)\n",
    "    val_idx = np.setdiff1d(df.index, train_idx)\n",
    "    return df.loc[train_idx].reset_index(drop=True), df.loc[val_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(tr, val, features, target_col, params=None, shadow_features=False, sklearn_style=False):\n",
    "\n",
    "    tr_shadow, val_shadow = pd.DataFrame(), pd.DataFrame()\n",
    "    if shadow_features:\n",
    "        tr_shadow, val_shadow = get_shadow_features(tr, val)\n",
    "\n",
    "    X_tr = pd.concat([tr[features], tr_shadow], axis=1, sort=False)\n",
    "    X_val = pd.concat([val[features], val_shadow], axis=1, sort=False)\n",
    "    tr_lgb = lgb.Dataset(X_tr, tr[target_col])\n",
    "    val_lgb = lgb.Dataset(X_val, val[target_col])\n",
    "\n",
    "    params_ = {\n",
    "        'nthread': 16,\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': 0.01,\n",
    "        'metric': 'auc',\n",
    "        'verbose': -1\n",
    "    }\n",
    "    if params is not None:\n",
    "        params_.update(params)\n",
    "\n",
    "    if not sklearn_style:\n",
    "        model = lgb.train(params_, tr_lgb, num_boost_round=300, valid_sets=[val_lgb], callbacks=[lgb.early_stopping(100)])\n",
    "    else:\n",
    "        model = lgb.LGBMClassifier(**params, n_estimators=300)\n",
    "        model.fit(X_tr, tr[target_col], eval_set=[(X_val, val[target_col])], callbacks=[lgb.early_stopping(100)])\n",
    "    \n",
    "    if shadow_features:\n",
    "        return model, tr_shadow.columns, X_tr, X_val\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lgbm_importance(model, features, importance_type='split', top_k=20, sklearn_style=False, imps=None, round_to=0):\n",
    "    if sklearn_style and imps is None:\n",
    "        imps = model.feature_importances_\n",
    "    elif imps is None:\n",
    "        imps = model.feature_importance(importance_type=importance_type)\n",
    "        \n",
    "    idx = np.argsort(imps)\n",
    "    sorted_imps = imps[idx][::-1][:top_k][::-1]\n",
    "    sorted_features = np.array(features)[idx][::-1][:top_k][::-1]\n",
    "    if round_to == 0:\n",
    "        sorted_imps = sorted_imps.astype(int)\n",
    "    else:\n",
    "        sorted_imps = np.round(sorted_imps, round_to)\n",
    "        \n",
    "    bar_container = plt.barh(width=sorted_imps, y=sorted_features)\n",
    "    plt.bar_label(bar_container, sorted_imps, color='red')\n",
    "    plt.gcf().set_size_inches(5, top_k/6 + 1)\n",
    "    plt.xlabel(importance_type, fontsize=15)\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_aggs = {}\n",
    "actions_id_to_suf = {\n",
    "    1: \"click\",\n",
    "    2: \"favorite\", \n",
    "    3: \"order\",\n",
    "    5: \"to_cart\",\n",
    "}\n",
    "\n",
    "all_aggs = []\n",
    "numeric_features = []\n",
    "\n",
    "for id_, suf in actions_id_to_suf.items():\n",
    "    aggs = (\n",
    "        actions_history\n",
    "        .filter(pl.col('timestamp').dt.date() <= train_end_date)\n",
    "        .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30 * 4))\n",
    "        .filter(pl.col('action_type_id') == id_)\n",
    "        .join(\n",
    "            product_information.select('product_id', 'discount_price'),\n",
    "            on='product_id',\n",
    "        )\n",
    "        .group_by('user_id')\n",
    "        .agg(\n",
    "            pl.count('product_id').cast(pl.Int32).alias(f'num_products_{suf}'),\n",
    "            pl.sum('discount_price').cast(pl.Float32).alias(f'sum_discount_price_{suf}'),\n",
    "            pl.max('discount_price').cast(pl.Float32).alias(f'max_discount_price_{suf}'),\n",
    "            pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "            pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "        )\n",
    "        .with_columns([\n",
    "            (pl.lit(val_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "            \n",
    "            (pl.lit(val_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    numeric_features.extend([\n",
    "        f'num_products_{suf}',\n",
    "        f'sum_discount_price_{suf}', \n",
    "        f'max_discount_price_{suf}',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "    ])\n",
    "    \n",
    "    actions_aggs[id_] = aggs\n",
    "    all_aggs.append(aggs)\n",
    "\n",
    "combined = all_aggs[0]\n",
    "for i, agg in enumerate(all_aggs[1:], 1):\n",
    "    combined = combined.join(\n",
    "        agg, \n",
    "        on='user_id', \n",
    "        how='left',\n",
    "        suffix=f\"_{i}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = 4\n",
    "suf = 'search'\n",
    "\n",
    "actions_aggs[id_] = (\n",
    "    search_history\n",
    "    .filter(pl.col('action_type_id') == id_)\n",
    "    .filter(pl.col('timestamp').dt.date() <= train_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30 * 5))\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        # Общее количество поисков за 5 месяцев\n",
    "        pl.count('search_query').cast(pl.Int32).alias(f'num_{suf}'),\n",
    "        pl.col('search_query').n_unique().alias(f'unique_{suf}_queries'),\n",
    "        \n",
    "        # Количество поисков за последний месяц\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_month'),\n",
    "        \n",
    "        # Количество поисков за последнюю неделю\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=7))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_week'),\n",
    "\n",
    "        (pl.count() / (pl.max('timestamp') - pl.min('timestamp')).dt.total_days()).alias(f'{suf}_daily_rate'),\n",
    "\n",
    "\n",
    "        pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "        pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "    )\n",
    "    .with_columns([\n",
    "        (pl.lit(val_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "\n",
    "        (pl.lit(val_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "    ])\n",
    "    .select(\n",
    "        'user_id',\n",
    "        f'num_{suf}',\n",
    "        f'unique_{suf}_queries',\n",
    "        f'num_{suf}_last_month',\n",
    "        f'num_{suf}_last_week',\n",
    "        f'{suf}_daily_rate',\n",
    "        f'{suf}_mean_query_len',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "        f'last_{suf}_time',\n",
    "        f'first_{suf}_time',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = val_target\n",
    "for _, actions_aggs_df in actions_aggs.items():\n",
    "    df_main = (\n",
    "        df_main\n",
    "        .join(actions_aggs_df, on='user_id', how='left')\n",
    "    )\n",
    "    \n",
    "df_pd = df_main.to_pandas()\n",
    "\n",
    "num_cols = ['num_products_click', 'sum_discount_price_click', 'max_discount_price_click', 'days_since_last_click', 'days_since_first_click', \n",
    "    'num_products_favorite', 'sum_discount_price_favorite', 'max_discount_price_favorite', 'days_since_last_favorite', \n",
    "    'days_since_first_favorite', 'num_products_order', 'sum_discount_price_order', 'max_discount_price_order', 'days_since_last_order', \n",
    "    'days_since_first_order', 'num_products_to_cart', 'sum_discount_price_to_cart', 'max_discount_price_to_cart', 'days_since_last_to_cart', \n",
    "    'days_since_first_to_cart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = add_polynomial_features_pd(df_pd, num_cols, degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df_pd[df_pd['user_id'] != 2346229]\n",
    "\n",
    "tr, val = get_split(df_pd, val_size=0.3)\n",
    "\n",
    "cols = [\n",
    "    'num_products_click', \n",
    "    'sum_discount_price_click', 'max_discount_price_click',\n",
    "    'days_since_last_click', 'days_since_first_click',\n",
    "    'num_products_favorite', 'sum_discount_price_favorite',\n",
    "    'max_discount_price_favorite', 'days_since_last_favorite',\n",
    "    'days_since_first_favorite', 'num_products_order',\n",
    "    'sum_discount_price_order', 'max_discount_price_order',\n",
    "    'days_since_last_order', 'days_since_first_order',\n",
    "    'num_products_to_cart', 'sum_discount_price_to_cart',\n",
    "    'max_discount_price_to_cart', 'days_since_last_to_cart',\n",
    "    'days_since_first_to_cart', 'num_search', 'days_since_last_search',\n",
    "    'days_since_first_search', \n",
    "    \n",
    "    'unique_search_queries',\n",
    "    'num_search_last_month',\n",
    "    'num_search_last_week',\n",
    "    'search_daily_rate',\n",
    "    'search_mean_query_len',\n",
    "]\n",
    "\n",
    "params={\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 100,\n",
    "        'num_iterations': 700,\n",
    "        'early_stopping_rounds': 60,\n",
    "        'verbose': 1,\n",
    "        'importance_type': 'split'\n",
    "    }\n",
    "\n",
    "model = train_model(tr, val, cols, 'target', params=params, shadow_features=False, sklearn_style=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('feature_importance:', model.feature_importances_, '\\n')\n",
    "\n",
    "plot_lgbm_importance(model, cols, importance_type='split', top_k=30, sklearn_style=True)\n",
    "\n",
    "test_users_submission = (\n",
    "    pl.read_csv(os.path.join(data_path, 'test_users.csv'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_aggs = {}\n",
    "actions_id_to_suf = {\n",
    "    1: \"click\",\n",
    "    2: \"favorite\", \n",
    "    3: \"order\",\n",
    "    5: \"to_cart\",\n",
    "}\n",
    "\n",
    "all_aggs = []\n",
    "numeric_features = []\n",
    "\n",
    "for id_, suf in actions_id_to_suf.items():\n",
    "    aggs = (\n",
    "        actions_history\n",
    "        .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "        .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30 * 5))\n",
    "        .filter(pl.col('action_type_id') == id_)\n",
    "        .join(\n",
    "            product_information.select('product_id', 'discount_price'),\n",
    "            on='product_id',\n",
    "        )\n",
    "        .group_by('user_id')\n",
    "        .agg(\n",
    "            pl.count('product_id').cast(pl.Int32).alias(f'num_products_{suf}'),\n",
    "            pl.sum('discount_price').cast(pl.Float32).alias(f'sum_discount_price_{suf}'),\n",
    "            pl.max('discount_price').cast(pl.Float32).alias(f'max_discount_price_{suf}'),\n",
    "            pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "            pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "        )\n",
    "        .with_columns([\n",
    "            (pl.lit(test_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "            \n",
    "            (pl.lit(test_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    numeric_features.extend([\n",
    "        f'num_products_{suf}',\n",
    "        f'sum_discount_price_{suf}', \n",
    "        f'max_discount_price_{suf}',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "    ])\n",
    "    \n",
    "    actions_aggs[id_] = aggs\n",
    "    all_aggs.append(aggs)\n",
    "\n",
    "combined_val = all_aggs[0]\n",
    "for i, agg in enumerate(all_aggs[1:], 1):\n",
    "    combined_val = combined_val.join(\n",
    "        agg, \n",
    "        on='user_id', \n",
    "        how='outer',\n",
    "        suffix=f\"_{i}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = 4\n",
    "suf = 'search'\n",
    "\n",
    "actions_aggs[id_] = (\n",
    "    search_history\n",
    "    .filter(pl.col('action_type_id') == id_)\n",
    "    .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30 * 5))\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        # Общее количество поисков за 5 месяцев\n",
    "        pl.count('search_query').cast(pl.Int32).alias(f'num_{suf}'),\n",
    "        pl.col('search_query').n_unique().alias(f'unique_{suf}_queries'),\n",
    "        \n",
    "        # Количество поисков за последний месяц\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_month'),\n",
    "        \n",
    "        # Количество поисков за последнюю неделю\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=7))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_week'),\n",
    "\n",
    "        (pl.count() / (pl.max('timestamp') - pl.min('timestamp')).dt.total_days()).alias(f'{suf}_daily_rate'),\n",
    "\n",
    "        pl.col('search_query').str.len_chars().mean().alias(f'{suf}_mean_query_len'),\n",
    "        \n",
    "\n",
    "        pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "        pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "    )\n",
    "    .with_columns([\n",
    "        (pl.lit(test_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "\n",
    "        (pl.lit(test_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "    ])\n",
    "    .select(\n",
    "        'user_id',\n",
    "        f'num_{suf}',\n",
    "        f'unique_{suf}_queries',\n",
    "        f'num_{suf}_last_month',\n",
    "        f'num_{suf}_last_week',\n",
    "        f'{suf}_daily_rate',\n",
    "        f'{suf}_mean_query_len',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "        f'last_{suf}_time',\n",
    "        f'first_{suf}_time',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = test_users_submission\n",
    "for _, actions_aggs_df in actions_aggs.items():\n",
    "    df_main = (\n",
    "        df_main\n",
    "        .join(actions_aggs_df, on='user_id', how='left')\n",
    "    )\n",
    "    \n",
    "df_pd = df_main.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_utils import *\n",
    "\n",
    "df_pd = add_polynomial_features_pd(df_pd, num_cols, degree=2)\n",
    "\n",
    "df_pd['predict'] = model.predict_proba(df_pd[cols])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd[['user_id', 'predict']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd[['user_id', 'predict']].to_csv('new_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
