{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дорогой дневник"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Сначала я решил попробовать без генерации новых фичей и без подкрутки параметров запустить LightGBM и посмотреть, что будет. Скор 0.720\n",
    "При этом пока еще нет ни генерации новых признаков, ни использования таблицы поиска и тд. Попробуем добавить.\n",
    "\n",
    "2) Просто запускаем код из \"baseline_1_pandas.ipynb\" и получаем обещаный скор 0.817\n",
    "\n",
    "3) Оптюнил 10 минут, безрезультатно. Буду придумывать новые признаки. Думаю начать с кластеризации и knn. \n",
    "\n",
    "4) Сначала решил просто расширить плавающее окно с 4 до 5 месяцев, результат 0.8192.\n",
    "\n",
    "    Есть огромное количество идей, только что заменил пандас на поларс, потому что он реально на порядок быстрее. В первую очередь хочется применить знания с семинара по интерпретации бустингов, но сначала заменить катбуст на lgbm, ибо Илья утверждал, что при должном обращении он рвет и мечет.\n",
    "\n",
    "5) Провел маштабное расследование в данных. Сначала просто обучил lgbm со скором примерно 0.817 и применил на нам всю информацию из семинара про интерпретацию бустингов. Пока что не совсем понятно как это оформиать и как интерпретировать некотрые вещи, но что точно ясно это то, что модель досточно устойчива и надежна, она не реазирует на теневые фичи, а это значит что местами можно делать бред не боясь за последствия. Например сделать полиномиальные фичи - всео со всеми. \n",
    "\n",
    "6) Это я и сделал, вмете с новыми фиками добавил количество поисков на последний месяц и неделю. Паралельно с этим убрад один выбрас user_id = 2346229. У человека были абсолютно неадекватные значения. Есть еще большое количество таких людей, но у них значения не такие радикальные и я боюсь что они могут оказаться просто очень активными пользователями, выкидывать их значит лишать себя бесценной информации. После добавления пачуи полиномиальных фичей скор стал 0.8210. Дальше планирую применить кластеризацию на поиске и заказах. Мне кажется очень логичным, что если человек совершал покупки одной категории, то он вероятнее купит еще, чем другой человек с та4ким же числом покупок, но разных категорий. \n",
    "\n",
    "7) Кластеризация. Очень долго ей занимался, сейчас подведу итоги всех трех дней. Главная идея такая - модель уже использует агригаты, так что для того чтобы повысить качество нужно использовать какую то иную информацию. Первое что проходит на ум - запросы человека. Банально провести кластеризацию на запросах и потом построить на этих метках новые агрегаты. Но есть проблема, у нас 77кк строк запросов и даже если написать нормальный алгоритм это будет работать 19 часов. Помимо того на сэмпле и понижении размерности через PCA выяснилось, что данные это полосы, крупные классы запиывают мелкие и те забиваются одной неразделимой кучей в углу, не получается чегото адекватного даже если использовать гауссовскую кластеризациюд или GMM. 300к обьектов обрабатывались несколько минут и я решил сделать кластеризацию полуавтоматически. Я создал столбец 'cluster' и заполнил его -1. Построив кластеризацию на 10_000 обьектах выделил некоторые классы, они ясно выделаются, например 20-30 обьектов с словом \"вода\" в названии, и заполнил их метки руками. Потом снова построил кластеризацию на оставшихся. Леноточная структура осталась, но обьекты былит уже другими. за каждую такую итерацию отсеивалось от 3кк до 10кк обьектов. Итого у меня определилось 18 кластеров - напитки, питомцы, мясо и многое другое. В итоге еще осталось 23кк обьектов, но там очень сложно выделить группы. В принципе я могу свести их до 10кк, но вопрос есть ли в этом смысл. После таких не простых манипуляций мы получаем метки, но это просто инструмент. То что реально даст прирост качества это агрегаты. Не просто было придумать их и не все оказались удачными. Я добавил последний доминирующий кластер, концентрация интересов, переключения между кластерами, стабильность кластеров, время в основном кластере, среднее время между сменами кластеров, средняя длина запроса, разница длины запросов. В итоге скор получился 0.8221."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost\n",
    "import os\n",
    "from datetime import date, timedelta\n",
    "from itertools import combinations\n",
    "\n",
    "from local_utils import *\n",
    "import lightgbm as lgb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "test_start_date = date(2024, 8, 1)\n",
    "val_start_date = date(2024, 7, 1)\n",
    "val_end_date = date(2024, 7, 31)\n",
    "train_end_date = date(2024, 6, 30)\n",
    "data_path = \"C:\\\\Users\\\\Admin\\\\Desktop\\\\AIM 2сем\\\\ML2\\\\hw2\"\n",
    "\n",
    "actions_history = pl.scan_parquet(os.path.join(data_path, 'actions_history/*.parquet')).collect()\n",
    "search_history = pl.scan_parquet(os.path.join(data_path, 'search_history/*.parquet')).collect()\n",
    "product_information = pl.read_csv(\n",
    "    os.path.join(data_path, 'product_information.csv'),\n",
    "    ignore_errors=True\n",
    ")\n",
    "\n",
    "val_target = (\n",
    "    actions_history\n",
    "    .filter(pl.col('timestamp').dt.date() >= val_start_date)\n",
    "    .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "    .select('user_id', (pl.col('action_type_id') == 3).alias('has_order'))\n",
    "    .group_by('user_id')\n",
    "    .agg(pl.max('has_order').cast(pl.Int32).alias('target'))\n",
    ")\n",
    "\n",
    "val_target.group_by('target').agg(pl.count('user_id'))\n",
    "\n",
    "sh_topd = search_history.to_pandas()\n",
    "sh_topd\n",
    "\n",
    "sh_topd = sh_topd.dropna(subset=['search_query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_topd['cluster'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milk = ['кефир', 'сметан', 'масло', 'йогурт', 'морожен', 'творог', 'яйц', 'молок', 'сливк', 'ряжен', 'простокваш', 'пахт', 'сгущенк', 'сыворот']\n",
    "\n",
    "sweet = ['конфет', 'мармелад', 'сироп', 'энергетик', 'кекс', 'сахар', 'шоколад', 'батончик', 'варен', 'печень', 'торт', 'чипс', 'лейс', 'lays', 'халв', \n",
    "         'зефир', 'пастил', 'пряник', 'леден', 'драже', 'карамел', 'джем', 'мед', 'вафл', 'хлебец', 'семечки']\n",
    "\n",
    "cereals = [\"греч\", \"рис\", \"овсян\", \"пшен\", \"перлов\", \"манк\", \"кукурузн\", \"ячнев\", \"булгур\", \"киноа\", \"макарон\", \"спагетт\", \"лапш\", \"вермишел\", \"фасол\", \n",
    "           \"горох\", \"нут\", \"чечевиц\", \"полб\", 'каша']\n",
    "\n",
    "veg_and_fruits = ['изюм', 'перец', 'огур', 'помидор', 'шампиньон', 'банан', 'фрукт', 'овощ', 'апельсин', \n",
    "                  'томат', 'свекл', 'баклажан', 'мандарин', 'ягод', 'гриб', 'лимон', 'персик', 'цукин', \n",
    "                  'яблок', 'груш', 'салат', 'черри', 'морков', 'патиссон', 'картош', 'картоф','капуст',\n",
    "                  'груша', 'персик', 'апельсин', 'мандарин', 'баклажан', 'лук', 'авокадо', 'клубника', 'зелен', 'кукуруз'\n",
    "]\n",
    "\n",
    "drink = ['квас', 'сок', 'вод', 'кофе', 'чай', 'лимонад', 'напитк', 'кола']\n",
    "\n",
    "bread = ['хлеб', 'булк', 'батон', 'багет', 'еда', 'мука', 'пицца', 'лаваш']\n",
    "\n",
    "cheese = ['сыр', 'брынз', 'моцарел']\n",
    "\n",
    "pets = ['корм', 'наполнит', 'лакомств', 'вискас', 'педигри', 'собак', 'кошек', 'кроликов']\n",
    "\n",
    "meat = ['рыб', 'сосис', 'колбас', 'куриц', 'филе', 'говядин', 'мяс', 'крабов', 'креветк', 'свинин', 'фарш', 'кальмар', 'ветчина', 'пельмени', 'бекон'\n",
    "        'индейка', 'котлет', 'суп', 'икр', 'груд', ]\n",
    "\n",
    "household = ['мыл', 'гел', 'паст', 'туалетн', 'крем', 'средств', 'краск', 'стакан', 'волос', 'шампун', 'кондиционер', 'салфетк', 'порошок', 'жидкост', \n",
    "             'капсул', 'бумаг', 'бумаж', 'полотен', 'проклад']\n",
    "\n",
    "kids = ['подгузник', 'памперс', 'дет', 'соск', 'пюре', ]\n",
    "\n",
    "clothing = [\n",
    "    'куртк', 'пальт', 'пуховик', 'ветровк', 'плащ', 'шуб', 'дубленк', 'зимн', 'демисезонн',\n",
    "    'футболк', 'рубашк', 'блузк', 'водолазк', 'толстовк', 'свитер', 'джемпер', 'кардиган', 'худи',\n",
    "    'кофт', 'боди', 'топ', 'майк',\n",
    "    'брюк', 'джинс', 'штан', 'штаны', 'леггинс', 'бридж', 'капри', 'юбк', 'сарафан', 'комбинезон',\n",
    "    'плать', 'сарафан', 'туник', 'сарафан',\n",
    "    'трусы', 'трусики', 'бюстгальтер', 'лифчик', 'белье', 'пижам', 'ночн', 'халат', 'комплект',\n",
    "    'спортивн', 'лосин', 'велосипедк', 'термобелье', 'олимпийк', 'тренировочн', 'спорткостюм',\n",
    "    'купальник', 'бикини', 'плавк', 'монокини', 'танкини', 'пляжн',\n",
    "    'шапк', 'кепк', 'шляп', 'панам', 'косынк', 'бандан', 'берет',\n",
    "    'туфл', 'кроссовк', 'ботинк', 'сапог', 'босоножк', 'сандал', 'угг', 'каблук', 'мокасин', 'лофер',\n",
    "    'черепик', 'кед', 'балетк', 'сабо', 'дутик',\n",
    "    'ремен', 'пояс', 'галстук', 'бабочк', 'шарф', 'перчатк', 'варежк', 'зонт', 'очк', 'солнцезащитн',\n",
    "    'сумк', 'рюкзак', 'клатч', 'кошельк', 'портфель',\n",
    "    'халат', 'униформа', 'спецовк', 'комбинезон', 'фартук',\n",
    "    'детск', 'малыш', 'подросток', 'распашонк', 'ползунк', 'пеленк',\n",
    "    'носок', 'гольф', 'чулк', 'колготк', 'подследник',\n",
    "    'аксессуар', 'декор', 'украшен', 'бижутери', 'стильн', 'модн', 'брендов'\n",
    "]\n",
    "\n",
    "health = [\n",
    "    # Органы и системы\n",
    "    'глаз', 'зрен', 'офтальм', 'зрит', 'сетчатк', 'роговиц',\n",
    "    'сердц', 'карди', 'гипертон', 'давлен', 'пульс', 'аритм',\n",
    "    'печен', 'гепат', 'желч', 'желуд', 'гастр', 'кишечн', 'панкреат',\n",
    "    'почк', 'нефр', 'мочевой', 'цистит', 'простат',\n",
    "    'легк', 'пневм', 'бронх', 'астм', 'кашел', 'насморк', 'горл', 'отит',\n",
    "    'зуб', 'стомат', 'десн', 'кариес', 'флюс',\n",
    "    'кож', 'дермат', 'акне', 'псориаз', 'экзем', 'аллерг',\n",
    "    'нерв', 'неврол', 'головн', 'мигрен', 'инсульт', 'эпилепс',\n",
    "    'сустав', 'артр', 'остеохондр', 'позвон', 'ревмат', 'артрит',\n",
    "    'щитовид', 'эндокрин', 'диабет', 'гормон', 'инсулин',\n",
    "    \n",
    "    # Препараты и формы выпуска\n",
    "    'таблетк', 'капсул', 'капл', 'маз', 'крем', 'гел', 'спрей', 'ингалятор',\n",
    "    'суспенз', 'сироп', 'настойк', 'экстракт', 'назол', 'отинум',\n",
    "    'антибиот', 'противовирусн', 'антигистаминн', 'анальгин', 'парацетамол',\n",
    "    'ибупрофен', 'спазмолгон', 'но-шп', 'корвалол', 'валидол',\n",
    "    'витамин', 'минерал', 'омега', 'магн', 'коллаген', 'пробиот',\n",
    "    \n",
    "    # Симптомы и состояния\n",
    "    'бол', 'воспален', 'отек', 'опухол', 'зуд', 'жжен', 'тошнот', 'рвот',\n",
    "    'диаре', 'запо', 'метеоризм', 'слабост', 'усталост', 'стресс', 'депресс',\n",
    "    'бессонн', 'тревожн', 'паническ', 'судорог', 'свист', 'хрип',\n",
    "    'кровоточивост', 'кров', 'гемоглоб', 'анеми', 'варикоз', 'геморро',\n",
    "]\n",
    "\n",
    "sauces = [\n",
    "    'майонез', 'кетчуп', 'горчиц', 'аджик', 'хрен', 'соус', 'тартар', 'бешамель', 'песто', 'цезар','барбекю', 'чили', 'терияк', 'сальса', 'гуакамоле',\n",
    "    'сметанн', 'чесночн', 'имбирн', 'устричн', 'рыбн','ткемал', 'кисло-сладк', 'бальзамик', 'вустершир', 'карри','сырн', 'шашлычн', 'гриль', 'паприк', 'чеддер',\n",
    "    'провансаль', 'брусничн', 'клюквен', 'мятн', 'укропн'\n",
    "]\n",
    "\n",
    "gadgets = [\n",
    "    'смартфон', 'iphone', 'андроид', 'телефон', 'кнопочный телефон', 'раскладушка',\n",
    "    'xiaomi', 'samsung', 'honor', 'huawei', 'nokia', 'realme', 'oppo', 'vivo',\n",
    "    'планшет', 'ipad', 'электронная книга', 'электронная читалка', 'букридер',\n",
    "    'ноутбук', 'ультрабук', 'macbook', 'asus', 'lenovo', 'hp', 'dell', 'acer', 'msi',\n",
    "    'компьютер', 'системный блок', 'моноблок', 'неттоп',\n",
    "    'клавиатура', 'мышь', 'мышка', 'коврик для мыши', 'трекпад', 'стилус', 'графический планшет',\n",
    "    'веб-камера', 'микрофон', 'док-станция',\n",
    "    'процессор', 'видеокарта', 'оперативная память', 'ssd', 'жесткий диск', 'hdd',\n",
    "    'материнская плата', 'блок питания', 'охлаждение', 'кулер', 'термопаста',\n",
    "    'наушники', 'airpods', 'гарнитура', 'беспроводные наушники', 'накладные наушники',\n",
    "    'вкладыши', 'вакуумные', 'колонка', 'динамик', 'bluetooth колонка', 'портативная колонка',\n",
    "    'микрофон', 'звуковая карта', 'dac', 'усилитель',\n",
    "    'умные часы', 'smart watch', 'фитнес-браслет', 'умный браслет', 'пульсометр',\n",
    "    'шагомер', 'gps трекер', 'умное кольцо',\n",
    "    'чехол', 'защитное стекло', 'плёночка', 'бампер', 'держатель', 'попсокет',\n",
    "    'кардхолдер', 'кабель', 'зарядка', 'адаптер', 'power bank', 'повербанк',\n",
    "    'кабель usb', 'кабель type-c', 'lightning', 'переходник', 'разветвитель',\n",
    "    'подставка', 'держатель для телефона', 'автодержатель',\n",
    "    'игровая мышь', 'игровая клавиатура', 'джойстик', 'геймпад', 'руль', 'педали',\n",
    "    'аркадный стик', 'vr', 'виртуальная реальность', 'очки vr', 'гарнитура vr',\n",
    "    'фотоаппарат', 'зеркалка', 'беззеркалка', 'экшн-камера', 'go pro', 'объектив',\n",
    "    'штатив', 'монопод', 'стабилизатор', 'гимбал', 'квадрокоптер', 'дрон',\n",
    "    'умная лампа', 'умная розетка', 'умный дом', 'датчик', 'сенсор', 'камера наблюдения',\n",
    "    'видеоняня', 'умный замок', 'робот-пылесос', 'умный термостат',\n",
    "    'принтер', 'сканер', 'ксерокс', 'мфу', 'факс', 'ламинтор', 'брошюровщик',\n",
    "    'уничтожитель бумаг', 'шредер',\n",
    "    'электроника', 'гаджет', 'новинка', 'аксессуар', 'комплектующая', 'устройство',\n",
    "    'беспроводной', 'bluetooth', 'wi-fi', 'usb', 'type-c', 'lightning', 'адаптер'\n",
    "]\n",
    "\n",
    "spices = [\n",
    "    'перец', 'соль', 'паприк', 'куркум', 'кориандр', 'кардамон', 'имбир', \n",
    "    'чеснок', 'лук', 'лавров', 'розмарин', 'тимьян', 'базилик', 'орегано', \n",
    "    'шалфей', 'мята', 'укроп', 'петрушк', 'кинз', 'тмин', 'зир', 'кориц', \n",
    "    'гвоздик', 'мускат', 'ванил', 'шафран', 'горчиц', 'хрен', 'карри', \n",
    "    'прованс', 'фенхел', 'анис', 'кумин', 'чили', 'бадьян', 'кайен', 'тархун'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(milk), case=False, na=False), 'cluster'] = 1\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(sweet), case=False, na=False), 'cluster'] = 2\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(cereals), case=False, na=False), 'cluster'] = 3\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(veg_and_fruits), case=False, na=False), 'cluster'] = 4\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(drink), case=False, na=False), 'cluster'] = 5\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(bread), case=False, na=False), 'cluster'] = 6\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(cheese), case=False, na=False), 'cluster'] = 7\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(pets), case=False, na=False), 'cluster'] = 8\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(meat), case=False, na=False), 'cluster'] = 9\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(household), case=False, na=False), 'cluster'] = 10\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(kids), case=False, na=False), 'cluster'] = 11\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(clothing), case=False, na=False), 'cluster'] = 12\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(health), case=False, na=False), 'cluster'] = 13\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(sauces), case=False, na=False), 'cluster'] = 14\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(gadgets), case=False, na=False), 'cluster'] = 15\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('для', case=False, na=False), 'cluster'] = 16\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('цвет', case=False, na=False), 'cluster'] = 17\n",
    "sh_topd.loc[sh_topd['search_query'].str.contains('|'.join(spices), case=False, na=False), 'cluster'] = 18\n",
    "\n",
    "\n",
    "remaining_negatives = (sh_topd['cluster'] == -1).sum()\n",
    "print(f\"Осталось строк с меткой -1: {remaining_negatives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_topd_filtered = sh_topd[sh_topd['cluster'] == -1].copy()\n",
    "cluster_search_queries(sh_topd_filtered, 'search_query', 100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost\n",
    "import os\n",
    "from datetime import date, timedelta\n",
    "from itertools import combinations\n",
    "\n",
    "from local_utils import *\n",
    "import lightgbm as lgb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_date = date(2024, 8, 1)\n",
    "val_start_date = date(2024, 7, 1)\n",
    "val_end_date = date(2024, 7, 31)\n",
    "train_end_date = date(2024, 6, 30)\n",
    "data_path = \"C:\\\\Users\\\\Admin\\\\Desktop\\\\AIM 2сем\\\\ML2\\\\hw2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_history = pl.scan_parquet(os.path.join(data_path, 'actions_history/*.parquet')).collect()\n",
    "search_history = pl.scan_parquet(os.path.join(data_path, 'search_history/*.parquet')).collect()\n",
    "product_information = pl.read_csv(\n",
    "    os.path.join(data_path, 'product_information.csv'),\n",
    "    ignore_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_target = (\n",
    "    actions_history\n",
    "    .filter(pl.col('timestamp').dt.date() >= val_start_date)\n",
    "    .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "    .select('user_id', (pl.col('action_type_id') == 3).alias('has_order'))\n",
    "    .group_by('user_id')\n",
    "    .agg(pl.max('has_order').cast(pl.Int32).alias('target'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>target</th><th>user_id</th></tr><tr><td>i32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>1227381</td></tr><tr><td>1</td><td>647575</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌────────┬─────────┐\n",
       "│ target ┆ user_id │\n",
       "│ ---    ┆ ---     │\n",
       "│ i32    ┆ u32     │\n",
       "╞════════╪═════════╡\n",
       "│ 0      ┆ 1227381 │\n",
       "│ 1      ┆ 647575  │\n",
       "└────────┴─────────┘"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_target.group_by('target').agg(pl.count('user_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_aggs = {}\n",
    "actions_id_to_suf = {\n",
    "    1: \"click\",\n",
    "    2: \"favorite\", \n",
    "    3: \"order\",\n",
    "    5: \"to_cart\",\n",
    "}\n",
    "\n",
    "# Сначала соберем все агрегированные данные\n",
    "all_aggs = []\n",
    "numeric_features = []\n",
    "\n",
    "for id_, suf in actions_id_to_suf.items():\n",
    "    aggs = (\n",
    "        actions_history\n",
    "        .filter(pl.col('timestamp').dt.date() <= train_end_date)\n",
    "        .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30 * 4))\n",
    "        .filter(pl.col('action_type_id') == id_)\n",
    "        .join(\n",
    "            product_information.select('product_id', 'discount_price'),\n",
    "            on='product_id',\n",
    "        )\n",
    "        .group_by('user_id')\n",
    "        .agg(\n",
    "            pl.count('product_id').cast(pl.Int32).alias(f'num_products_{suf}'),\n",
    "            pl.sum('discount_price').cast(pl.Float32).alias(f'sum_discount_price_{suf}'),\n",
    "            pl.max('discount_price').cast(pl.Float32).alias(f'max_discount_price_{suf}'),\n",
    "            pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "            pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "        )\n",
    "        .with_columns([\n",
    "            (pl.lit(val_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "            \n",
    "            (pl.lit(val_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Сохраняем имена числовых фичей для последующего создания полиномов\n",
    "    numeric_features.extend([\n",
    "        f'num_products_{suf}',\n",
    "        f'sum_discount_price_{suf}', \n",
    "        f'max_discount_price_{suf}',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "    ])\n",
    "    \n",
    "    actions_aggs[id_] = aggs\n",
    "    all_aggs.append(aggs)\n",
    "\n",
    "# Объединяем все агрегации по user_id с указанием суффиксов\n",
    "combined = all_aggs[0]\n",
    "for i, agg in enumerate(all_aggs[1:], 1):\n",
    "    combined = combined.join(\n",
    "        agg, \n",
    "        on='user_id', \n",
    "        how='left',\n",
    "        suffix=f\"_{i}\"  # Добавляем уникальный суффикс для каждого соединения\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 and (not isinstance(sh_topd, pl.DataFrame)):\n",
    "    sh_topd = pl.from_pandas(sh_topd)  # Конвертируем pandas -> polars\n",
    "\n",
    "    # Ключи для объединения\n",
    "    join_keys = ['user_id', 'timestamp', 'search_query']\n",
    "\n",
    "    # Выполняем left join\n",
    "    search_history = search_history.join(\n",
    "        sh_topd.select(join_keys + ['cluster']),  # Выбираем нужные колонки\n",
    "        on=join_keys,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "# search_aggs\n",
    "id_ = 4\n",
    "suf = 'search'\n",
    "\n",
    "# Сначала вычислим value_counts отдельно и развернем их в плоскую структуру\n",
    "cluster_counts = (\n",
    "    search_history\n",
    "    .filter(pl.col('action_type_id') == id_)\n",
    "    .filter(pl.col('timestamp').dt.date() <= train_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30 * 5))\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        pl.col('cluster').value_counts().alias('cluster_counts')\n",
    "    )\n",
    "    .explode('cluster_counts')\n",
    "    .with_columns(\n",
    "        pl.col('cluster_counts').struct.field('cluster').alias('cluster_name'),\n",
    "        pl.col('cluster_counts').struct.field('count').alias('cluster_count')\n",
    "    )\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        pl.col('cluster_name').sort_by('cluster_count', descending=True).head(3).alias('top3_clusters'),\n",
    "        pl.col('cluster_count').sort(descending=True).head(3).alias('top3_counts')\n",
    "    )\n",
    ")\n",
    "\n",
    "actions_aggs[id_] = (\n",
    "    search_history\n",
    "    .filter(pl.col('action_type_id') == id_)\n",
    "    .filter(pl.col('timestamp').dt.date() <= train_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30 * 5))\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        # Общее количество поисков за 5 месяцев\n",
    "        pl.count('search_query').cast(pl.Int32).alias(f'num_{suf}'),\n",
    "        pl.col('search_query').n_unique().alias(f'unique_{suf}_queries'),\n",
    "        \n",
    "        # Количество поисков за последний месяц (30 дней)\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_month'),\n",
    "        \n",
    "        # Количество поисков за последнюю неделю (7 дней)\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=7))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_week'),\n",
    "\n",
    "        (pl.count() / (pl.max('timestamp') - pl.min('timestamp')).dt.total_days()).alias(f'{suf}_daily_rate'),\n",
    "\n",
    "        pl.col('cluster').n_unique().alias(f'num_{suf}_clusters'),\n",
    "        pl.col('cluster').mode().first().alias(f'main_{suf}_cluster'),\n",
    "        \n",
    "        # Динамика кластеров\n",
    "        pl.col('cluster')\n",
    "            .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30))\n",
    "            .mode().first()\n",
    "            .alias(f'recent_{suf}_cluster'),\n",
    "\n",
    "        (pl.col('cluster').value_counts().struct.field('count').max() / pl.col('cluster').count()).alias(f'{suf}_cluster_concentration'),\n",
    "        \n",
    "        # Энтропия кластеров (мера разнообразия)\n",
    "        (-(pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count()).log()\n",
    "            * (pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count())\n",
    "            .sum()).alias(f'{suf}_cluster_entropy'),\n",
    "        \n",
    "        # Переключения между кластерами\n",
    "        pl.col('cluster').diff().fill_null(0).abs().sum().alias(f'{suf}_cluster_switches'),\n",
    "        \n",
    "        # Стабильность кластеров (процент повторяющихся)\n",
    "        ((pl.col('cluster').count() - pl.col('cluster').n_unique()) / pl.col('cluster').count())\n",
    "            .alias(f'{suf}_cluster_stability'),\n",
    "        \n",
    "        # Время в основном кластере\n",
    "        (pl.col('timestamp')\n",
    "            .filter(pl.col('cluster') == pl.col('cluster').mode().first())\n",
    "            .count() / pl.col('timestamp').count())\n",
    "            .alias(f'main_{suf}_cluster_time_ratio'),\n",
    "\n",
    "        pl.col('timestamp').filter(pl.col('cluster').diff().fill_null(0) != 0)\n",
    "            .diff()\n",
    "            .dt.total_days()\n",
    "            .mean()\n",
    "            .alias(f'{suf}_mean_cluster_switch_days'),\n",
    "\n",
    "        pl.col('search_query').str.len_chars().mean().alias(f'{suf}_mean_query_len'),\n",
    "        \n",
    "        (pl.col('search_query').str.len_chars()\n",
    "            .filter(pl.col('cluster') == pl.col('cluster').mode().first()).mean() - \n",
    "            pl.col('search_query').str.len_chars()\n",
    "                .filter(pl.col('cluster') != pl.col('cluster').mode().first()).mean())\n",
    "                .alias(f'{suf}_main_cluster_query_len_diff'),\n",
    "\n",
    "        pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "        pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "    )\n",
    "    .join(cluster_counts, on='user_id', how='left')\n",
    "    .with_columns([\n",
    "        (pl.lit(val_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "\n",
    "        (pl.lit(val_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "    ])\n",
    "    .select(\n",
    "        'user_id',\n",
    "        f'num_{suf}',\n",
    "        f'unique_{suf}_queries',\n",
    "        f'num_{suf}_last_month',\n",
    "        f'num_{suf}_last_week',\n",
    "        f'{suf}_daily_rate',\n",
    "        f'num_{suf}_clusters',\n",
    "        f'main_{suf}_cluster',\n",
    "        pl.col('top3_clusters').alias(f'top3_{suf}_clusters'),\n",
    "        pl.col('top3_counts').alias(f'top3_{suf}_counts'),\n",
    "        f'recent_{suf}_cluster',\n",
    "        f'{suf}_cluster_concentration',\n",
    "        f'{suf}_cluster_entropy',\n",
    "        f'{suf}_cluster_switches',\n",
    "        f'{suf}_cluster_stability',\n",
    "        f'main_{suf}_cluster_time_ratio',\n",
    "        f'{suf}_mean_cluster_switch_days',\n",
    "        f'{suf}_mean_query_len',\n",
    "        f'{suf}_main_cluster_query_len_diff',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "        f'last_{suf}_time',\n",
    "        f'first_{suf}_time',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = val_target\n",
    "for _, actions_aggs_df in actions_aggs.items():\n",
    "    df_main = (\n",
    "        df_main\n",
    "        .join(actions_aggs_df, on='user_id', how='left')\n",
    "    )\n",
    "    \n",
    "df_pd = df_main.to_pandas()\n",
    "\n",
    "num_cols = ['num_products_click', 'sum_discount_price_click', 'max_discount_price_click', 'days_since_last_click', 'days_since_first_click', \n",
    "    'num_products_favorite', 'sum_discount_price_favorite', 'max_discount_price_favorite', 'days_since_last_favorite', \n",
    "    'days_since_first_favorite', 'num_products_order', 'sum_discount_price_order', 'max_discount_price_order', 'days_since_last_order', \n",
    "    'days_since_first_order', 'num_products_to_cart', 'sum_discount_price_to_cart', 'max_discount_price_to_cart', 'days_since_last_to_cart', \n",
    "    'days_since_first_to_cart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_utils import *\n",
    "\n",
    "df_pd = add_polynomial_features_pd(df_pd, num_cols, degree=3)\n",
    "\n",
    "df_pd = df_pd[df_pd['user_id'] != 2346229]\n",
    "\n",
    "tr, val = get_split(df_pd, val_size=0.3)\n",
    "\n",
    "cols = [\n",
    "    # Существующие столбцы\n",
    "    'num_products_click', \n",
    "    'sum_discount_price_click', 'max_discount_price_click',\n",
    "    'days_since_last_click', 'days_since_first_click',\n",
    "    'num_products_favorite', 'sum_discount_price_favorite',\n",
    "    'max_discount_price_favorite', 'days_since_last_favorite',\n",
    "    'days_since_first_favorite', 'num_products_order',\n",
    "    'sum_discount_price_order', 'max_discount_price_order',\n",
    "    'days_since_last_order', 'days_since_first_order',\n",
    "    'num_products_to_cart', 'sum_discount_price_to_cart',\n",
    "    'max_discount_price_to_cart', 'days_since_last_to_cart',\n",
    "    'days_since_first_to_cart', 'num_search', 'days_since_last_search',\n",
    "    'days_since_first_search', \n",
    "    'main_search_cluster', 'num_search_clusters',\n",
    "    \n",
    "    # Новые столбцы из анализа поиска\n",
    "    'unique_search_queries',               # Количество уникальных поисковых запросов\n",
    "    'num_search_last_month',               # Поисков за последний месяц\n",
    "    'num_search_last_week',                # Поисков за последнюю неделю\n",
    "    'search_daily_rate',                   # Среднедневная частота поисков\n",
    "    'recent_search_cluster',               # Последний доминирующий кластер\n",
    "    'search_cluster_concentration',        # Концентрация интересов\n",
    "    'search_cluster_switches',             # Переключения между кластерами\n",
    "    'search_cluster_stability',            # Стабильность кластеров\n",
    "    'main_search_cluster_time_ratio',      # Время в основном кластере\n",
    "    'search_mean_cluster_switch_days',     # Среднее время между сменами кластеров\n",
    "    'search_mean_query_len',               # Средняя длина запроса\n",
    "    'search_main_cluster_query_len_diff',  # Разница длины запросов\n",
    "]\n",
    "\n",
    "params={\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 100,\n",
    "        'num_iterations': 700,\n",
    "        'early_stopping_rounds': 60,\n",
    "        'verbose': 1,\n",
    "        'importance_type': 'split'\n",
    "    }\n",
    "\n",
    "model = train_model(tr, val, cols, 'target', params=params, shadow_features=False, sklearn_style=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_utils import *\n",
    "\n",
    "print('feature_importance:', model.feature_importances_, '\\n')\n",
    "\n",
    "plot_lgbm_importance(model, cols, importance_type='split', top_k=30, sklearn_style=True)\n",
    "\n",
    "test_users_submission = (\n",
    "    pl.read_csv(os.path.join(data_path, 'test_users.csv'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_aggs = {}\n",
    "actions_id_to_suf = {\n",
    "    1: \"click\",\n",
    "    2: \"favorite\", \n",
    "    3: \"order\",\n",
    "    5: \"to_cart\",\n",
    "}\n",
    "\n",
    "# Сначала соберем все агрегированные данные\n",
    "all_aggs = []\n",
    "numeric_features = []\n",
    "\n",
    "for id_, suf in actions_id_to_suf.items():\n",
    "    aggs = (\n",
    "        actions_history\n",
    "        .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "        .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30 * 5))\n",
    "        .filter(pl.col('action_type_id') == id_)\n",
    "        .join(\n",
    "            product_information.select('product_id', 'discount_price'),\n",
    "            on='product_id',\n",
    "        )\n",
    "        .group_by('user_id')\n",
    "        .agg(\n",
    "            pl.count('product_id').cast(pl.Int32).alias(f'num_products_{suf}'),\n",
    "            pl.sum('discount_price').cast(pl.Float32).alias(f'sum_discount_price_{suf}'),\n",
    "            pl.max('discount_price').cast(pl.Float32).alias(f'max_discount_price_{suf}'),\n",
    "            pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "            pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "        )\n",
    "        .with_columns([\n",
    "            (pl.lit(test_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "            \n",
    "            (pl.lit(test_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Сохраняем имена числовых фичей для создания полиномов\n",
    "    numeric_features.extend([\n",
    "        f'num_products_{suf}',\n",
    "        f'sum_discount_price_{suf}', \n",
    "        f'max_discount_price_{suf}',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "    ])\n",
    "    \n",
    "    actions_aggs[id_] = aggs\n",
    "    all_aggs.append(aggs)\n",
    "\n",
    "# Объединяем все агрегации по user_id с суффиксами\n",
    "combined_val = all_aggs[0]\n",
    "for i, agg in enumerate(all_aggs[1:], 1):\n",
    "    combined_val = combined_val.join(\n",
    "        agg, \n",
    "        on='user_id', \n",
    "        how='outer',\n",
    "        suffix=f\"_{i}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = 4\n",
    "suf = 'search'\n",
    "\n",
    "# Вычисляем top3 кластеров для валидации (аналогично трейну)\n",
    "val_cluster_counts = (\n",
    "    search_history\n",
    "    .filter(pl.col('action_type_id') == id_)\n",
    "    .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30 * 5))\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        pl.col('cluster').value_counts().alias('cluster_counts')\n",
    "    )\n",
    "    .explode('cluster_counts')\n",
    "    .with_columns(\n",
    "        pl.col('cluster_counts').struct.field('cluster').alias('cluster_name'),\n",
    "        pl.col('cluster_counts').struct.field('count').alias('cluster_count')\n",
    "    )\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        pl.col('cluster_name').sort_by('cluster_count', descending=True).head(3).alias('top3_clusters'),\n",
    "        pl.col('cluster_count').sort(descending=True).head(3).alias('top3_counts')\n",
    "    )\n",
    ")\n",
    "\n",
    "actions_aggs[id_] = (\n",
    "    search_history\n",
    "    .filter(pl.col('action_type_id') == id_)\n",
    "    .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30 * 5))\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        # Общее количество поисков за 5 месяцев\n",
    "        pl.count('search_query').cast(pl.Int32).alias(f'num_{suf}'),\n",
    "        pl.col('search_query').n_unique().alias(f'unique_{suf}_queries'),\n",
    "        \n",
    "        # Количество поисков за последний месяц (30 дней)\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_month'),\n",
    "        \n",
    "        # Количество поисков за последнюю неделю (7 дней)\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=7))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_week'),\n",
    "\n",
    "        (pl.count() / (pl.max('timestamp') - pl.min('timestamp')).dt.total_days()).alias(f'{suf}_daily_rate'),\n",
    "\n",
    "        pl.col('cluster').n_unique().alias(f'num_{suf}_clusters'),\n",
    "        pl.col('cluster').mode().first().alias(f'main_{suf}_cluster'),\n",
    "        \n",
    "        # Динамика кластеров\n",
    "        pl.col('cluster')\n",
    "            .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30))\n",
    "            .mode().first()\n",
    "            .alias(f'recent_{suf}_cluster'),\n",
    "\n",
    "        (pl.col('cluster').value_counts().struct.field('count').max() / pl.col('cluster').count()).alias(f'{suf}_cluster_concentration'),\n",
    "        \n",
    "        # Энтропия кластеров\n",
    "        (-(pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count()).log()\n",
    "            * (pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count())\n",
    "            .sum()).alias(f'{suf}_cluster_entropy'),\n",
    "        \n",
    "        # Переключения между кластерами\n",
    "        pl.col('cluster').diff().fill_null(0).abs().sum().alias(f'{suf}_cluster_switches'),\n",
    "        \n",
    "        # Стабильность кластеров\n",
    "        ((pl.col('cluster').count() - pl.col('cluster').n_unique()) / pl.col('cluster').count())\n",
    "            .alias(f'{suf}_cluster_stability'),\n",
    "        \n",
    "        # Время в основном кластере\n",
    "        (pl.col('timestamp')\n",
    "            .filter(pl.col('cluster') == pl.col('cluster').mode().first())\n",
    "            .count() / pl.col('timestamp').count())\n",
    "            .alias(f'main_{suf}_cluster_time_ratio'),\n",
    "\n",
    "        pl.col('timestamp').filter(pl.col('cluster').diff().fill_null(0) != 0)\n",
    "            .diff()\n",
    "            .dt.total_days()\n",
    "            .mean()\n",
    "            .alias(f'{suf}_mean_cluster_switch_days'),\n",
    "\n",
    "        pl.col('search_query').str.len_chars().mean().alias(f'{suf}_mean_query_len'),\n",
    "        \n",
    "        (pl.col('search_query').str.len_chars()\n",
    "            .filter(pl.col('cluster') == pl.col('cluster').mode().first()).mean() - \n",
    "            pl.col('search_query').str.len_chars()\n",
    "                .filter(pl.col('cluster') != pl.col('cluster').mode().first()).mean())\n",
    "                .alias(f'{suf}_main_cluster_query_len_diff'),\n",
    "\n",
    "        pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "        pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "    )\n",
    "    .join(val_cluster_counts, on='user_id', how='left')\n",
    "    .with_columns([\n",
    "        (pl.lit(test_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "\n",
    "        (pl.lit(test_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "    ])\n",
    "    .select(\n",
    "        'user_id',\n",
    "        f'num_{suf}',\n",
    "        f'unique_{suf}_queries',\n",
    "        f'num_{suf}_last_month',\n",
    "        f'num_{suf}_last_week',\n",
    "        f'{suf}_daily_rate',\n",
    "        f'num_{suf}_clusters',\n",
    "        f'main_{suf}_cluster',\n",
    "        pl.col('top3_clusters').alias(f'top3_{suf}_clusters'),\n",
    "        pl.col('top3_counts').alias(f'top3_{suf}_counts'),\n",
    "        f'recent_{suf}_cluster',\n",
    "        f'{suf}_cluster_concentration',\n",
    "        f'{suf}_cluster_entropy',\n",
    "        f'{suf}_cluster_switches',\n",
    "        f'{suf}_cluster_stability',\n",
    "        f'main_{suf}_cluster_time_ratio',\n",
    "        f'{suf}_mean_cluster_switch_days',\n",
    "        f'{suf}_mean_query_len',\n",
    "        f'{suf}_main_cluster_query_len_diff',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "        f'last_{suf}_time',\n",
    "        f'first_{suf}_time',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = test_users_submission\n",
    "for _, actions_aggs_df in actions_aggs.items():\n",
    "    df_main = (\n",
    "        df_main\n",
    "        .join(actions_aggs_df, on='user_id', how='left')\n",
    "    )\n",
    "    \n",
    "df_pd = df_main.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_utils import *\n",
    "\n",
    "df_pd = add_polynomial_features_pd(df_pd, num_cols, degree=3)\n",
    "\n",
    "df_pd['predict'] = model.predict_proba(df_pd[cols])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd[['user_id', 'predict']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd[['user_id', 'predict']].to_csv('res.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
