{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2d3475",
   "metadata": {},
   "source": [
    "# Дорогой дневник"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e3f73",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8676fd",
   "metadata": {},
   "source": [
    "1) Сначала я решил попробовать без генерации новых фичей и без подкрутки параметров запустить LightGBM и посмотреть, что будет. Скор 0.720\n",
    "При этом пока еще нет ни генерации новых признаков, ни использования таблицы поиска и тд. Попробуем добавить.\n",
    "\n",
    "2) Просто запускаем код из \"baseline_1_pandas.ipynb\" и получаем обещаный скор 0.817\n",
    "\n",
    "3) Оптюнил 10 минут, безрезультатно. Буду придумывать новые признаки. Думаю начать с кластеризации и knn. \n",
    "\n",
    "4) Сначала решил просто расширить плавающее окно с 4 до 5 месяцев, результат 0.8192.\n",
    "\n",
    "    Есть огромное количество идей, только что заменил пандас на поларс, потому что он реально на порядок быстрее. В первую очередь хочется применить знания с семинара по интерпретации бустингов, но сначала заменить катбуст на lgbm, ибо Илья утверждал, что при должном обращении он рвет и мечет."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcb1b5",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b591fa",
   "metadata": {},
   "source": [
    "# Другой подход к кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b705cb0",
   "metadata": {},
   "source": [
    "Пробуем новую идею, если не получается кластеризовать все обьекты, будем делать так: мы же уже знаем какие есть кластеры, просто раздадим метки по ключевым словам, олценим сколько осталось и проведем еще одну кластеризацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfafca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>target</th><th>user_id</th></tr><tr><td>i32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>1227381</td></tr><tr><td>1</td><td>647575</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌────────┬─────────┐\n",
       "│ target ┆ user_id │\n",
       "│ ---    ┆ ---     │\n",
       "│ i32    ┆ u32     │\n",
       "╞════════╪═════════╡\n",
       "│ 0      ┆ 1227381 │\n",
       "│ 1      ┆ 647575  │\n",
       "└────────┴─────────┘"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost\n",
    "import os\n",
    "from datetime import date, timedelta\n",
    "from itertools import combinations\n",
    "\n",
    "from local_utils import *\n",
    "import lightgbm as lgb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "test_start_date = date(2024, 8, 1)\n",
    "val_start_date = date(2024, 7, 1)\n",
    "val_end_date = date(2024, 7, 31)\n",
    "train_end_date = date(2024, 6, 30)\n",
    "data_path = \"C:\\\\Users\\\\Admin\\\\Desktop\\\\AIM 2сем\\\\ML2\\\\hw2\"\n",
    "\n",
    "actions_history = pl.scan_parquet(os.path.join(data_path, 'actions_history/*.parquet')).collect()\n",
    "search_history = pl.scan_parquet(os.path.join(data_path, 'cluster_search/*.parquet')).collect()\n",
    "product_information = pl.read_csv(\n",
    "    os.path.join(data_path, 'cluster_product_information.csv'),\n",
    "    ignore_errors=True\n",
    ")\n",
    "\n",
    "val_target = (\n",
    "    actions_history\n",
    "    .filter(pl.col('timestamp').dt.date() >= val_start_date)\n",
    "    .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "    .select('user_id', (pl.col('action_type_id') == 3).alias('has_order'))\n",
    "    .group_by('user_id')\n",
    "    .agg(pl.max('has_order').cast(pl.Int32).alias('target'))\n",
    ")\n",
    "\n",
    "val_target.group_by('target').agg(pl.count('user_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f61834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_aggs = {}\n",
    "actions_id_to_suf = {\n",
    "    1: \"click\",\n",
    "    2: \"favorite\", \n",
    "    3: \"order\",\n",
    "    5: \"to_cart\",\n",
    "}\n",
    "\n",
    "# Сначала соберем все агрегированные данные\n",
    "all_aggs = []\n",
    "numeric_features = []\n",
    "\n",
    "for id_, suf in actions_id_to_suf.items():\n",
    "    aggs = (\n",
    "        actions_history\n",
    "        .filter(pl.col('timestamp').dt.date() <= train_end_date)\n",
    "        .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30 * 4))\n",
    "        .filter(pl.col('action_type_id') == id_)\n",
    "        .join(\n",
    "            product_information.select('product_id', 'discount_price'),\n",
    "            on='product_id',\n",
    "        )\n",
    "        .group_by('user_id')\n",
    "        .agg(\n",
    "            pl.count('product_id').cast(pl.Int32).alias(f'num_products_{suf}'),\n",
    "            pl.sum('discount_price').cast(pl.Float32).alias(f'sum_discount_price_{suf}'),\n",
    "            pl.max('discount_price').cast(pl.Float32).alias(f'max_discount_price_{suf}'),\n",
    "            pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "            pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "        )\n",
    "        .with_columns([\n",
    "            (pl.lit(val_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "            \n",
    "            (pl.lit(val_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Сохраняем имена числовых фичей для последующего создания полиномов\n",
    "    numeric_features.extend([\n",
    "        f'num_products_{suf}',\n",
    "        f'sum_discount_price_{suf}', \n",
    "        f'max_discount_price_{suf}',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "    ])\n",
    "    \n",
    "    actions_aggs[id_] = aggs\n",
    "    all_aggs.append(aggs)\n",
    "\n",
    "# Объединяем все агрегации по user_id с указанием суффиксов\n",
    "combined = all_aggs[0]\n",
    "for i, agg in enumerate(all_aggs[1:], 1):\n",
    "    combined = combined.join(\n",
    "        agg, \n",
    "        on='user_id', \n",
    "        how='left',\n",
    "        suffix=f\"_{i}\"  # Добавляем уникальный суффикс для каждого соединения\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df901c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33968\\276488151.py:52: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  (pl.count() / (pl.max('timestamp') - pl.min('timestamp')).dt.total_days()).alias(f'{suf}_daily_rate'),\n"
     ]
    }
   ],
   "source": [
    "# search_aggs\n",
    "id_ = 4\n",
    "suf = 'search'\n",
    "\n",
    "# Сначала вычислим value_counts отдельно и развернем их в плоскую структуру\n",
    "cluster_counts = (\n",
    "    search_history\n",
    "    .filter(pl.col('action_type_id') == id_)\n",
    "    .filter(pl.col('timestamp').dt.date() <= train_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30 * 5))\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        pl.col('cluster').value_counts().alias('cluster_counts')\n",
    "    )\n",
    "    .explode('cluster_counts')\n",
    "    .with_columns(\n",
    "        pl.col('cluster_counts').struct.field('cluster').alias('cluster_name'),\n",
    "        pl.col('cluster_counts').struct.field('count').alias('cluster_count')\n",
    "    )\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        pl.col('cluster_name').sort_by('cluster_count', descending=True).head(3).alias('top3_clusters'),\n",
    "        pl.col('cluster_count').sort(descending=True).head(3).alias('top3_counts')\n",
    "    )\n",
    ")\n",
    "\n",
    "actions_aggs[id_] = (\n",
    "    search_history\n",
    "    .filter(pl.col('action_type_id') == id_)\n",
    "    .filter(pl.col('timestamp').dt.date() <= train_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30 * 5))\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        # Общее количество поисков за 5 месяцев\n",
    "        pl.count('search_query').cast(pl.Int32).alias(f'num_{suf}'),\n",
    "        pl.col('search_query').n_unique().alias(f'unique_{suf}_queries'),\n",
    "        \n",
    "        # Количество поисков за последний месяц (30 дней)\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_month'),\n",
    "        \n",
    "        # Количество поисков за последнюю неделю (7 дней)\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=7))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_week'),\n",
    "\n",
    "        (pl.count() / (pl.max('timestamp') - pl.min('timestamp')).dt.total_days()).alias(f'{suf}_daily_rate'),\n",
    "\n",
    "        pl.col('cluster').n_unique().alias(f'num_{suf}_clusters'),\n",
    "        pl.col('cluster').mode().first().alias(f'main_{suf}_cluster'),\n",
    "        \n",
    "        # Динамика кластеров\n",
    "        pl.col('cluster')\n",
    "            .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30))\n",
    "            .mode().first()\n",
    "            .alias(f'recent_{suf}_cluster'),\n",
    "\n",
    "        (pl.col('cluster').value_counts().struct.field('count').max() / pl.col('cluster').count()).alias(f'{suf}_cluster_concentration'),\n",
    "        \n",
    "        # Энтропия кластеров (мера разнообразия)\n",
    "        (-(pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count()).log()\n",
    "            * (pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count())\n",
    "            .sum()).alias(f'{suf}_cluster_entropy'),\n",
    "        \n",
    "        # Переключения между кластерами\n",
    "        pl.col('cluster').diff().fill_null(0).abs().sum().alias(f'{suf}_cluster_switches'),\n",
    "        \n",
    "        # Стабильность кластеров (процент повторяющихся)\n",
    "        ((pl.col('cluster').count() - pl.col('cluster').n_unique()) / pl.col('cluster').count())\n",
    "            .alias(f'{suf}_cluster_stability'),\n",
    "        \n",
    "        # Время в основном кластере\n",
    "        (pl.col('timestamp')\n",
    "            .filter(pl.col('cluster') == pl.col('cluster').mode().first())\n",
    "            .count() / pl.col('timestamp').count())\n",
    "            .alias(f'main_{suf}_cluster_time_ratio'),\n",
    "\n",
    "        pl.col('timestamp').filter(pl.col('cluster').diff().fill_null(0) != 0)\n",
    "            .diff()\n",
    "            .dt.total_days()\n",
    "            .mean()\n",
    "            .alias(f'{suf}_mean_cluster_switch_days'),\n",
    "\n",
    "        pl.col('search_query').str.len_chars().mean().alias(f'{suf}_mean_query_len'),\n",
    "        \n",
    "        (pl.col('search_query').str.len_chars()\n",
    "            .filter(pl.col('cluster') == pl.col('cluster').mode().first()).mean() - \n",
    "            pl.col('search_query').str.len_chars()\n",
    "                .filter(pl.col('cluster') != pl.col('cluster').mode().first()).mean())\n",
    "                .alias(f'{suf}_main_cluster_query_len_diff'),\n",
    "\n",
    "        pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "        pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "    )\n",
    "    .join(cluster_counts, on='user_id', how='left')\n",
    "    .with_columns([\n",
    "        (pl.lit(val_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "\n",
    "        (pl.lit(val_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "    ])\n",
    "    .select(\n",
    "        'user_id',\n",
    "        f'num_{suf}',\n",
    "        f'unique_{suf}_queries',\n",
    "        f'num_{suf}_last_month',\n",
    "        f'num_{suf}_last_week',\n",
    "        f'{suf}_daily_rate',\n",
    "        f'num_{suf}_clusters',\n",
    "        f'main_{suf}_cluster',\n",
    "        pl.col('top3_clusters').alias(f'top3_{suf}_clusters'),\n",
    "        pl.col('top3_counts').alias(f'top3_{suf}_counts'),\n",
    "        f'recent_{suf}_cluster',\n",
    "        f'{suf}_cluster_concentration',\n",
    "        f'{suf}_cluster_entropy',\n",
    "        f'{suf}_cluster_switches',\n",
    "        f'{suf}_cluster_stability',\n",
    "        f'main_{suf}_cluster_time_ratio',\n",
    "        f'{suf}_mean_cluster_switch_days',\n",
    "        f'{suf}_mean_query_len',\n",
    "        f'{suf}_main_cluster_query_len_diff',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "        f'last_{suf}_time',\n",
    "        f'first_{suf}_time',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88399248",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_cluster_aggs = (\n",
    "    actions_history\n",
    "    .filter(pl.col('timestamp').dt.date() <= train_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= train_end_date - timedelta(days=30 * 5))\n",
    "    .join(\n",
    "        product_information.select('product_id', 'cluster'),\n",
    "        on='product_id'\n",
    "    )\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        # Общие агрегаты по кластерам продуктов\n",
    "        pl.col('cluster').n_unique().alias('num_product_clusters'),\n",
    "        pl.col('cluster').mode().first().alias('main_product_cluster'),\n",
    "        \n",
    "        # Аналогичные агрегаты как для search\n",
    "        (pl.col('cluster').value_counts().struct.field('count').max() / pl.col('cluster').count()).alias('product_cluster_concentration'),\n",
    "        \n",
    "        # Энтропия кластеров продуктов\n",
    "        (-(pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count()).log()\n",
    "            * (pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count())\n",
    "            .sum()).alias('product_cluster_entropy'),\n",
    "        \n",
    "        # Стабильность кластеров продуктов\n",
    "        ((pl.col('cluster').count() - pl.col('cluster').n_unique()) / pl.col('cluster').count())\n",
    "            .alias('product_cluster_stability'),\n",
    "        \n",
    "        # Время в основном кластере продуктов\n",
    "        (pl.col('timestamp')\n",
    "            .filter(pl.col('cluster') == pl.col('cluster').mode().first())\n",
    "            .count() / pl.col('timestamp').count()\n",
    "        ).alias('main_product_cluster_time_ratio'),\n",
    "            \n",
    "        # Top 3 кластеров продуктов\n",
    "        pl.col('cluster').value_counts().struct.field('cluster').alias('top_product_clusters'),\n",
    "        pl.col('cluster').value_counts().struct.field('count').alias('top_product_counts')\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col('top_product_clusters').list.head(3).alias('top3_product_clusters'),\n",
    "        pl.col('top_product_counts').list.head(3).alias('top3_product_counts')\n",
    "    )\n",
    "    .drop(['top_product_clusters', 'top_product_counts'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "582ce0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33968\\1964511306.py:14: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  pl.count().alias('total_actions_30d'),\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33968\\1964511306.py:31: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  (pl.col('cluster').value_counts().struct.field('count').max() / pl.count()).alias('main_cluster_ratio_30d')\n"
     ]
    }
   ],
   "source": [
    "train_last_month_features = (\n",
    "    actions_history\n",
    "    .filter(\n",
    "        (pl.col('timestamp').dt.date() < val_start_date) &  # до валидации\n",
    "        (pl.col('timestamp').dt.date() >= val_start_date - timedelta(days=30))  # последние 30 дней\n",
    "    )\n",
    "    .join(\n",
    "        product_information.select('product_id', 'discount_price', 'cluster'),\n",
    "        on='product_id'\n",
    "    )\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        # Общая активность\n",
    "        pl.count().alias('total_actions_30d'),\n",
    "        \n",
    "        # Разбивка по типам действий\n",
    "        (pl.col('action_type_id') == 1).sum().alias('clicks_30d'),\n",
    "        (pl.col('action_type_id') == 2).sum().alias('favorites_30d'),\n",
    "        (pl.col('action_type_id') == 5).sum().alias('cart_adds_30d'),\n",
    "        \n",
    "        # Финансовые метрики\n",
    "        pl.sum('discount_price').alias('total_spent_30d'),\n",
    "        pl.mean('discount_price').alias('avg_price_30d'),\n",
    "        \n",
    "        # Временные характеристики (исправлено!)\n",
    "        (val_start_date - pl.col('timestamp').max().dt.date()).dt.total_days().alias('days_since_last_action'),\n",
    "        (pl.col('timestamp').max() - pl.col('timestamp').min()).dt.total_days().alias('active_days_30d'),\n",
    "        \n",
    "        # Метрики кластеров\n",
    "        pl.col('cluster').n_unique().alias('unique_clusters_30d'),\n",
    "        (pl.col('cluster').value_counts().struct.field('count').max() / pl.count()).alias('main_cluster_ratio_30d')\n",
    "    )\n",
    "    .with_columns(\n",
    "        # Производные признаки\n",
    "        (pl.col('total_actions_30d') / pl.col('active_days_30d')).alias('daily_actions_rate_30d'),\n",
    "        (pl.col('cart_adds_30d') / pl.col('total_actions_30d')).alias('cart_add_ratio_30d'),\n",
    "        (pl.col('favorites_30d') / pl.col('total_actions_30d')).alias('favorite_ratio_30d')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a83618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = val_target\n",
    "for _, actions_aggs_df in actions_aggs.items():\n",
    "    df_main = (\n",
    "        df_main\n",
    "        .join(actions_aggs_df, on='user_id', how='left')\n",
    "    )\n",
    "\n",
    "df_main = df_main.join(product_cluster_aggs, on='user_id', how='left')\n",
    "df_main = df_main.join(train_last_month_features, on='user_id', how='left')\n",
    "    \n",
    "df_pd = df_main.to_pandas()\n",
    "\n",
    "columns_to_log = ['max_discount_price_click', 'num_products_favorite', 'sum_discount_price_favorite', 'max_discount_price_favorite',  'num_products_order', 'sum_discount_price_order', 'sum_discount_price_order',  'num_products_to_cart', 'max_discount_price_to_cart', 'num_search', 'unique_search_queries', 'num_search_last_month', 'num_search_last_week', 'search_daily_rate', 'search_cluster_switches', 'search_mean_query_len', 'search_main_cluster_query_len_diff']\n",
    "\n",
    "df_pd = apply_log_transform(df_pd, columns_to_log, drop_original=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937b516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b6266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4bde7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f46198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nans filled\n"
     ]
    }
   ],
   "source": [
    "from local_utils import *\n",
    "\n",
    "pca_cols = list(set(df_pd.columns) - {'user_id', 'target', 'last_click_time', 'first_click_time', 'last_favorite_time', 'first_favorite_time', \n",
    "                                'last_order_time', 'first_order_time', 'last_to_cart_time', 'first_to_cart_time', 'last_search_time', 'first_search_time',\n",
    "                                'top3_search_clusters', 'top3_search_counts', 'search_cluster_entropy', 'top3_product_counts', 'product_cluster_entropy', 'top3_product_clusters'})\n",
    "df_pd = add_pca_columns(df_pd,  pca_cols,  n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a90335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nans filled\n",
      "Data scaled\n",
      "Using CPU\n",
      "FAISS index built\n",
      "KNN search done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN features created\n"
     ]
    }
   ],
   "source": [
    "knn_cols = ['days_since_first_order', 'days_since_last_order', 'sum_discount_price_to_cart', 'num_products_click', 'main_search_cluster', 'search_cluster_stability', 'product_cluster_stability']\n",
    "\n",
    "df_pd = add_knn_features_faiss(df_pd, knn_cols, n_neighbors=5, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89385d9",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcbca9c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f200c7f5",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4232f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3e5a2dc",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070d59b",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c184c11",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ad191a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33968\\2116881979.py:59: DeprecationWarning: Use of `how='outer'` should be replaced with `how='full'`.\n",
      "  combined_val = combined_val.join(\n"
     ]
    }
   ],
   "source": [
    "actions_aggs = {}\n",
    "actions_id_to_suf = {\n",
    "    1: \"click\",\n",
    "    2: \"favorite\", \n",
    "    3: \"order\",\n",
    "    5: \"to_cart\",\n",
    "}\n",
    "\n",
    "# Сначала соберем все агрегированные данные\n",
    "all_aggs = []\n",
    "numeric_features = []\n",
    "\n",
    "for id_, suf in actions_id_to_suf.items():\n",
    "    aggs = (\n",
    "        actions_history\n",
    "        .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "        .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30 * 5))\n",
    "        .filter(pl.col('action_type_id') == id_)\n",
    "        .join(\n",
    "            product_information.select('product_id', 'discount_price'),\n",
    "            on='product_id',\n",
    "        )\n",
    "        .group_by('user_id')\n",
    "        .agg(\n",
    "            pl.count('product_id').cast(pl.Int32).alias(f'num_products_{suf}'),\n",
    "            pl.sum('discount_price').cast(pl.Float32).alias(f'sum_discount_price_{suf}'),\n",
    "            pl.max('discount_price').cast(pl.Float32).alias(f'max_discount_price_{suf}'),\n",
    "            pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "            pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "        )\n",
    "        .with_columns([\n",
    "            (pl.lit(test_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "            \n",
    "            (pl.lit(test_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Сохраняем имена числовых фичей для создания полиномов\n",
    "    numeric_features.extend([\n",
    "        f'num_products_{suf}',\n",
    "        f'sum_discount_price_{suf}', \n",
    "        f'max_discount_price_{suf}',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "    ])\n",
    "    \n",
    "    actions_aggs[id_] = aggs\n",
    "    all_aggs.append(aggs)\n",
    "\n",
    "# Объединяем все агрегации по user_id с суффиксами\n",
    "combined_val = all_aggs[0]\n",
    "for i, agg in enumerate(all_aggs[1:], 1):\n",
    "    combined_val = combined_val.join(\n",
    "        agg, \n",
    "        on='user_id', \n",
    "        how='outer',\n",
    "        suffix=f\"_{i}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81add27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33968\\555555092.py:51: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  (pl.count() / (pl.max('timestamp') - pl.min('timestamp')).dt.total_days()).alias(f'{suf}_daily_rate'),\n"
     ]
    }
   ],
   "source": [
    "id_ = 4\n",
    "suf = 'search'\n",
    "\n",
    "# Вычисляем top3 кластеров для валидации (аналогично трейну)\n",
    "val_cluster_counts = (\n",
    "    search_history\n",
    "    .filter(pl.col('action_type_id') == id_)\n",
    "    .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30 * 5))\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        pl.col('cluster').value_counts().alias('cluster_counts')\n",
    "    )\n",
    "    .explode('cluster_counts')\n",
    "    .with_columns(\n",
    "        pl.col('cluster_counts').struct.field('cluster').alias('cluster_name'),\n",
    "        pl.col('cluster_counts').struct.field('count').alias('cluster_count')\n",
    "    )\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        pl.col('cluster_name').sort_by('cluster_count', descending=True).head(3).alias('top3_clusters'),\n",
    "        pl.col('cluster_count').sort(descending=True).head(3).alias('top3_counts')\n",
    "    )\n",
    ")\n",
    "\n",
    "actions_aggs[id_] = (\n",
    "    search_history\n",
    "    .filter(pl.col('action_type_id') == id_)\n",
    "    .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30 * 5))\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        # Общее количество поисков за 5 месяцев\n",
    "        pl.count('search_query').cast(pl.Int32).alias(f'num_{suf}'),\n",
    "        pl.col('search_query').n_unique().alias(f'unique_{suf}_queries'),\n",
    "        \n",
    "        # Количество поисков за последний месяц (30 дней)\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_month'),\n",
    "        \n",
    "        # Количество поисков за последнюю неделю (7 дней)\n",
    "        pl.col('search_query')\n",
    "            .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=7))\n",
    "            .count()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'num_{suf}_last_week'),\n",
    "\n",
    "        (pl.count() / (pl.max('timestamp') - pl.min('timestamp')).dt.total_days()).alias(f'{suf}_daily_rate'),\n",
    "\n",
    "        pl.col('cluster').n_unique().alias(f'num_{suf}_clusters'),\n",
    "        pl.col('cluster').mode().first().alias(f'main_{suf}_cluster'),\n",
    "        \n",
    "        # Динамика кластеров\n",
    "        pl.col('cluster')\n",
    "            .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30))\n",
    "            .mode().first()\n",
    "            .alias(f'recent_{suf}_cluster'),\n",
    "\n",
    "        (pl.col('cluster').value_counts().struct.field('count').max() / pl.col('cluster').count()).alias(f'{suf}_cluster_concentration'),\n",
    "        \n",
    "        # Энтропия кластеров\n",
    "        (-(pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count()).log()\n",
    "            * (pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count())\n",
    "            .sum()).alias(f'{suf}_cluster_entropy'),\n",
    "        \n",
    "        # Переключения между кластерами\n",
    "        pl.col('cluster').diff().fill_null(0).abs().sum().alias(f'{suf}_cluster_switches'),\n",
    "        \n",
    "        # Стабильность кластеров\n",
    "        ((pl.col('cluster').count() - pl.col('cluster').n_unique()) / pl.col('cluster').count())\n",
    "            .alias(f'{suf}_cluster_stability'),\n",
    "        \n",
    "        # Время в основном кластере\n",
    "        (pl.col('timestamp')\n",
    "            .filter(pl.col('cluster') == pl.col('cluster').mode().first())\n",
    "            .count() / pl.col('timestamp').count())\n",
    "            .alias(f'main_{suf}_cluster_time_ratio'),\n",
    "\n",
    "        pl.col('timestamp').filter(pl.col('cluster').diff().fill_null(0) != 0)\n",
    "            .diff()\n",
    "            .dt.total_days()\n",
    "            .mean()\n",
    "            .alias(f'{suf}_mean_cluster_switch_days'),\n",
    "\n",
    "        pl.col('search_query').str.len_chars().mean().alias(f'{suf}_mean_query_len'),\n",
    "        \n",
    "        (pl.col('search_query').str.len_chars()\n",
    "            .filter(pl.col('cluster') == pl.col('cluster').mode().first()).mean() - \n",
    "            pl.col('search_query').str.len_chars()\n",
    "                .filter(pl.col('cluster') != pl.col('cluster').mode().first()).mean())\n",
    "                .alias(f'{suf}_main_cluster_query_len_diff'),\n",
    "\n",
    "        pl.max('timestamp').alias(f'last_{suf}_time'),\n",
    "        pl.min('timestamp').alias(f'first_{suf}_time'),\n",
    "    )\n",
    "    .join(val_cluster_counts, on='user_id', how='left')\n",
    "    .with_columns([\n",
    "        (pl.lit(test_start_date) - pl.col(f'last_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_last_{suf}'),\n",
    "\n",
    "        (pl.lit(test_start_date) - pl.col(f'first_{suf}_time'))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(f'days_since_first_{suf}'),\n",
    "    ])\n",
    "    .select(\n",
    "        'user_id',\n",
    "        f'num_{suf}',\n",
    "        f'unique_{suf}_queries',\n",
    "        f'num_{suf}_last_month',\n",
    "        f'num_{suf}_last_week',\n",
    "        f'{suf}_daily_rate',\n",
    "        f'num_{suf}_clusters',\n",
    "        f'main_{suf}_cluster',\n",
    "        pl.col('top3_clusters').alias(f'top3_{suf}_clusters'),\n",
    "        pl.col('top3_counts').alias(f'top3_{suf}_counts'),\n",
    "        f'recent_{suf}_cluster',\n",
    "        f'{suf}_cluster_concentration',\n",
    "        f'{suf}_cluster_entropy',\n",
    "        f'{suf}_cluster_switches',\n",
    "        f'{suf}_cluster_stability',\n",
    "        f'main_{suf}_cluster_time_ratio',\n",
    "        f'{suf}_mean_cluster_switch_days',\n",
    "        f'{suf}_mean_query_len',\n",
    "        f'{suf}_main_cluster_query_len_diff',\n",
    "        f'days_since_last_{suf}',\n",
    "        f'days_since_first_{suf}',\n",
    "        f'last_{suf}_time',\n",
    "        f'first_{suf}_time',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a829d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_product_cluster_aggs = (\n",
    "    actions_history\n",
    "    .filter(pl.col('timestamp').dt.date() <= val_end_date)\n",
    "    .filter(pl.col('timestamp').dt.date() >= val_end_date - timedelta(days=30 * 5))\n",
    "    .join(\n",
    "        product_information.select('product_id', 'cluster'),\n",
    "        on='product_id'\n",
    "    )\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        # Общие агрегаты по кластерам продуктов\n",
    "        pl.col('cluster').n_unique().alias('num_product_clusters'),\n",
    "        pl.col('cluster').mode().first().alias('main_product_cluster'),\n",
    "        \n",
    "        # Аналогичные агрегаты как для search\n",
    "        (pl.col('cluster').value_counts().struct.field('count').max() / pl.col('cluster').count()).alias('product_cluster_concentration'),\n",
    "        \n",
    "        # Энтропия кластеров продуктов\n",
    "        (-(pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count()).log()\n",
    "            * (pl.col('cluster').value_counts().struct.field('count') / pl.col('cluster').count())\n",
    "            .sum()).alias('product_cluster_entropy'),\n",
    "        \n",
    "        # Стабильность кластеров продуктов\n",
    "        ((pl.col('cluster').count() - pl.col('cluster').n_unique()) / pl.col('cluster').count())\n",
    "            .alias('product_cluster_stability'),\n",
    "        \n",
    "        # Время в основном кластере продуктов\n",
    "        (pl.col('timestamp')\n",
    "            .filter(pl.col('cluster') == pl.col('cluster').mode().first())\n",
    "            .count() / pl.col('timestamp').count()\n",
    "        ).alias('main_product_cluster_time_ratio'),\n",
    "            \n",
    "        # Top 3 кластеров продуктов\n",
    "        pl.col('cluster').value_counts().struct.field('cluster').alias('top_product_clusters'),\n",
    "        pl.col('cluster').value_counts().struct.field('count').alias('top_product_counts')\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col('top_product_clusters').list.head(3).alias('top3_product_clusters'),\n",
    "        pl.col('top_product_counts').list.head(3).alias('top3_product_counts')\n",
    "    )\n",
    "    .drop(['top_product_clusters', 'top_product_counts'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9872e1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33968\\4126633476.py:14: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  pl.count().alias('total_actions_30d'),\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_33968\\4126633476.py:26: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  (pl.col('cluster').value_counts().struct.field('count').max() / pl.count()).alias('main_cluster_ratio_30d')\n"
     ]
    }
   ],
   "source": [
    "test_last_month_features = (\n",
    "    actions_history\n",
    "    .filter(\n",
    "        (pl.col('timestamp').dt.date() < test_start_date) &  # до теста\n",
    "        (pl.col('timestamp').dt.date() >= test_start_date - timedelta(days=30))  # последние 30 дней\n",
    "    )\n",
    "    .join(\n",
    "        product_information.select('product_id', 'discount_price', 'cluster'),\n",
    "        on='product_id'\n",
    "    )\n",
    "    .group_by('user_id')\n",
    "    .agg(\n",
    "        # Те же метрики, что и для трейна\n",
    "        pl.count().alias('total_actions_30d'),\n",
    "        (pl.col('action_type_id') == 1).sum().alias('clicks_30d'),\n",
    "        (pl.col('action_type_id') == 2).sum().alias('favorites_30d'),\n",
    "        (pl.col('action_type_id') == 5).sum().alias('cart_adds_30d'),\n",
    "        \n",
    "        pl.sum('discount_price').alias('total_spent_30d'),\n",
    "        pl.mean('discount_price').alias('avg_price_30d'),\n",
    "        \n",
    "        (test_start_date - pl.col('timestamp').max().dt.date()).dt.total_days().alias('days_since_last_action'),\n",
    "        (pl.col('timestamp').max() - pl.col('timestamp').min()).dt.total_days().alias('active_days_30d'),\n",
    "        \n",
    "        pl.col('cluster').n_unique().alias('unique_clusters_30d'),\n",
    "        (pl.col('cluster').value_counts().struct.field('count').max() / pl.count()).alias('main_cluster_ratio_30d')\n",
    "    )\n",
    "    .with_columns(\n",
    "        (pl.col('total_actions_30d') / pl.col('active_days_30d')).alias('daily_actions_rate_30d'),\n",
    "        (pl.col('cart_adds_30d') / pl.col('total_actions_30d')).alias('cart_add_ratio_30d'),\n",
    "        (pl.col('favorites_30d') / pl.col('total_actions_30d')).alias('favorite_ratio_30d')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c2ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_utils import *\n",
    "\n",
    "test_users_submission = (\n",
    "    pl.read_csv(os.path.join(data_path, 'test_users.csv'))\n",
    ")\n",
    "\n",
    "test_df_main = test_users_submission\n",
    "for _, actions_aggs_df in actions_aggs.items():\n",
    "    test_df_main = (\n",
    "        test_df_main\n",
    "        .join(actions_aggs_df, on='user_id', how='left')\n",
    "    )\n",
    "test_df_main = test_df_main.join(val_product_cluster_aggs, on='user_id', how='left')\n",
    "test_df_main = test_df_main.join(test_last_month_features, on='user_id', how='left')\n",
    "\n",
    "test_df_pd = test_df_main.to_pandas()\n",
    "\n",
    "test_df_pd = apply_log_transform(test_df_pd, columns_to_log, drop_original=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89010e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nans filled\n"
     ]
    }
   ],
   "source": [
    "test_df_pd = add_pca_columns(test_df_pd,  pca_cols,  n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95d7ef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nans filled\n",
      "Data scaled\n",
      "Using CPU\n",
      "FAISS index built\n",
      "KNN search done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN features created\n"
     ]
    }
   ],
   "source": [
    "test_df_pd = add_knn_features_faiss(test_df_pd, knn_cols, n_neighbors=5, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1535c2e6",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be644220",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ы' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ы\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ы' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e1e3c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training catboost...\n",
      "Training lgbm...\n",
      "[LightGBM] [Info] Number of positive: 518271, number of negative: 981693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24857\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499964, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345522 -> initscore=-0.638780\n",
      "[LightGBM] [Info] Start training from score -0.638780\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 518137, number of negative: 981828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.122892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24841\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345433 -> initscore=-0.639176\n",
      "[LightGBM] [Info] Start training from score -0.639176\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 518198, number of negative: 981767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24847\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345473 -> initscore=-0.638997\n",
      "[LightGBM] [Info] Start training from score -0.638997\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 517856, number of negative: 982109\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24856\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345245 -> initscore=-0.640005\n",
      "[LightGBM] [Info] Start training from score -0.640005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 517838, number of negative: 982127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.284011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345233 -> initscore=-0.640058\n",
      "[LightGBM] [Info] Start training from score -0.640058\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training lgbm_deep...\n",
      "[LightGBM] [Info] Number of positive: 518271, number of negative: 981693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24857\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499964, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345522 -> initscore=-0.638780\n",
      "[LightGBM] [Info] Start training from score -0.638780\n",
      "[LightGBM] [Info] Number of positive: 518137, number of negative: 981828\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24841\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345433 -> initscore=-0.639176\n",
      "[LightGBM] [Info] Start training from score -0.639176\n",
      "[LightGBM] [Info] Number of positive: 518198, number of negative: 981767\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.299827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24847\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345473 -> initscore=-0.638997\n",
      "[LightGBM] [Info] Start training from score -0.638997\n",
      "[LightGBM] [Info] Number of positive: 517856, number of negative: 982109\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.267454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24856\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345245 -> initscore=-0.640005\n",
      "[LightGBM] [Info] Start training from score -0.640005\n",
      "[LightGBM] [Info] Number of positive: 517838, number of negative: 982127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266582 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345233 -> initscore=-0.640058\n",
      "[LightGBM] [Info] Start training from score -0.640058\n",
      "Training lgbm_fast...\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 518271, number of negative: 981693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24857\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499964, number of used features: 124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345522 -> initscore=-0.638780\n",
      "[LightGBM] [Info] Start training from score -0.638780\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 518137, number of negative: 981828\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24841\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345433 -> initscore=-0.639176\n",
      "[LightGBM] [Info] Start training from score -0.639176\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 518198, number of negative: 981767\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.117443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24847\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345473 -> initscore=-0.638997\n",
      "[LightGBM] [Info] Start training from score -0.638997\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 517856, number of negative: 982109\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.292108 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24856\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345245 -> initscore=-0.640005\n",
      "[LightGBM] [Info] Start training from score -0.640005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Number of positive: 517838, number of negative: 982127\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.121214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345233 -> initscore=-0.640058\n",
      "[LightGBM] [Info] Start training from score -0.640058\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "Training lgbm_goss...\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 518271, number of negative: 981693\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.288058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24857\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499964, number of used features: 124\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345522 -> initscore=-0.638780\n",
      "[LightGBM] [Info] Start training from score -0.638780\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 518137, number of negative: 981828\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115518 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24841\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345433 -> initscore=-0.639176\n",
      "[LightGBM] [Info] Start training from score -0.639176\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 518198, number of negative: 981767\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.119476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24847\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345473 -> initscore=-0.638997\n",
      "[LightGBM] [Info] Start training from score -0.638997\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 517856, number of negative: 982109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.118615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 24856\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345245 -> initscore=-0.640005\n",
      "[LightGBM] [Info] Start training from score -0.640005\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] Number of positive: 517838, number of negative: 982127\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266988 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 24854\n",
      "[LightGBM] [Info] Number of data points in the train set: 1499965, number of used features: 124\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345233 -> initscore=-0.640058\n",
      "[LightGBM] [Info] Start training from score -0.640058\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "[LightGBM] [Warning] Found boosting=goss. For backwards compatibility reasons, LightGBM interprets this as boosting=gbdt, data_sample_strategy=goss.To suppress this warning, set data_sample_strategy=goss instead.\n",
      "Training catboost_balanced...\n",
      "Training catboost_tuned...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# 1. Полностью очищаем данные от временных столбцов и нечисловых типов\n",
    "cols = list(set(df_pd.columns) - {'user_id', 'target', 'last_click_time', 'first_click_time', 'last_favorite_time', 'first_favorite_time', \n",
    "                                  'last_order_time', 'first_order_time', 'last_to_cart_time', 'first_to_cart_time', 'last_search_time', 'first_search_time',\n",
    "                                  'top3_search_clusters', 'top3_search_counts', 'search_cluster_entropy', 'top3_product_counts', 'product_cluster_entropy', 'top3_product_clusters'})\n",
    "\n",
    "# 2. Явно преобразуем все данные в float и заменяем оставшиеся NaT/NaN\n",
    "X_train = df_pd[cols].astype(float).values\n",
    "y_train = df_pd['target'].values\n",
    "\n",
    "X_test = test_df_pd[cols].astype(float).values\n",
    "\n",
    "# Базовые модели с поддержкой NaN\n",
    "base_models = {\n",
    "    \"catboost\": CatBoostClassifier(\n",
    "        iterations=150, learning_rate=0.05, depth=5, random_state=42,\n",
    "        verbose=0, allow_writing_files=False\n",
    "    ),\n",
    "    \"lgbm\": lgb.LGBMClassifier(\n",
    "        n_estimators=150, learning_rate=0.05, max_depth=5, random_state=42\n",
    "    ),\n",
    "    # Новые модели с совершенно другими параметрами\n",
    "    \"lgbm_deep\": lgb.LGBMClassifier(\n",
    "        n_estimators=300, learning_rate=0.01, max_depth=10, num_leaves=64,\n",
    "        min_child_samples=20, reg_alpha=0.1, reg_lambda=0.1,\n",
    "        subsample=0.8, colsample_bytree=0.7, random_state=42\n",
    "    ),\n",
    "    \"lgbm_fast\": lgb.LGBMClassifier(\n",
    "        n_estimators=80, learning_rate=0.2, max_depth=3,\n",
    "        min_data_in_leaf=10, boosting_type='dart',\n",
    "        random_state=42, bagging_freq=1, bagging_fraction=0.9\n",
    "    ),\n",
    "    \"lgbm_goss\": lgb.LGBMClassifier(\n",
    "        boosting_type='goss',\n",
    "        n_estimators=200, learning_rate=0.02,\n",
    "        max_depth=7, num_leaves=50,\n",
    "        top_rate=0.2, other_rate=0.1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"catboost_balanced\": CatBoostClassifier(\n",
    "        iterations=200, learning_rate=0.03, depth=8,\n",
    "        l2_leaf_reg=5, scale_pos_weight=(len(y_train)-sum(y_train))/sum(y_train),\n",
    "        random_seed=42, verbose=0, allow_writing_files=False,\n",
    "        grow_policy='Lossguide'\n",
    "    ),\n",
    "    \"catboost_tuned\": CatBoostClassifier(\n",
    "        iterations=100, learning_rate=0.1, depth=4,\n",
    "        border_count=128, random_strength=0.5,\n",
    "        bagging_temperature=0.8, od_type='Iter',\n",
    "        od_wait=50, random_seed=42, verbose=0\n",
    "    )\n",
    "}\n",
    "\n",
    "# Подготовка OOF-прогнозов\n",
    "n_models = len(base_models)\n",
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "\n",
    "meta_train = np.zeros((n_train, n_models))\n",
    "meta_test = np.zeros((n_test, n_models))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (name, model) in enumerate(base_models.items()):\n",
    "    print(f\"Training {name}...\")\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        if name.startswith('lgbm'):\n",
    "            model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)])\n",
    "        elif name.startswith('catboost'):\n",
    "            model.fit(X_tr, y_tr, eval_set=(X_val, y_val), verbose=0)\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "        \n",
    "        meta_train[val_idx, i] = model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "    \n",
    "    try:\n",
    "        meta_test[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {name} predict: {str(e)}\")\n",
    "        meta_test[:, i] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48f4fa4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21186571, 0.17236895, 0.18420134, ..., 0.18192606, 0.27707778,\n",
       "        0.17589141],\n",
       "       [0.68631946, 0.78158597, 0.74764962, ..., 0.76131111, 0.82645268,\n",
       "        0.6579162 ],\n",
       "       [0.25964519, 0.21470835, 0.22364766, ..., 0.24045381, 0.34127315,\n",
       "        0.21091497],\n",
       "       ...,\n",
       "       [0.61983921, 0.63479345, 0.56406308, ..., 0.59530627, 0.73613724,\n",
       "        0.64297975],\n",
       "       [0.50364527, 0.52507877, 0.52562602, ..., 0.52954635, 0.67915716,\n",
       "        0.52082136],\n",
       "       [0.4883099 , 0.50563655, 0.50518336, ..., 0.52611808, 0.66105509,\n",
       "        0.50868375]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52d41a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-04 20:13:23,364] A new study created in memory with name: no-name-30de6478-0393-497e-9f6d-5949957239cb\n",
      "[I 2025-04-04 20:13:37,149] Trial 0 finished with value: 0.759961494263126 and parameters: {'learning_rate': 0.00986099751972483, 'num_leaves': 153, 'max_depth': 3, 'min_child_samples': 12, 'subsample': 0.5609515483773001, 'colsample_bytree': 0.7121200852634904, 'reg_alpha': 0.4709557756637617, 'reg_lambda': 4.868078072391354}. Best is trial 0 with value: 0.759961494263126.\n",
      "[I 2025-04-04 20:13:59,084] Trial 1 finished with value: 0.7602531098355813 and parameters: {'learning_rate': 0.0072400055643950575, 'num_leaves': 176, 'max_depth': 7, 'min_child_samples': 92, 'subsample': 0.541197862106498, 'colsample_bytree': 0.5291111180254446, 'reg_alpha': 8.70224153237558, 'reg_lambda': 9.228144134812505}. Best is trial 1 with value: 0.7602531098355813.\n",
      "[I 2025-04-04 20:14:06,338] Trial 2 finished with value: 0.7600513580609557 and parameters: {'learning_rate': 0.002502939792935633, 'num_leaves': 236, 'max_depth': 12, 'min_child_samples': 51, 'subsample': 0.5056220057309267, 'colsample_bytree': 0.9565833929243528, 'reg_alpha': 1.4589755912613822, 'reg_lambda': 1.1045862502088122}. Best is trial 1 with value: 0.7602531098355813.\n",
      "[I 2025-04-04 20:14:24,243] Trial 3 finished with value: 0.7603823478255322 and parameters: {'learning_rate': 0.04076159826481596, 'num_leaves': 37, 'max_depth': 12, 'min_child_samples': 80, 'subsample': 0.8986670182073726, 'colsample_bytree': 0.9540784349229545, 'reg_alpha': 3.7952981076373593, 'reg_lambda': 0.30049330242276406}. Best is trial 3 with value: 0.7603823478255322.\n",
      "[I 2025-04-04 20:14:31,632] Trial 4 finished with value: 0.7601390279429824 and parameters: {'learning_rate': 0.004736478401397122, 'num_leaves': 219, 'max_depth': 10, 'min_child_samples': 18, 'subsample': 0.6131407773713631, 'colsample_bytree': 0.9747615365801592, 'reg_alpha': 1.5335970979323077, 'reg_lambda': 3.0195859615249896}. Best is trial 3 with value: 0.7603823478255322.\n",
      "[I 2025-04-04 20:14:49,119] Trial 5 finished with value: 0.7604282345667098 and parameters: {'learning_rate': 0.04508323124311318, 'num_leaves': 253, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.5642069660762108, 'colsample_bytree': 0.9917041975143336, 'reg_alpha': 9.276617915524, 'reg_lambda': 5.063580639402161}. Best is trial 5 with value: 0.7604282345667098.\n",
      "[I 2025-04-04 20:14:54,616] Trial 6 finished with value: 0.7603474496510891 and parameters: {'learning_rate': 0.013029907172152069, 'num_leaves': 57, 'max_depth': 6, 'min_child_samples': 24, 'subsample': 0.812926141268271, 'colsample_bytree': 0.7190936347684829, 'reg_alpha': 1.0763679583969943, 'reg_lambda': 6.495620656157714}. Best is trial 5 with value: 0.7604282345667098.\n",
      "[I 2025-04-04 20:15:00,255] Trial 7 finished with value: 0.7601939528057743 and parameters: {'learning_rate': 0.0011097100097874771, 'num_leaves': 120, 'max_depth': 7, 'min_child_samples': 51, 'subsample': 0.8866179341724637, 'colsample_bytree': 0.5199802261584162, 'reg_alpha': 4.169670084712323, 'reg_lambda': 8.616311999019729}. Best is trial 5 with value: 0.7604282345667098.\n",
      "[I 2025-04-04 20:15:06,204] Trial 8 finished with value: 0.7603790298383081 and parameters: {'learning_rate': 0.03059449210375629, 'num_leaves': 97, 'max_depth': 12, 'min_child_samples': 28, 'subsample': 0.7615418399881637, 'colsample_bytree': 0.6551415112890523, 'reg_alpha': 1.1331975782190362, 'reg_lambda': 1.7269438722789066}. Best is trial 5 with value: 0.7604282345667098.\n",
      "[I 2025-04-04 20:15:11,933] Trial 9 finished with value: 0.7604258479955291 and parameters: {'learning_rate': 0.017847300539137658, 'num_leaves': 247, 'max_depth': 7, 'min_child_samples': 52, 'subsample': 0.5497789805317872, 'colsample_bytree': 0.6187773212407387, 'reg_alpha': 3.8620028432064366, 'reg_lambda': 9.538136216491}. Best is trial 5 with value: 0.7604282345667098.\n",
      "[I 2025-04-04 20:15:16,097] Trial 10 finished with value: 0.7603410170376059 and parameters: {'learning_rate': 0.07089074613305735, 'num_leaves': 296, 'max_depth': 4, 'min_child_samples': 71, 'subsample': 0.6694059977272873, 'colsample_bytree': 0.8514159432588463, 'reg_alpha': 9.787116201764585, 'reg_lambda': 5.964901729475982}. Best is trial 5 with value: 0.7604282345667098.\n",
      "[I 2025-04-04 20:15:21,034] Trial 11 finished with value: 0.7603113834940473 and parameters: {'learning_rate': 0.021911262945360235, 'num_leaves': 299, 'max_depth': 5, 'min_child_samples': 41, 'subsample': 0.6779711881274973, 'colsample_bytree': 0.6064353122897402, 'reg_alpha': 7.225843576829338, 'reg_lambda': 7.613242965994122}. Best is trial 5 with value: 0.7604282345667098.\n",
      "[I 2025-04-04 20:15:26,906] Trial 12 finished with value: 0.7602163536731913 and parameters: {'learning_rate': 0.09551485078540653, 'num_leaves': 232, 'max_depth': 9, 'min_child_samples': 67, 'subsample': 0.6305267693035101, 'colsample_bytree': 0.8306844802673676, 'reg_alpha': 6.006153275669158, 'reg_lambda': 4.069387931228842}. Best is trial 5 with value: 0.7604282345667098.\n",
      "[I 2025-04-04 20:15:33,876] Trial 13 finished with value: 0.7603296888824307 and parameters: {'learning_rate': 0.02141837656874492, 'num_leaves': 258, 'max_depth': 9, 'min_child_samples': 38, 'subsample': 0.5892298000046696, 'colsample_bytree': 0.8215827585482378, 'reg_alpha': 3.1126441835037086, 'reg_lambda': 7.436989832437545}. Best is trial 5 with value: 0.7604282345667098.\n",
      "[I 2025-04-04 20:15:50,808] Trial 14 finished with value: 0.760451774929671 and parameters: {'learning_rate': 0.051076830301917196, 'num_leaves': 201, 'max_depth': 5, 'min_child_samples': 5, 'subsample': 0.9790386294763549, 'colsample_bytree': 0.6094323422707668, 'reg_alpha': 6.283996927509508, 'reg_lambda': 9.848844959143278}. Best is trial 14 with value: 0.760451774929671.\n",
      "[I 2025-04-04 20:16:06,517] Trial 15 finished with value: 0.7604868358411938 and parameters: {'learning_rate': 0.049790767614631665, 'num_leaves': 192, 'max_depth': 5, 'min_child_samples': 5, 'subsample': 0.9976440345754102, 'colsample_bytree': 0.901722997835807, 'reg_alpha': 7.056435880186102, 'reg_lambda': 3.117700650996915}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:10,190] Trial 16 finished with value: 0.7602477694090416 and parameters: {'learning_rate': 0.06880807073172346, 'num_leaves': 187, 'max_depth': 3, 'min_child_samples': 5, 'subsample': 0.9931364514726135, 'colsample_bytree': 0.8845010088425284, 'reg_alpha': 6.7579180571020885, 'reg_lambda': 2.5912988032772377}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:14,913] Trial 17 finished with value: 0.7604099459632022 and parameters: {'learning_rate': 0.04523980387693162, 'num_leaves': 136, 'max_depth': 5, 'min_child_samples': 7, 'subsample': 0.9996998844042019, 'colsample_bytree': 0.7765341593558691, 'reg_alpha': 5.502575323373838, 'reg_lambda': 2.9582105936088734}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:18,977] Trial 18 finished with value: 0.7604810596337344 and parameters: {'learning_rate': 0.09641960215089601, 'num_leaves': 196, 'max_depth': 4, 'min_child_samples': 22, 'subsample': 0.9063214006714004, 'colsample_bytree': 0.8901725024466334, 'reg_alpha': 7.7861118013565624, 'reg_lambda': 3.6724999467917776}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:23,094] Trial 19 finished with value: 0.7603940940407181 and parameters: {'learning_rate': 0.08839321356504228, 'num_leaves': 104, 'max_depth': 4, 'min_child_samples': 20, 'subsample': 0.9206628398606193, 'colsample_bytree': 0.906433804835717, 'reg_alpha': 8.112838342311049, 'reg_lambda': 4.159755096283191}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:28,489] Trial 20 finished with value: 0.7603748621132663 and parameters: {'learning_rate': 0.029485161681591373, 'num_leaves': 158, 'max_depth': 6, 'min_child_samples': 28, 'subsample': 0.8401506819981692, 'colsample_bytree': 0.9080897733863583, 'reg_alpha': 7.810621047526111, 'reg_lambda': 2.0617347027017177}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:32,691] Trial 21 finished with value: 0.7603199982913419 and parameters: {'learning_rate': 0.05509526842353942, 'num_leaves': 197, 'max_depth': 4, 'min_child_samples': 13, 'subsample': 0.93915176720896, 'colsample_bytree': 0.8095141384585613, 'reg_alpha': 6.57396587099437, 'reg_lambda': 4.108176688233382}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:37,465] Trial 22 finished with value: 0.7604388336198148 and parameters: {'learning_rate': 0.09476323792126773, 'num_leaves': 206, 'max_depth': 6, 'min_child_samples': 5, 'subsample': 0.9560598034241996, 'colsample_bytree': 0.7624070384091401, 'reg_alpha': 5.320102133161905, 'reg_lambda': 3.3888310729808486}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:41,275] Trial 23 finished with value: 0.7600847448476652 and parameters: {'learning_rate': 0.03363535925876648, 'num_leaves': 172, 'max_depth': 3, 'min_child_samples': 16, 'subsample': 0.8476037257191986, 'colsample_bytree': 0.8728679141234137, 'reg_alpha': 7.632680641239977, 'reg_lambda': 5.64885955213193}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:45,870] Trial 24 finished with value: 0.7604236900692545 and parameters: {'learning_rate': 0.0629237135233329, 'num_leaves': 212, 'max_depth': 5, 'min_child_samples': 33, 'subsample': 0.9621183189231877, 'colsample_bytree': 0.9246519717758183, 'reg_alpha': 8.568541009758423, 'reg_lambda': 6.845332979418356}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:50,030] Trial 25 finished with value: 0.760328364247323 and parameters: {'learning_rate': 0.0574764148862779, 'num_leaves': 267, 'max_depth': 4, 'min_child_samples': 12, 'subsample': 0.8788699411271407, 'colsample_bytree': 0.6721075246831467, 'reg_alpha': 5.9950745246655455, 'reg_lambda': 1.199813017739388}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:16:56,104] Trial 26 finished with value: 0.7603659229306062 and parameters: {'learning_rate': 0.015192819852158283, 'num_leaves': 140, 'max_depth': 8, 'min_child_samples': 21, 'subsample': 0.786623829374384, 'colsample_bytree': 0.5578566303560712, 'reg_alpha': 4.8235623775080345, 'reg_lambda': 8.238328419339258}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:01,462] Trial 27 finished with value: 0.7604006495899744 and parameters: {'learning_rate': 0.02598947387509798, 'num_leaves': 188, 'max_depth': 6, 'min_child_samples': 9, 'subsample': 0.9683607245243692, 'colsample_bytree': 0.7826355184371878, 'reg_alpha': 6.822986090512574, 'reg_lambda': 4.909257355386007}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:05,855] Trial 28 finished with value: 0.7600006729911813 and parameters: {'learning_rate': 0.004286918045708471, 'num_leaves': 278, 'max_depth': 4, 'min_child_samples': 44, 'subsample': 0.9252808170145674, 'colsample_bytree': 0.7198042762540613, 'reg_alpha': 8.66427683493898, 'reg_lambda': 3.6462962673684016}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:09,654] Trial 29 finished with value: 0.7599280216831673 and parameters: {'learning_rate': 0.00781176755017048, 'num_leaves': 160, 'max_depth': 3, 'min_child_samples': 14, 'subsample': 0.7141041140060874, 'colsample_bytree': 0.6697775610024377, 'reg_alpha': 6.156052752872583, 'reg_lambda': 2.2081977956505234}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:14,565] Trial 30 finished with value: 0.76014494394299 and parameters: {'learning_rate': 0.010444870339958891, 'num_leaves': 225, 'max_depth': 5, 'min_child_samples': 27, 'subsample': 0.8539145400182376, 'colsample_bytree': 0.5806435036246784, 'reg_alpha': 7.360064763205451, 'reg_lambda': 9.96792790102488}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:19,229] Trial 31 finished with value: 0.7603650634364657 and parameters: {'learning_rate': 0.09945504338115446, 'num_leaves': 208, 'max_depth': 6, 'min_child_samples': 7, 'subsample': 0.9586459778389856, 'colsample_bytree': 0.7558423156758723, 'reg_alpha': 5.577046091189424, 'reg_lambda': 3.9865358515423566}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:24,219] Trial 32 finished with value: 0.7604330400864959 and parameters: {'learning_rate': 0.07472946174864702, 'num_leaves': 195, 'max_depth': 6, 'min_child_samples': 5, 'subsample': 0.9994303290921102, 'colsample_bytree': 0.858673047048326, 'reg_alpha': 5.199968635949844, 'reg_lambda': 3.185626990102869}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:30,389] Trial 33 finished with value: 0.7604432309663117 and parameters: {'learning_rate': 0.04158110011896012, 'num_leaves': 175, 'max_depth': 7, 'min_child_samples': 98, 'subsample': 0.9423222136945031, 'colsample_bytree': 0.9380240198998381, 'reg_alpha': 2.8488670732124937, 'reg_lambda': 4.824900618666123}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:37,131] Trial 34 finished with value: 0.7603559863015192 and parameters: {'learning_rate': 0.037674976068117004, 'num_leaves': 173, 'max_depth': 8, 'min_child_samples': 88, 'subsample': 0.9161083816919903, 'colsample_bytree': 0.9531710205979514, 'reg_alpha': 2.1666615962824674, 'reg_lambda': 5.583021747205754}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:43,095] Trial 35 finished with value: 0.7604284616009985 and parameters: {'learning_rate': 0.051726236214224106, 'num_leaves': 144, 'max_depth': 7, 'min_child_samples': 67, 'subsample': 0.9718590831753446, 'colsample_bytree': 0.9316041837162, 'reg_alpha': 2.679474272012078, 'reg_lambda': 4.608916065368862}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:48,176] Trial 36 finished with value: 0.7604496342163235 and parameters: {'learning_rate': 0.040131973180313835, 'num_leaves': 173, 'max_depth': 5, 'min_child_samples': 97, 'subsample': 0.8979782837902305, 'colsample_bytree': 0.9808344961551327, 'reg_alpha': 4.71440555249227, 'reg_lambda': 4.5647420393104925}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:52,074] Trial 37 finished with value: 0.7602646153467599 and parameters: {'learning_rate': 0.07337143948154491, 'num_leaves': 241, 'max_depth': 3, 'min_child_samples': 84, 'subsample': 0.8978741678481843, 'colsample_bytree': 0.9957128666901107, 'reg_alpha': 4.592355919128639, 'reg_lambda': 1.2364321649798198}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:17:57,336] Trial 38 finished with value: 0.7597435803605731 and parameters: {'learning_rate': 0.0012715557527766532, 'num_leaves': 116, 'max_depth': 5, 'min_child_samples': 75, 'subsample': 0.8715521660574987, 'colsample_bytree': 0.965308329983311, 'reg_alpha': 9.430111441844488, 'reg_lambda': 0.6777848959446011}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:01,675] Trial 39 finished with value: 0.7602833097078521 and parameters: {'learning_rate': 0.0504145773389787, 'num_leaves': 70, 'max_depth': 4, 'min_child_samples': 46, 'subsample': 0.809047621335101, 'colsample_bytree': 0.8943083175903823, 'reg_alpha': 8.180857950129479, 'reg_lambda': 2.4217947863328932}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:06,998] Trial 40 finished with value: 0.7603334565570735 and parameters: {'learning_rate': 0.02520658091017067, 'num_leaves': 219, 'max_depth': 5, 'min_child_samples': 100, 'subsample': 0.914988471271244, 'colsample_bytree': 0.9769273339232512, 'reg_alpha': 4.480841184379273, 'reg_lambda': 6.192616844937783}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:13,221] Trial 41 finished with value: 0.7604332131247173 and parameters: {'learning_rate': 0.04017584011677547, 'num_leaves': 173, 'max_depth': 7, 'min_child_samples': 91, 'subsample': 0.939410901641696, 'colsample_bytree': 0.9322661136512801, 'reg_alpha': 0.41218902946063984, 'reg_lambda': 5.57723924278777}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:18,292] Trial 42 finished with value: 0.7603943931841893 and parameters: {'learning_rate': 0.04058028327321747, 'num_leaves': 184, 'max_depth': 5, 'min_child_samples': 97, 'subsample': 0.9743738184043951, 'colsample_bytree': 0.9469792523310917, 'reg_alpha': 2.9423311258129123, 'reg_lambda': 4.564971083733879}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:25,334] Trial 43 finished with value: 0.7603412804640585 and parameters: {'learning_rate': 0.03420457448192366, 'num_leaves': 148, 'max_depth': 10, 'min_child_samples': 95, 'subsample': 0.945700254107169, 'colsample_bytree': 0.9810002937394591, 'reg_alpha': 3.5503361839525294, 'reg_lambda': 5.111869460854527}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:29,494] Trial 44 finished with value: 0.7603951111893014 and parameters: {'learning_rate': 0.07799253642021335, 'num_leaves': 166, 'max_depth': 4, 'min_child_samples': 78, 'subsample': 0.8968215268073844, 'colsample_bytree': 0.8467180890201325, 'reg_alpha': 2.0158071641790385, 'reg_lambda': 3.6209321807659887}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:36,211] Trial 45 finished with value: 0.7603910714005462 and parameters: {'learning_rate': 0.017693795259757422, 'num_leaves': 201, 'max_depth': 11, 'min_child_samples': 62, 'subsample': 0.9787993039406663, 'colsample_bytree': 0.5005514582766706, 'reg_alpha': 6.984368475398112, 'reg_lambda': 2.819918429496261}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:41,659] Trial 46 finished with value: 0.7604544568884677 and parameters: {'learning_rate': 0.06032607893379614, 'num_leaves': 128, 'max_depth': 6, 'min_child_samples': 56, 'subsample': 0.8723300551213402, 'colsample_bytree': 0.9996582529560804, 'reg_alpha': 6.452337088696297, 'reg_lambda': 5.202263456371474}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:46,718] Trial 47 finished with value: 0.7604333485513121 and parameters: {'learning_rate': 0.05573529741738625, 'num_leaves': 21, 'max_depth': 6, 'min_child_samples': 59, 'subsample': 0.8663054832833481, 'colsample_bytree': 0.9913496007283549, 'reg_alpha': 6.529220314061558, 'reg_lambda': 8.579327917420834}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:51,242] Trial 48 finished with value: 0.760467477433859 and parameters: {'learning_rate': 0.08063384883183729, 'num_leaves': 128, 'max_depth': 5, 'min_child_samples': 33, 'subsample': 0.8266554570757378, 'colsample_bytree': 0.6383926272269468, 'reg_alpha': 5.897846717471646, 'reg_lambda': 0.12076710606360308}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:18:55,356] Trial 49 finished with value: 0.7604479990184796 and parameters: {'learning_rate': 0.08236173240501088, 'num_leaves': 89, 'max_depth': 4, 'min_child_samples': 33, 'subsample': 0.8235781766161494, 'colsample_bytree': 0.619936771590776, 'reg_alpha': 6.047973358241073, 'reg_lambda': 0.3193056711260567}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:00,359] Trial 50 finished with value: 0.7604512643851185 and parameters: {'learning_rate': 0.06407592425432719, 'num_leaves': 133, 'max_depth': 6, 'min_child_samples': 23, 'subsample': 0.7805514048943402, 'colsample_bytree': 0.7086458613807656, 'reg_alpha': 7.300242433728039, 'reg_lambda': 1.7232081785891569}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:05,377] Trial 51 finished with value: 0.7604600762756123 and parameters: {'learning_rate': 0.06816201283303407, 'num_leaves': 126, 'max_depth': 6, 'min_child_samples': 47, 'subsample': 0.7543844619715223, 'colsample_bytree': 0.7199783595258907, 'reg_alpha': 7.362275326067097, 'reg_lambda': 1.4901892393436986}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:10,000] Trial 52 finished with value: 0.7604578827688636 and parameters: {'learning_rate': 0.06351979335142136, 'num_leaves': 127, 'max_depth': 5, 'min_child_samples': 58, 'subsample': 0.7507228819993638, 'colsample_bytree': 0.6338413788834728, 'reg_alpha': 7.8296019673478785, 'reg_lambda': 0.07540192399590939}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:14,721] Trial 53 finished with value: 0.76040342574962 and parameters: {'learning_rate': 0.09897565798566538, 'num_leaves': 107, 'max_depth': 6, 'min_child_samples': 58, 'subsample': 0.742390451937941, 'colsample_bytree': 0.6989019728165626, 'reg_alpha': 8.014647163554919, 'reg_lambda': 0.05897355275617964}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:19,340] Trial 54 finished with value: 0.7604584576320459 and parameters: {'learning_rate': 0.06590296258098867, 'num_leaves': 126, 'max_depth': 5, 'min_child_samples': 49, 'subsample': 0.7171567927492137, 'colsample_bytree': 0.6415944058658123, 'reg_alpha': 7.58440147330269, 'reg_lambda': 0.752090125987172}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:23,704] Trial 55 finished with value: 0.7604378046547469 and parameters: {'learning_rate': 0.08420055092180749, 'num_leaves': 89, 'max_depth': 5, 'min_child_samples': 47, 'subsample': 0.7085801611628187, 'colsample_bytree': 0.6324811806605612, 'reg_alpha': 8.993672233193436, 'reg_lambda': 0.7379568498264815}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:27,721] Trial 56 finished with value: 0.7604188321629307 and parameters: {'learning_rate': 0.07208406579498161, 'num_leaves': 118, 'max_depth': 4, 'min_child_samples': 50, 'subsample': 0.7449956444499033, 'colsample_bytree': 0.6478777130918366, 'reg_alpha': 7.612525241614314, 'reg_lambda': 1.6146329150504823}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:32,382] Trial 57 finished with value: 0.7604169527978017 and parameters: {'learning_rate': 0.047567015137589146, 'num_leaves': 69, 'max_depth': 5, 'min_child_samples': 40, 'subsample': 0.6582325399639759, 'colsample_bytree': 0.5943448074058334, 'reg_alpha': 8.407949431347358, 'reg_lambda': 0.6792329698672159}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:36,859] Trial 58 finished with value: 0.7604632974606588 and parameters: {'learning_rate': 0.06390706487275156, 'num_leaves': 155, 'max_depth': 5, 'min_child_samples': 37, 'subsample': 0.769943637867289, 'colsample_bytree': 0.7363252452835765, 'reg_alpha': 7.024407073685387, 'reg_lambda': 0.027710220772918015}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:40,597] Trial 59 finished with value: 0.7596788350673108 and parameters: {'learning_rate': 0.002523492185993616, 'num_leaves': 154, 'max_depth': 3, 'min_child_samples': 34, 'subsample': 0.7222844397581881, 'colsample_bytree': 0.7427482183851657, 'reg_alpha': 7.027932711201337, 'reg_lambda': 0.9603995699322003}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:44,677] Trial 60 finished with value: 0.7604224891233694 and parameters: {'learning_rate': 0.08370883906071319, 'num_leaves': 101, 'max_depth': 4, 'min_child_samples': 42, 'subsample': 0.6886686002634245, 'colsample_bytree': 0.686282884024519, 'reg_alpha': 7.409440317840054, 'reg_lambda': 1.4068821213978278}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:49,219] Trial 61 finished with value: 0.7604615067227657 and parameters: {'learning_rate': 0.06627986169437473, 'num_leaves': 124, 'max_depth': 5, 'min_child_samples': 38, 'subsample': 0.769662181814579, 'colsample_bytree': 0.7262064528472532, 'reg_alpha': 7.82341506272325, 'reg_lambda': 0.024557872722310387}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:53,694] Trial 62 finished with value: 0.7604629086961188 and parameters: {'learning_rate': 0.06630064114901267, 'num_leaves': 114, 'max_depth': 5, 'min_child_samples': 37, 'subsample': 0.7722149983062508, 'colsample_bytree': 0.7323104152497772, 'reg_alpha': 5.799427103644038, 'reg_lambda': 0.3965628236069443}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:19:57,905] Trial 63 finished with value: 0.7602864832592595 and parameters: {'learning_rate': 0.049788759642779894, 'num_leaves': 114, 'max_depth': 4, 'min_child_samples': 37, 'subsample': 0.7795028875415078, 'colsample_bytree': 0.7264754775158233, 'reg_alpha': 5.891832618880489, 'reg_lambda': 0.38689521052142917}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:20:01,926] Trial 64 finished with value: 0.7604512842205048 and parameters: {'learning_rate': 0.0840841088703656, 'num_leaves': 149, 'max_depth': 4, 'min_child_samples': 30, 'subsample': 0.8082289807062761, 'colsample_bytree': 0.7772789880924137, 'reg_alpha': 5.632066630680432, 'reg_lambda': 0.4600731132873048}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:20:06,547] Trial 65 finished with value: 0.7604400523860129 and parameters: {'learning_rate': 0.05828331735855859, 'num_leaves': 90, 'max_depth': 5, 'min_child_samples': 37, 'subsample': 0.7639408133919567, 'colsample_bytree': 0.8001629492161363, 'reg_alpha': 9.038847738331857, 'reg_lambda': 1.8831665812282763}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:20:11,170] Trial 66 finished with value: 0.7603659524238694 and parameters: {'learning_rate': 0.0996258304155165, 'num_leaves': 135, 'max_depth': 6, 'min_child_samples': 30, 'subsample': 0.8271152966935745, 'colsample_bytree': 0.7385607095469429, 'reg_alpha': 6.787395533674149, 'reg_lambda': 0.01923728713977635}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:20:15,947] Trial 67 finished with value: 0.7603749927082694 and parameters: {'learning_rate': 0.032269464984950195, 'num_leaves': 160, 'max_depth': 5, 'min_child_samples': 18, 'subsample': 0.794775031081549, 'colsample_bytree': 0.6659762723810949, 'reg_alpha': 8.305108751780002, 'reg_lambda': 1.0103668141311706}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:20:21,161] Trial 68 finished with value: 0.7604358844002078 and parameters: {'learning_rate': 0.07018417554257439, 'num_leaves': 74, 'max_depth': 7, 'min_child_samples': 26, 'subsample': 0.7604811568046731, 'colsample_bytree': 0.6861420821398054, 'reg_alpha': 6.398977965645499, 'reg_lambda': 1.5021137186021478}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:20:24,963] Trial 69 finished with value: 0.7601431414519327 and parameters: {'learning_rate': 0.044784255284365165, 'num_leaves': 185, 'max_depth': 3, 'min_child_samples': 40, 'subsample': 0.7325824751079963, 'colsample_bytree': 0.7975056017037577, 'reg_alpha': 7.064451649292472, 'reg_lambda': 2.150386582849876}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:20:30,321] Trial 70 finished with value: 0.7602567922441908 and parameters: {'learning_rate': 0.005596364366115929, 'num_leaves': 165, 'max_depth': 6, 'min_child_samples': 43, 'subsample': 0.5022865591165724, 'colsample_bytree': 0.7319691639405891, 'reg_alpha': 5.106288848624764, 'reg_lambda': 0.4458775924686725}. Best is trial 15 with value: 0.7604868358411938.\n",
      "[I 2025-04-04 20:20:45,446] Trial 71 finished with value: 0.7604881748930931 and parameters: {'learning_rate': 0.06628042182246136, 'num_leaves': 125, 'max_depth': 5, 'min_child_samples': 49, 'subsample': 0.6888176047217951, 'colsample_bytree': 0.7603471601838853, 'reg_alpha': 7.640111949947403, 'reg_lambda': 0.8415198723473465}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:20:49,770] Trial 72 finished with value: 0.7604852261059533 and parameters: {'learning_rate': 0.08807250761765945, 'num_leaves': 112, 'max_depth': 5, 'min_child_samples': 45, 'subsample': 0.6496657658984646, 'colsample_bytree': 0.7614397102430347, 'reg_alpha': 7.922769470812012, 'reg_lambda': 1.062069665848406}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:20:53,753] Trial 73 finished with value: 0.7604519432240383 and parameters: {'learning_rate': 0.08490603749513585, 'num_leaves': 143, 'max_depth': 4, 'min_child_samples': 30, 'subsample': 0.6239641110830322, 'colsample_bytree': 0.7612184617234414, 'reg_alpha': 7.894289391388185, 'reg_lambda': 1.077018522580051}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:20:58,293] Trial 74 finished with value: 0.7604588297314238 and parameters: {'learning_rate': 0.05573608022458242, 'num_leaves': 113, 'max_depth': 5, 'min_child_samples': 52, 'subsample': 0.5889088234304887, 'colsample_bytree': 0.7697642999658082, 'reg_alpha': 6.713445697745151, 'reg_lambda': 0.24974498640795867}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:02,766] Trial 75 finished with value: 0.7604707491396226 and parameters: {'learning_rate': 0.07624431500967956, 'num_leaves': 98, 'max_depth': 5, 'min_child_samples': 36, 'subsample': 0.6560358727216647, 'colsample_bytree': 0.9119241986317714, 'reg_alpha': 8.859715571160175, 'reg_lambda': 0.860039284142635}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:06,855] Trial 76 finished with value: 0.7604410910032944 and parameters: {'learning_rate': 0.09074615185927459, 'num_leaves': 109, 'max_depth': 4, 'min_child_samples': 23, 'subsample': 0.6493773716912284, 'colsample_bytree': 0.8834075549307173, 'reg_alpha': 9.946708990574678, 'reg_lambda': 2.6733029263449506}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:11,356] Trial 77 finished with value: 0.7604683330093639 and parameters: {'learning_rate': 0.07270202454024094, 'num_leaves': 96, 'max_depth': 5, 'min_child_samples': 54, 'subsample': 0.6922344851558166, 'colsample_bytree': 0.8224417363698102, 'reg_alpha': 9.647068494428702, 'reg_lambda': 1.2233099574827981}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:15,487] Trial 78 finished with value: 0.7604323116191681 and parameters: {'learning_rate': 0.0746337374660953, 'num_leaves': 80, 'max_depth': 4, 'min_child_samples': 45, 'subsample': 0.6905111588963141, 'colsample_bytree': 0.8270988338420009, 'reg_alpha': 9.222524152634104, 'reg_lambda': 1.8863872478466033}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:19,894] Trial 79 finished with value: 0.7604610973988525 and parameters: {'learning_rate': 0.09097435390455752, 'num_leaves': 98, 'max_depth': 5, 'min_child_samples': 10, 'subsample': 0.6447353271572519, 'colsample_bytree': 0.906183907212262, 'reg_alpha': 8.88324095510505, 'reg_lambda': 2.4078255137345037}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:23,482] Trial 80 finished with value: 0.7603042170296895 and parameters: {'learning_rate': 0.07721886258673079, 'num_leaves': 79, 'max_depth': 3, 'min_child_samples': 53, 'subsample': 0.6097461612719184, 'colsample_bytree': 0.843134488421615, 'reg_alpha': 9.53640149662814, 'reg_lambda': 1.2517990433923663}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:28,193] Trial 81 finished with value: 0.7604310171662112 and parameters: {'learning_rate': 0.054168232073350156, 'num_leaves': 56, 'max_depth': 5, 'min_child_samples': 54, 'subsample': 0.6701898222227198, 'colsample_bytree': 0.8684709495125088, 'reg_alpha': 8.463717518154544, 'reg_lambda': 0.8186375781497212}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:32,971] Trial 82 finished with value: 0.7604845126484724 and parameters: {'learning_rate': 0.04419527055983025, 'num_leaves': 103, 'max_depth': 5, 'min_child_samples': 36, 'subsample': 0.7014786708700392, 'colsample_bytree': 0.9128692841198054, 'reg_alpha': 9.643513864284898, 'reg_lambda': 0.9860376249608566}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:38,224] Trial 83 finished with value: 0.7604170924285419 and parameters: {'learning_rate': 0.03719942248698203, 'num_leaves': 53, 'max_depth': 6, 'min_child_samples': 33, 'subsample': 0.6955323117668409, 'colsample_bytree': 0.9203828064494971, 'reg_alpha': 9.528303557426469, 'reg_lambda': 1.2442371362649172}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:43,099] Trial 84 finished with value: 0.7603842683957622 and parameters: {'learning_rate': 0.027415271171607834, 'num_leaves': 95, 'max_depth': 5, 'min_child_samples': 15, 'subsample': 0.6735150132968329, 'colsample_bytree': 0.8137906038837991, 'reg_alpha': 9.288988695355927, 'reg_lambda': 0.9260933739844414}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:48,281] Trial 85 finished with value: 0.7604329171775758 and parameters: {'learning_rate': 0.045865400274178354, 'num_leaves': 104, 'max_depth': 6, 'min_child_samples': 49, 'subsample': 0.704329617063252, 'colsample_bytree': 0.8816629086700674, 'reg_alpha': 8.693037980906897, 'reg_lambda': 3.256870719295823}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:52,404] Trial 86 finished with value: 0.7603253952045402 and parameters: {'learning_rate': 0.06021538969214085, 'num_leaves': 231, 'max_depth': 4, 'min_child_samples': 41, 'subsample': 0.6350476750108109, 'colsample_bytree': 0.8994952533949168, 'reg_alpha': 9.794912896015054, 'reg_lambda': 1.9030918355980067}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:21:56,868] Trial 87 finished with value: 0.7604546855392319 and parameters: {'learning_rate': 0.07679852459093682, 'num_leaves': 84, 'max_depth': 5, 'min_child_samples': 31, 'subsample': 0.7266213562359397, 'colsample_bytree': 0.7917143157156266, 'reg_alpha': 8.143930960155895, 'reg_lambda': 0.5895182516082917}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:22:01,164] Trial 88 finished with value: 0.7602884151098438 and parameters: {'learning_rate': 0.051097291020019205, 'num_leaves': 136, 'max_depth': 4, 'min_child_samples': 35, 'subsample': 0.6994790477076878, 'colsample_bytree': 0.5544797098387301, 'reg_alpha': 0.05448282845006336, 'reg_lambda': 1.379833682082929}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:22:05,642] Trial 89 finished with value: 0.7604517295513428 and parameters: {'learning_rate': 0.09117045789267797, 'num_leaves': 97, 'max_depth': 5, 'min_child_samples': 63, 'subsample': 0.6606609811218807, 'colsample_bytree': 0.8657321025734297, 'reg_alpha': 8.802998940447797, 'reg_lambda': 2.3981032444463377}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:22:10,685] Trial 90 finished with value: 0.7604489312928804 and parameters: {'learning_rate': 0.059491751739833273, 'num_leaves': 196, 'max_depth': 6, 'min_child_samples': 20, 'subsample': 0.6823657327100023, 'colsample_bytree': 0.9195566674801915, 'reg_alpha': 9.134032635681798, 'reg_lambda': 4.287602806454769}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:22:15,604] Trial 91 finished with value: 0.7601800444660372 and parameters: {'learning_rate': 0.012267049477490367, 'num_leaves': 120, 'max_depth': 5, 'min_child_samples': 25, 'subsample': 0.5274893570221095, 'colsample_bytree': 0.8432235574909641, 'reg_alpha': 5.441443701876006, 'reg_lambda': 0.5166352414128963}. Best is trial 71 with value: 0.7604881748930931.\n",
      "[I 2025-04-04 20:22:30,849] Trial 92 finished with value: 0.7604899273131108 and parameters: {'learning_rate': 0.06662510200478253, 'num_leaves': 112, 'max_depth': 5, 'min_child_samples': 36, 'subsample': 0.7347292313901483, 'colsample_bytree': 0.8911435158607248, 'reg_alpha': 7.543907405409026, 'reg_lambda': 0.2763181224089397}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:22:35,346] Trial 93 finished with value: 0.760450224416983 and parameters: {'learning_rate': 0.07856282280357194, 'num_leaves': 217, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.7395275021535153, 'colsample_bytree': 0.8978760177844332, 'reg_alpha': 7.517203076791074, 'reg_lambda': 0.24956806492120825}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:22:40,543] Trial 94 finished with value: 0.7604180655758995 and parameters: {'learning_rate': 0.043644377368800516, 'num_leaves': 66, 'max_depth': 5, 'min_child_samples': 44, 'subsample': 0.8563921078914775, 'colsample_bytree': 0.9641553067701916, 'reg_alpha': 7.037453590433062, 'reg_lambda': 1.0945177973293154}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:22:44,671] Trial 95 finished with value: 0.7603431295095536 and parameters: {'learning_rate': 0.07196784077477693, 'num_leaves': 108, 'max_depth': 4, 'min_child_samples': 40, 'subsample': 0.6605816163834849, 'colsample_bytree': 0.9129437656290799, 'reg_alpha': 7.207672627164223, 'reg_lambda': 3.755479618577184}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:22:49,348] Trial 96 finished with value: 0.7604384695960827 and parameters: {'learning_rate': 0.09133556514186777, 'num_leaves': 179, 'max_depth': 5, 'min_child_samples': 28, 'subsample': 0.6073049939737578, 'colsample_bytree': 0.9417144735862966, 'reg_alpha': 9.632694092719666, 'reg_lambda': 0.7298907291189594}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:22:55,668] Trial 97 finished with value: 0.7603774008759134 and parameters: {'learning_rate': 0.03651029216617803, 'num_leaves': 131, 'max_depth': 9, 'min_child_samples': 48, 'subsample': 0.6811464606685782, 'colsample_bytree': 0.8561673957953081, 'reg_alpha': 6.276441445130558, 'reg_lambda': 2.980270298111065}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:23:00,666] Trial 98 finished with value: 0.7604477135569593 and parameters: {'learning_rate': 0.06261304656001151, 'num_leaves': 142, 'max_depth': 6, 'min_child_samples': 45, 'subsample': 0.7095665590708751, 'colsample_bytree': 0.8204435710593966, 'reg_alpha': 7.772278885356769, 'reg_lambda': 0.24053937961772365}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:23:06,032] Trial 99 finished with value: 0.7603063977022523 and parameters: {'learning_rate': 0.09941020146560116, 'num_leaves': 104, 'max_depth': 8, 'min_child_samples': 38, 'subsample': 0.9301555728631058, 'colsample_bytree': 0.8769014073642856, 'reg_alpha': 8.599257359781296, 'reg_lambda': 0.9063949385450649}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:23:10,233] Trial 100 finished with value: 0.7603108384756184 and parameters: {'learning_rate': 0.05425361117822938, 'num_leaves': 122, 'max_depth': 4, 'min_child_samples': 56, 'subsample': 0.7956232238533686, 'colsample_bytree': 0.886982782536854, 'reg_alpha': 8.074265078350006, 'reg_lambda': 0.6010570376483466}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:23:14,739] Trial 101 finished with value: 0.7604851199086778 and parameters: {'learning_rate': 0.06713815994824636, 'num_leaves': 111, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.7298544735916189, 'colsample_bytree': 0.7453446066625276, 'reg_alpha': 5.808682773541237, 'reg_lambda': 0.231860125416537}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:23:19,193] Trial 102 finished with value: 0.760463681990379 and parameters: {'learning_rate': 0.0691906991503926, 'num_leaves': 110, 'max_depth': 5, 'min_child_samples': 33, 'subsample': 0.7303257747632389, 'colsample_bytree': 0.7044763986610727, 'reg_alpha': 6.655577012871653, 'reg_lambda': 0.24890097083486257}. Best is trial 92 with value: 0.7604899273131108.\n",
      "[I 2025-04-04 20:23:33,819] Trial 103 finished with value: 0.7604959698475998 and parameters: {'learning_rate': 0.08169745029248386, 'num_leaves': 109, 'max_depth': 5, 'min_child_samples': 33, 'subsample': 0.7314690476439925, 'colsample_bytree': 0.7558926532485852, 'reg_alpha': 6.143735761125286, 'reg_lambda': 1.5966126252431154}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:23:38,243] Trial 104 finished with value: 0.7604643854805074 and parameters: {'learning_rate': 0.08176473451142274, 'num_leaves': 93, 'max_depth': 5, 'min_child_samples': 32, 'subsample': 0.7188843999740545, 'colsample_bytree': 0.7897460054423453, 'reg_alpha': 8.30010654322394, 'reg_lambda': 1.7078556082195355}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:23:42,911] Trial 105 finished with value: 0.7604196603419063 and parameters: {'learning_rate': 0.08802316281005647, 'num_leaves': 84, 'max_depth': 6, 'min_child_samples': 29, 'subsample': 0.6454742988545616, 'colsample_bytree': 0.7517719640571779, 'reg_alpha': 6.251618553026572, 'reg_lambda': 1.1406323118345754}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:23:47,440] Trial 106 finished with value: 0.7602987227760328 and parameters: {'learning_rate': 0.04911426088208266, 'num_leaves': 101, 'max_depth': 4, 'min_child_samples': 35, 'subsample': 0.6995016161489563, 'colsample_bytree': 0.9320097640369575, 'reg_alpha': 5.836477258966888, 'reg_lambda': 1.542305684488422}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:23:51,902] Trial 107 finished with value: 0.7604581194241457 and parameters: {'learning_rate': 0.07627104921560653, 'num_leaves': 118, 'max_depth': 5, 'min_child_samples': 22, 'subsample': 0.6300743187963, 'colsample_bytree': 0.8373089045927923, 'reg_alpha': 4.23543598894125, 'reg_lambda': 2.0478402309270396}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:23:56,899] Trial 108 finished with value: 0.7604249622369351 and parameters: {'learning_rate': 0.06969345649683513, 'num_leaves': 191, 'max_depth': 6, 'min_child_samples': 27, 'subsample': 0.6861660774428603, 'colsample_bytree': 0.8924151980143211, 'reg_alpha': 5.274270977281321, 'reg_lambda': 1.333631456091815}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:01,453] Trial 109 finished with value: 0.7604377359806177 and parameters: {'learning_rate': 0.058584881900297064, 'num_leaves': 88, 'max_depth': 5, 'min_child_samples': 51, 'subsample': 0.7532005863442814, 'colsample_bytree': 0.7730625342834878, 'reg_alpha': 4.95019635730953, 'reg_lambda': 0.8589574108289719}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:05,786] Trial 110 finished with value: 0.7604494396576292 and parameters: {'learning_rate': 0.08156803516456722, 'num_leaves': 99, 'max_depth': 5, 'min_child_samples': 42, 'subsample': 0.9060718429495741, 'colsample_bytree': 0.7628803171419892, 'reg_alpha': 9.783617295547135, 'reg_lambda': 0.5681849736975741}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:10,229] Trial 111 finished with value: 0.760451123703236 and parameters: {'learning_rate': 0.08287488492875945, 'num_leaves': 92, 'max_depth': 5, 'min_child_samples': 18, 'subsample': 0.7171916800231467, 'colsample_bytree': 0.7899861745212942, 'reg_alpha': 8.321717244515888, 'reg_lambda': 1.5584174599488543}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:14,227] Trial 112 finished with value: 0.760442947576772 and parameters: {'learning_rate': 0.09219190096748667, 'num_leaves': 111, 'max_depth': 4, 'min_child_samples': 31, 'subsample': 0.735427520546779, 'colsample_bytree': 0.7457564490726011, 'reg_alpha': 7.515799889338659, 'reg_lambda': 1.7454098640047206}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:18,762] Trial 113 finished with value: 0.7604746096425804 and parameters: {'learning_rate': 0.06621170146025566, 'num_leaves': 204, 'max_depth': 5, 'min_child_samples': 39, 'subsample': 0.7207460816569644, 'colsample_bytree': 0.8091126822676965, 'reg_alpha': 7.977861996119704, 'reg_lambda': 1.0470543260089291}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:23,323] Trial 114 finished with value: 0.760467206250727 and parameters: {'learning_rate': 0.06496427690729649, 'num_leaves': 207, 'max_depth': 5, 'min_child_samples': 40, 'subsample': 0.7067359697598885, 'colsample_bytree': 0.80220297683788, 'reg_alpha': 7.7066942645494825, 'reg_lambda': 0.9856431997020088}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:28,384] Trial 115 finished with value: 0.7604153772254995 and parameters: {'learning_rate': 0.07235598882798505, 'num_leaves': 213, 'max_depth': 6, 'min_child_samples': 38, 'subsample': 0.6762523961949863, 'colsample_bytree': 0.9090686197080914, 'reg_alpha': 9.99004970174413, 'reg_lambda': 0.7429591360347322}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:33,417] Trial 116 finished with value: 0.7601000654287852 and parameters: {'learning_rate': 0.003040837052463763, 'num_leaves': 203, 'max_depth': 5, 'min_child_samples': 43, 'subsample': 0.8879406212460739, 'colsample_bytree': 0.8155941895715209, 'reg_alpha': 9.380780782624862, 'reg_lambda': 1.175790459823966}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:39,420] Trial 117 finished with value: 0.7601994076010451 and parameters: {'learning_rate': 0.00806707342773078, 'num_leaves': 122, 'max_depth': 6, 'min_child_samples': 46, 'subsample': 0.7259175500496744, 'colsample_bytree': 0.9511158173763868, 'reg_alpha': 7.241999952061052, 'reg_lambda': 0.35577435516746236}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:44,093] Trial 118 finished with value: 0.7604046736154656 and parameters: {'learning_rate': 0.05238282842293359, 'num_leaves': 224, 'max_depth': 5, 'min_child_samples': 36, 'subsample': 0.6661300597039864, 'colsample_bytree': 0.8566383031044366, 'reg_alpha': 8.044096977040835, 'reg_lambda': 1.3572629412528237}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:48,437] Trial 119 finished with value: 0.7600918127138351 and parameters: {'learning_rate': 0.020713076176458338, 'num_leaves': 130, 'max_depth': 4, 'min_child_samples': 25, 'subsample': 0.7437048146900715, 'colsample_bytree': 0.9238288620081107, 'reg_alpha': 8.997853253849364, 'reg_lambda': 7.067081982916772}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:53,030] Trial 120 finished with value: 0.7604591657407433 and parameters: {'learning_rate': 0.058682531927864705, 'num_leaves': 115, 'max_depth': 5, 'min_child_samples': 39, 'subsample': 0.6929540937908238, 'colsample_bytree': 0.7140127676048847, 'reg_alpha': 6.508626734869913, 'reg_lambda': 0.5884548478290452}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:24:57,609] Trial 121 finished with value: 0.7604953823016265 and parameters: {'learning_rate': 0.06616795626766261, 'num_leaves': 208, 'max_depth': 5, 'min_child_samples': 40, 'subsample': 0.7096231262162954, 'colsample_bytree': 0.8004417079793414, 'reg_alpha': 7.745641039526182, 'reg_lambda': 0.9289375314598325}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:02,155] Trial 122 finished with value: 0.7604609970153214 and parameters: {'learning_rate': 0.06352402929717266, 'num_leaves': 201, 'max_depth': 5, 'min_child_samples': 44, 'subsample': 0.7105908014119249, 'colsample_bytree': 0.8316494343865525, 'reg_alpha': 6.9024996855019936, 'reg_lambda': 0.8610249241550033}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:07,006] Trial 123 finished with value: 0.7599824969971788 and parameters: {'learning_rate': 0.0012192029876638634, 'num_leaves': 213, 'max_depth': 5, 'min_child_samples': 34, 'subsample': 0.6490563084582559, 'colsample_bytree': 0.8076657019240512, 'reg_alpha': 7.934089671970401, 'reg_lambda': 1.0966572643490027}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:12,697] Trial 124 finished with value: 0.760336340246674 and parameters: {'learning_rate': 0.07612489349334747, 'num_leaves': 104, 'max_depth': 10, 'min_child_samples': 42, 'subsample': 0.7555912353781176, 'colsample_bytree': 0.7733034127761773, 'reg_alpha': 6.096608719602152, 'reg_lambda': 0.20412054880688632}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:17,345] Trial 125 finished with value: 0.7604824949593855 and parameters: {'learning_rate': 0.046323044357718876, 'num_leaves': 192, 'max_depth': 5, 'min_child_samples': 54, 'subsample': 0.9547779421459791, 'colsample_bytree': 0.7848608695984384, 'reg_alpha': 7.362129562703231, 'reg_lambda': 0.5234573641643228}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:22,095] Trial 126 finished with value: 0.7604664525961763 and parameters: {'learning_rate': 0.04232859193687867, 'num_leaves': 190, 'max_depth': 5, 'min_child_samples': 50, 'subsample': 0.9829914704253273, 'colsample_bytree': 0.7806284047132682, 'reg_alpha': 7.408133748047133, 'reg_lambda': 3.5370438965616606}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:26,263] Trial 127 finished with value: 0.7602626892852795 and parameters: {'learning_rate': 0.04632878458585869, 'num_leaves': 179, 'max_depth': 4, 'min_child_samples': 48, 'subsample': 0.9911595536434962, 'colsample_bytree': 0.7836105624774068, 'reg_alpha': 7.664177426355423, 'reg_lambda': 0.4689519908778788}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:31,291] Trial 128 finished with value: 0.760483358884786 and parameters: {'learning_rate': 0.055191110986098635, 'num_leaves': 195, 'max_depth': 6, 'min_child_samples': 55, 'subsample': 0.9565323314339679, 'colsample_bytree': 0.7485009723132932, 'reg_alpha': 8.506316124221748, 'reg_lambda': 2.2232699323819847}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:36,837] Trial 129 finished with value: 0.7604072373189199 and parameters: {'learning_rate': 0.05390455344884591, 'num_leaves': 167, 'max_depth': 7, 'min_child_samples': 56, 'subsample': 0.9511976953533351, 'colsample_bytree': 0.7631409268919879, 'reg_alpha': 8.436663174798012, 'reg_lambda': 2.7650395822813643}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:42,053] Trial 130 finished with value: 0.7604378816565801 and parameters: {'learning_rate': 0.0484315294585132, 'num_leaves': 195, 'max_depth': 6, 'min_child_samples': 64, 'subsample': 0.9307758873293213, 'colsample_bytree': 0.7542998484076895, 'reg_alpha': 7.201813079810031, 'reg_lambda': 2.3402788977280733}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:46,534] Trial 131 finished with value: 0.760432482488984 and parameters: {'learning_rate': 0.06822529066765977, 'num_leaves': 209, 'max_depth': 5, 'min_child_samples': 61, 'subsample': 0.9707936540037453, 'colsample_bytree': 0.7443740875280294, 'reg_alpha': 8.130109063462578, 'reg_lambda': 1.8550374795887024}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:51,328] Trial 132 finished with value: 0.7604266536220167 and parameters: {'learning_rate': 0.03923836418285913, 'num_leaves': 187, 'max_depth': 5, 'min_child_samples': 55, 'subsample': 0.9571716786091731, 'colsample_bytree': 0.7666890163327827, 'reg_alpha': 8.719895374436167, 'reg_lambda': 2.1614048776613233}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:25:55,956] Trial 133 finished with value: 0.7604422882345289 and parameters: {'learning_rate': 0.05659646231796149, 'num_leaves': 201, 'max_depth': 5, 'min_child_samples': 52, 'subsample': 0.9863321623749399, 'colsample_bytree': 0.7556496850576707, 'reg_alpha': 7.92878709692108, 'reg_lambda': 1.249158469249155}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:00,991] Trial 134 finished with value: 0.7604565254325449 and parameters: {'learning_rate': 0.061964205792307075, 'num_leaves': 246, 'max_depth': 6, 'min_child_samples': 8, 'subsample': 0.7190054029636448, 'colsample_bytree': 0.8035708862598608, 'reg_alpha': 7.693006008381024, 'reg_lambda': 3.1640892285190754}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:05,107] Trial 135 finished with value: 0.760434447099529 and parameters: {'learning_rate': 0.08896327209065875, 'num_leaves': 183, 'max_depth': 4, 'min_child_samples': 58, 'subsample': 0.6986485456870886, 'colsample_bytree': 0.8678967351397716, 'reg_alpha': 8.49374162211257, 'reg_lambda': 0.9025495940340373}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:09,938] Trial 136 finished with value: 0.7604245545732583 and parameters: {'learning_rate': 0.030284569854219855, 'num_leaves': 221, 'max_depth': 5, 'min_child_samples': 54, 'subsample': 0.9664818869481919, 'colsample_bytree': 0.7854103147292961, 'reg_alpha': 8.173756079718116, 'reg_lambda': 1.4828063820311292}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:16,608] Trial 137 finished with value: 0.7602616396378817 and parameters: {'learning_rate': 0.07100706902242908, 'num_leaves': 195, 'max_depth': 11, 'min_child_samples': 47, 'subsample': 0.7360739040303892, 'colsample_bytree': 0.9040569026622696, 'reg_alpha': 8.869359758881961, 'reg_lambda': 0.7072140297883625}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:21,036] Trial 138 finished with value: 0.7604583163309752 and parameters: {'learning_rate': 0.09725894571695362, 'num_leaves': 215, 'max_depth': 5, 'min_child_samples': 36, 'subsample': 0.6771477670338749, 'colsample_bytree': 0.8228717272246301, 'reg_alpha': 7.499582769058117, 'reg_lambda': 1.1047174817758032}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:25,782] Trial 139 finished with value: 0.7604354971590443 and parameters: {'learning_rate': 0.049492073852205415, 'num_leaves': 107, 'max_depth': 5, 'min_child_samples': 59, 'subsample': 0.7266548592855804, 'colsample_bytree': 0.7482681042783609, 'reg_alpha': 7.850531340281991, 'reg_lambda': 1.67280144468862}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:29,946] Trial 140 finished with value: 0.7604204218238575 and parameters: {'learning_rate': 0.07684980450782979, 'num_leaves': 206, 'max_depth': 4, 'min_child_samples': 39, 'subsample': 0.9431805324189619, 'colsample_bytree': 0.7952078008556762, 'reg_alpha': 7.221926811904941, 'reg_lambda': 3.85669129128813}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:34,426] Trial 141 finished with value: 0.7604584574561898 and parameters: {'learning_rate': 0.08259438936773102, 'num_leaves': 232, 'max_depth': 5, 'min_child_samples': 32, 'subsample': 0.6861837944780218, 'colsample_bytree': 0.9148729158506266, 'reg_alpha': 5.639793424409612, 'reg_lambda': 0.015840365642469822}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:38,965] Trial 142 finished with value: 0.7604561170157235 and parameters: {'learning_rate': 0.06829919951907482, 'num_leaves': 118, 'max_depth': 5, 'min_child_samples': 41, 'subsample': 0.9098905652189264, 'colsample_bytree': 0.5379180150943565, 'reg_alpha': 9.2864010676655, 'reg_lambda': 0.33943301897681233}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:43,485] Trial 143 finished with value: 0.7603985386468389 and parameters: {'learning_rate': 0.061171722899680245, 'num_leaves': 97, 'max_depth': 5, 'min_child_samples': 10, 'subsample': 0.6195922879062523, 'colsample_bytree': 0.724784272729169, 'reg_alpha': 6.72588652075169, 'reg_lambda': 0.5414600663458484}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:26:48,337] Trial 144 finished with value: 0.7600566690648004 and parameters: {'learning_rate': 0.0016304791762622394, 'num_leaves': 192, 'max_depth': 5, 'min_child_samples': 34, 'subsample': 0.710236293720908, 'colsample_bytree': 0.7786692165134688, 'reg_alpha': 9.730421418890522, 'reg_lambda': 0.8104985760017515}. Best is trial 103 with value: 0.7604959698475998.\n",
      "[I 2025-04-04 20:27:04,964] Trial 145 finished with value: 0.7605087447829039 and parameters: {'learning_rate': 0.07774326184889872, 'num_leaves': 290, 'max_depth': 5, 'min_child_samples': 37, 'subsample': 0.6581048785724165, 'colsample_bytree': 0.9381416736880159, 'reg_alpha': 8.255666382390975, 'reg_lambda': 0.21322785677956857}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:10,461] Trial 146 finished with value: 0.7604371518906813 and parameters: {'learning_rate': 0.05370279255045013, 'num_leaves': 274, 'max_depth': 6, 'min_child_samples': 50, 'subsample': 0.6419773330439493, 'colsample_bytree': 0.9598363924298069, 'reg_alpha': 8.568671916781229, 'reg_lambda': 0.975355799672231}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:15,263] Trial 147 finished with value: 0.760481050922273 and parameters: {'learning_rate': 0.07344733741532691, 'num_leaves': 263, 'max_depth': 5, 'min_child_samples': 36, 'subsample': 0.6560745305100895, 'colsample_bytree': 0.9387961699399615, 'reg_alpha': 8.299667664439369, 'reg_lambda': 2.5259974559515417}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:19,685] Trial 148 finished with value: 0.7604097092122594 and parameters: {'learning_rate': 0.08795262276151161, 'num_leaves': 290, 'max_depth': 4, 'min_child_samples': 37, 'subsample': 0.9968304072119719, 'colsample_bytree': 0.9382262635688068, 'reg_alpha': 8.274077363265098, 'reg_lambda': 3.115027898301703}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:25,105] Trial 149 finished with value: 0.7604596280973736 and parameters: {'learning_rate': 0.06583458705052422, 'num_leaves': 293, 'max_depth': 6, 'min_child_samples': 39, 'subsample': 0.6536785681345001, 'colsample_bytree': 0.9296034777919712, 'reg_alpha': 7.932084189922762, 'reg_lambda': 2.8958673332915787}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:30,189] Trial 150 finished with value: 0.7604278261275481 and parameters: {'learning_rate': 0.04339371103436206, 'num_leaves': 283, 'max_depth': 5, 'min_child_samples': 36, 'subsample': 0.6676213074988868, 'colsample_bytree': 0.9707766136195642, 'reg_alpha': 7.666108830908855, 'reg_lambda': 2.53805235857633}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:35,014] Trial 151 finished with value: 0.760461933718048 and parameters: {'learning_rate': 0.07404829076374561, 'num_leaves': 278, 'max_depth': 5, 'min_child_samples': 43, 'subsample': 0.5992452482025329, 'colsample_bytree': 0.9460502513779122, 'reg_alpha': 7.427625801100231, 'reg_lambda': 2.593022139799045}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:39,493] Trial 152 finished with value: 0.7604801197050723 and parameters: {'learning_rate': 0.08065238513964068, 'num_leaves': 264, 'max_depth': 5, 'min_child_samples': 29, 'subsample': 0.6662589267898233, 'colsample_bytree': 0.8971101361174457, 'reg_alpha': 8.118168080628443, 'reg_lambda': 0.4091861328616779}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:43,986] Trial 153 finished with value: 0.760406479407556 and parameters: {'learning_rate': 0.08384838012581457, 'num_leaves': 257, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.6613204460972347, 'colsample_bytree': 0.8903947280818705, 'reg_alpha': 8.196296494981231, 'reg_lambda': 0.4213803385309488}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:48,335] Trial 154 finished with value: 0.760449390622373 and parameters: {'learning_rate': 0.09636492603209526, 'num_leaves': 247, 'max_depth': 5, 'min_child_samples': 28, 'subsample': 0.6365769348892426, 'colsample_bytree': 0.9211413788485023, 'reg_alpha': 8.412919337531218, 'reg_lambda': 0.6597506488115905}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:53,035] Trial 155 finished with value: 0.7604654657071787 and parameters: {'learning_rate': 0.05755469512444896, 'num_leaves': 266, 'max_depth': 5, 'min_child_samples': 30, 'subsample': 0.6561642653197646, 'colsample_bytree': 0.9026604617241378, 'reg_alpha': 7.9864134010972485, 'reg_lambda': 2.0091424930800224}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:27:57,593] Trial 156 finished with value: 0.7604830858150095 and parameters: {'learning_rate': 0.07840429800685726, 'num_leaves': 272, 'max_depth': 5, 'min_child_samples': 32, 'subsample': 0.6278286089483954, 'colsample_bytree': 0.8816233115470046, 'reg_alpha': 8.675882207888495, 'reg_lambda': 3.497689423690449}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:02,197] Trial 157 finished with value: 0.7604857577386706 and parameters: {'learning_rate': 0.06672518805103501, 'num_leaves': 268, 'max_depth': 5, 'min_child_samples': 33, 'subsample': 0.62901906484262, 'colsample_bytree': 0.8738014800903857, 'reg_alpha': 7.773420929012669, 'reg_lambda': 3.2589759106763516}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:06,748] Trial 158 finished with value: 0.760473362081303 and parameters: {'learning_rate': 0.07909891217849217, 'num_leaves': 265, 'max_depth': 5, 'min_child_samples': 31, 'subsample': 0.6337881245193847, 'colsample_bytree': 0.8823029779202092, 'reg_alpha': 7.769312720315165, 'reg_lambda': 4.200291346388841}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:10,849] Trial 159 finished with value: 0.760456534088783 and parameters: {'learning_rate': 0.08878312524954267, 'num_leaves': 271, 'max_depth': 4, 'min_child_samples': 33, 'subsample': 0.5702652065713706, 'colsample_bytree': 0.8911351955970603, 'reg_alpha': 7.541948810498822, 'reg_lambda': 2.9764958083265025}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:15,854] Trial 160 finished with value: 0.760456127497469 and parameters: {'learning_rate': 0.06266532796686489, 'num_leaves': 287, 'max_depth': 6, 'min_child_samples': 27, 'subsample': 0.6253025124642383, 'colsample_bytree': 0.7341552957264458, 'reg_alpha': 7.084572254297161, 'reg_lambda': 3.4807400320404174}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:20,407] Trial 161 finished with value: 0.7604880127749759 and parameters: {'learning_rate': 0.06884726736817562, 'num_leaves': 256, 'max_depth': 5, 'min_child_samples': 32, 'subsample': 0.6205392616720964, 'colsample_bytree': 0.8752582956018952, 'reg_alpha': 8.063287756648167, 'reg_lambda': 3.900438913962226}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:24,928] Trial 162 finished with value: 0.7605060852031912 and parameters: {'learning_rate': 0.07015778359460018, 'num_leaves': 261, 'max_depth': 5, 'min_child_samples': 32, 'subsample': 0.6206837538719935, 'colsample_bytree': 0.8751116921916868, 'reg_alpha': 8.57704970009322, 'reg_lambda': 4.375289712583419}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:29,540] Trial 163 finished with value: 0.7604770949093957 and parameters: {'learning_rate': 0.07036848538070307, 'num_leaves': 257, 'max_depth': 5, 'min_child_samples': 13, 'subsample': 0.614963782066889, 'colsample_bytree': 0.8691606484659774, 'reg_alpha': 8.464341202159554, 'reg_lambda': 3.9965261605364084}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:34,320] Trial 164 finished with value: 0.7604487853055456 and parameters: {'learning_rate': 0.05573821813146378, 'num_leaves': 249, 'max_depth': 5, 'min_child_samples': 32, 'subsample': 0.5846033791536458, 'colsample_bytree': 0.8748213415137885, 'reg_alpha': 8.627252216991266, 'reg_lambda': 3.3435225381883815}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:38,979] Trial 165 finished with value: 0.760464329929981 and parameters: {'learning_rate': 0.060131729638270716, 'num_leaves': 260, 'max_depth': 5, 'min_child_samples': 34, 'subsample': 0.6146074269961789, 'colsample_bytree': 0.8488663756174258, 'reg_alpha': 7.350047987463694, 'reg_lambda': 3.6470334991002105}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:43,356] Trial 166 finished with value: 0.7604727745163468 and parameters: {'learning_rate': 0.09958269615743497, 'num_leaves': 278, 'max_depth': 5, 'min_child_samples': 37, 'subsample': 0.5954847308483698, 'colsample_bytree': 0.8655664268464507, 'reg_alpha': 8.237447558647462, 'reg_lambda': 3.885810668801226}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:48,110] Trial 167 finished with value: 0.7604306698500601 and parameters: {'learning_rate': 0.050757541436960714, 'num_leaves': 282, 'max_depth': 5, 'min_child_samples': 31, 'subsample': 0.626292632929364, 'colsample_bytree': 0.8822375911062678, 'reg_alpha': 7.715647746245842, 'reg_lambda': 3.413544819554384}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:52,210] Trial 168 finished with value: 0.760407610800509 and parameters: {'learning_rate': 0.07030707796818074, 'num_leaves': 240, 'max_depth': 4, 'min_child_samples': 24, 'subsample': 0.6409234422631778, 'colsample_bytree': 0.7589231367538887, 'reg_alpha': 8.690286540485452, 'reg_lambda': 3.739709089181791}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:28:56,722] Trial 169 finished with value: 0.7604658955779818 and parameters: {'learning_rate': 0.0760338835052416, 'num_leaves': 300, 'max_depth': 5, 'min_child_samples': 34, 'subsample': 0.6134239089156197, 'colsample_bytree': 0.8619171742676136, 'reg_alpha': 7.850512440166585, 'reg_lambda': 3.196262498500795}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:01,827] Trial 170 finished with value: 0.7604529000413924 and parameters: {'learning_rate': 0.06497018632626303, 'num_leaves': 273, 'max_depth': 6, 'min_child_samples': 29, 'subsample': 0.6047191152973167, 'colsample_bytree': 0.8770712565547142, 'reg_alpha': 8.96439532157591, 'reg_lambda': 2.7293810287918236}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:06,342] Trial 171 finished with value: 0.7604801580597601 and parameters: {'learning_rate': 0.08259737730156952, 'num_leaves': 282, 'max_depth': 5, 'min_child_samples': 29, 'subsample': 0.670301071102941, 'colsample_bytree': 0.9020797062306692, 'reg_alpha': 8.059729517372539, 'reg_lambda': 4.356563602839894}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:10,872] Trial 172 finished with value: 0.7604516508510674 and parameters: {'learning_rate': 0.08435004203531817, 'num_leaves': 261, 'max_depth': 5, 'min_child_samples': 32, 'subsample': 0.6352614770088829, 'colsample_bytree': 0.9116248463055165, 'reg_alpha': 8.332583457212378, 'reg_lambda': 4.916668598541628}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:15,441] Trial 173 finished with value: 0.7604805247173229 and parameters: {'learning_rate': 0.07367427480938998, 'num_leaves': 271, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.6517671387382137, 'colsample_bytree': 0.9015642094045174, 'reg_alpha': 8.076628995135144, 'reg_lambda': 4.609744897016988}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:19,967] Trial 174 finished with value: 0.7604909494349474 and parameters: {'learning_rate': 0.07328222491791526, 'num_leaves': 273, 'max_depth': 5, 'min_child_samples': 17, 'subsample': 0.6470643567085912, 'colsample_bytree': 0.8924315398749676, 'reg_alpha': 7.702447389868099, 'reg_lambda': 4.804906127934687}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:24,672] Trial 175 finished with value: 0.760440077148298 and parameters: {'learning_rate': 0.05994464926920715, 'num_leaves': 255, 'max_depth': 5, 'min_child_samples': 16, 'subsample': 0.6265676432680595, 'colsample_bytree': 0.8901459711395631, 'reg_alpha': 7.603465051420121, 'reg_lambda': 5.268150326527029}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:28,641] Trial 176 finished with value: 0.760231008053212 and parameters: {'learning_rate': 0.06799382811731952, 'num_leaves': 267, 'max_depth': 3, 'min_child_samples': 11, 'subsample': 0.6478885112491883, 'colsample_bytree': 0.929488099161936, 'reg_alpha': 7.473076217906909, 'reg_lambda': 4.42295118901381}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:33,392] Trial 177 finished with value: 0.7604822082777759 and parameters: {'learning_rate': 0.04636716259623469, 'num_leaves': 278, 'max_depth': 5, 'min_child_samples': 19, 'subsample': 0.6332019307239332, 'colsample_bytree': 0.7445312035253644, 'reg_alpha': 6.942932325778598, 'reg_lambda': 4.052129193168004}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:37,775] Trial 178 finished with value: 0.7601869925127568 and parameters: {'learning_rate': 0.03591340716039792, 'num_leaves': 294, 'max_depth': 4, 'min_child_samples': 18, 'subsample': 0.6303948218957444, 'colsample_bytree': 0.7458448595413539, 'reg_alpha': 7.221143569257471, 'reg_lambda': 4.046731139217532}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:42,627] Trial 179 finished with value: 0.760485190758131 and parameters: {'learning_rate': 0.04405126548407602, 'num_leaves': 276, 'max_depth': 5, 'min_child_samples': 14, 'subsample': 0.9776473341776549, 'colsample_bytree': 0.7365495042948812, 'reg_alpha': 6.888956076184914, 'reg_lambda': 3.632676002954889}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:47,468] Trial 180 finished with value: 0.7604728965154405 and parameters: {'learning_rate': 0.04386757985020461, 'num_leaves': 277, 'max_depth': 5, 'min_child_samples': 7, 'subsample': 0.6211245556748278, 'colsample_bytree': 0.7196751651514467, 'reg_alpha': 6.643546022770966, 'reg_lambda': 4.807490941719026}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:52,259] Trial 181 finished with value: 0.7604192798420716 and parameters: {'learning_rate': 0.04007686781236969, 'num_leaves': 270, 'max_depth': 5, 'min_child_samples': 19, 'subsample': 0.9777778585162022, 'colsample_bytree': 0.7696274795053799, 'reg_alpha': 7.03636751552973, 'reg_lambda': 4.222589925102764}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:29:56,927] Trial 182 finished with value: 0.7604040171053715 and parameters: {'learning_rate': 0.048369610723059724, 'num_leaves': 280, 'max_depth': 5, 'min_child_samples': 14, 'subsample': 0.9663773172322127, 'colsample_bytree': 0.732177527660683, 'reg_alpha': 6.956047912148118, 'reg_lambda': 3.612157912668542}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:01,710] Trial 183 finished with value: 0.7603976966964398 and parameters: {'learning_rate': 0.032943887523365446, 'num_leaves': 289, 'max_depth': 5, 'min_child_samples': 21, 'subsample': 0.9850678700170727, 'colsample_bytree': 0.7397699013184876, 'reg_alpha': 6.808536327332878, 'reg_lambda': 3.832925760381114}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:06,330] Trial 184 finished with value: 0.7604860751286159 and parameters: {'learning_rate': 0.05298623178913424, 'num_leaves': 284, 'max_depth': 5, 'min_child_samples': 21, 'subsample': 0.9554894429131159, 'colsample_bytree': 0.7521734097037321, 'reg_alpha': 7.287444095820774, 'reg_lambda': 4.477411190102419}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:11,035] Trial 185 finished with value: 0.7604295225011796 and parameters: {'learning_rate': 0.045814921201379026, 'num_leaves': 288, 'max_depth': 5, 'min_child_samples': 12, 'subsample': 0.9547188679637316, 'colsample_bytree': 0.7551488425404392, 'reg_alpha': 7.33251318637619, 'reg_lambda': 4.04720019682151}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:15,677] Trial 186 finished with value: 0.7604366715394368 and parameters: {'learning_rate': 0.0503280536706998, 'num_leaves': 285, 'max_depth': 5, 'min_child_samples': 5, 'subsample': 0.9369674755057095, 'colsample_bytree': 0.7520016198335423, 'reg_alpha': 6.309318892156372, 'reg_lambda': 4.703651980704967}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:20,245] Trial 187 finished with value: 0.7604726095369756 and parameters: {'learning_rate': 0.05518608660557261, 'num_leaves': 276, 'max_depth': 5, 'min_child_samples': 20, 'subsample': 0.7459256805348513, 'colsample_bytree': 0.7650815040129046, 'reg_alpha': 6.8410842621307575, 'reg_lambda': 4.4066883926454965}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:24,884] Trial 188 finished with value: 0.760440123620171 and parameters: {'learning_rate': 0.05338038739705498, 'num_leaves': 296, 'max_depth': 5, 'min_child_samples': 22, 'subsample': 0.994089616626978, 'colsample_bytree': 0.6921378296449214, 'reg_alpha': 7.139833864203617, 'reg_lambda': 4.0902595284242995}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:29,517] Trial 189 finished with value: 0.7604785345283669 and parameters: {'learning_rate': 0.06011089250043351, 'num_leaves': 251, 'max_depth': 5, 'min_child_samples': 16, 'subsample': 0.9732060430659583, 'colsample_bytree': 0.7405061238412237, 'reg_alpha': 6.456891062265969, 'reg_lambda': 4.489113976688791}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:33,959] Trial 190 finished with value: 0.7604260758157332 and parameters: {'learning_rate': 0.0657050193879223, 'num_leaves': 269, 'max_depth': 5, 'min_child_samples': 16, 'subsample': 0.5847158592805529, 'colsample_bytree': 0.7119507181643719, 'reg_alpha': 7.5801476600028375, 'reg_lambda': 3.4220474148183273}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:38,750] Trial 191 finished with value: 0.7604877778371334 and parameters: {'learning_rate': 0.04114617484460424, 'num_leaves': 113, 'max_depth': 5, 'min_child_samples': 24, 'subsample': 0.9509945614496176, 'colsample_bytree': 0.8817853605055266, 'reg_alpha': 7.817674938867288, 'reg_lambda': 5.396361455203273}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:43,508] Trial 192 finished with value: 0.7604796183310203 and parameters: {'learning_rate': 0.0403405618275554, 'num_leaves': 109, 'max_depth': 5, 'min_child_samples': 25, 'subsample': 0.9612752463144734, 'colsample_bytree': 0.8788282062807892, 'reg_alpha': 7.798531999961976, 'reg_lambda': 5.767773683275299}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:48,155] Trial 193 finished with value: 0.7604155510127515 and parameters: {'learning_rate': 0.04632439065567105, 'num_leaves': 110, 'max_depth': 5, 'min_child_samples': 17, 'subsample': 0.9509971315933412, 'colsample_bytree': 0.7266395481733582, 'reg_alpha': 7.349704804390162, 'reg_lambda': 5.3445996906708935}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:52,843] Trial 194 finished with value: 0.7604176040411641 and parameters: {'learning_rate': 0.04263987974502783, 'num_leaves': 117, 'max_depth': 5, 'min_child_samples': 20, 'subsample': 0.6407318902533617, 'colsample_bytree': 0.749814288598168, 'reg_alpha': 7.536925443402395, 'reg_lambda': 6.131997728833095}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:30:57,504] Trial 195 finished with value: 0.7604684043206308 and parameters: {'learning_rate': 0.05117960668463643, 'num_leaves': 275, 'max_depth': 5, 'min_child_samples': 23, 'subsample': 0.9456708999951736, 'colsample_bytree': 0.7734239989830238, 'reg_alpha': 7.846914996196209, 'reg_lambda': 4.940855769157892}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:02,342] Trial 196 finished with value: 0.7604839858351666 and parameters: {'learning_rate': 0.034999505006155555, 'num_leaves': 102, 'max_depth': 5, 'min_child_samples': 26, 'subsample': 0.9784784763910226, 'colsample_bytree': 0.8592686327408647, 'reg_alpha': 7.320408922475896, 'reg_lambda': 5.196472406648538}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:06,990] Trial 197 finished with value: 0.7604798970969613 and parameters: {'learning_rate': 0.05670431420805407, 'num_leaves': 104, 'max_depth': 5, 'min_child_samples': 24, 'subsample': 0.9792020509284276, 'colsample_bytree': 0.8547361600332183, 'reg_alpha': 7.256458839042473, 'reg_lambda': 5.577761172590772}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:12,506] Trial 198 finished with value: 0.7604081760912639 and parameters: {'learning_rate': 0.0344402434522745, 'num_leaves': 112, 'max_depth': 6, 'min_child_samples': 27, 'subsample': 0.9665816401128396, 'colsample_bytree': 0.8724311525153793, 'reg_alpha': 7.697925706774675, 'reg_lambda': 5.22183173057898}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:17,263] Trial 199 finished with value: 0.760427665296882 and parameters: {'learning_rate': 0.039583613635434424, 'num_leaves': 123, 'max_depth': 5, 'min_child_samples': 33, 'subsample': 0.9891781224668825, 'colsample_bytree': 0.8932065308461435, 'reg_alpha': 7.470628396336907, 'reg_lambda': 5.367418664737186}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:22,165] Trial 200 finished with value: 0.7601924040010877 and parameters: {'learning_rate': 0.013763200177500127, 'num_leaves': 105, 'max_depth': 5, 'min_child_samples': 38, 'subsample': 0.9597310228661738, 'colsample_bytree': 0.8816386752759564, 'reg_alpha': 7.956537198844407, 'reg_lambda': 5.116422109974456}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:27,059] Trial 201 finished with value: 0.7604270431035648 and parameters: {'learning_rate': 0.036570658244163896, 'num_leaves': 285, 'max_depth': 5, 'min_child_samples': 19, 'subsample': 0.7325538659110529, 'colsample_bytree': 0.8621227139080327, 'reg_alpha': 7.119760077098593, 'reg_lambda': 3.786237768318306}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:31,775] Trial 202 finished with value: 0.7603850837329471 and parameters: {'learning_rate': 0.04280514590492926, 'num_leaves': 101, 'max_depth': 5, 'min_child_samples': 26, 'subsample': 0.9224363066602413, 'colsample_bytree': 0.7639917201257526, 'reg_alpha': 6.886149102487394, 'reg_lambda': 4.699785119389451}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:36,474] Trial 203 finished with value: 0.7604271176850247 and parameters: {'learning_rate': 0.0465792494431366, 'num_leaves': 273, 'max_depth': 5, 'min_child_samples': 22, 'subsample': 0.6046895903997848, 'colsample_bytree': 0.7367852039262437, 'reg_alpha': 7.37354780928544, 'reg_lambda': 4.961160023670671}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:41,115] Trial 204 finished with value: 0.7604957331434988 and parameters: {'learning_rate': 0.06290944900585467, 'num_leaves': 117, 'max_depth': 5, 'min_child_samples': 53, 'subsample': 0.9732518793328288, 'colsample_bytree': 0.8836726191831848, 'reg_alpha': 7.641770523268998, 'reg_lambda': 0.20812215195703246}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:45,735] Trial 205 finished with value: 0.7604446426093996 and parameters: {'learning_rate': 0.06354403750076362, 'num_leaves': 114, 'max_depth': 5, 'min_child_samples': 31, 'subsample': 0.9759513460608308, 'colsample_bytree': 0.8909573699580741, 'reg_alpha': 7.692324842357633, 'reg_lambda': 0.2358866290632402}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:50,216] Trial 206 finished with value: 0.7604788894813918 and parameters: {'learning_rate': 0.06848959385048319, 'num_leaves': 130, 'max_depth': 5, 'min_child_samples': 56, 'subsample': 0.995321464104118, 'colsample_bytree': 0.8696144201873265, 'reg_alpha': 7.950872147700088, 'reg_lambda': 0.2868547276310721}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:54,906] Trial 207 finished with value: 0.7604867279540378 and parameters: {'learning_rate': 0.05615258638096454, 'num_leaves': 121, 'max_depth': 5, 'min_child_samples': 53, 'subsample': 0.9703350972970359, 'colsample_bytree': 0.8857531867281706, 'reg_alpha': 7.568860919447985, 'reg_lambda': 0.19091696635155564}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:31:59,551] Trial 208 finished with value: 0.7604247846609065 and parameters: {'learning_rate': 0.05733709068029391, 'num_leaves': 124, 'max_depth': 5, 'min_child_samples': 51, 'subsample': 0.9823483188669384, 'colsample_bytree': 0.8861569502710365, 'reg_alpha': 7.771751270021535, 'reg_lambda': 0.04190810069779671}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:04,182] Trial 209 finished with value: 0.7604607810255993 and parameters: {'learning_rate': 0.06189994544340665, 'num_leaves': 137, 'max_depth': 5, 'min_child_samples': 52, 'subsample': 0.9658704815643082, 'colsample_bytree': 0.9061901475822007, 'reg_alpha': 8.09415761886559, 'reg_lambda': 5.842806736391162}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:09,087] Trial 210 finished with value: 0.7604433482181844 and parameters: {'learning_rate': 0.07845619714601589, 'num_leaves': 116, 'max_depth': 6, 'min_child_samples': 45, 'subsample': 0.9982671240313732, 'colsample_bytree': 0.8754266683133208, 'reg_alpha': 7.629890560336737, 'reg_lambda': 0.6426477830088736}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:13,726] Trial 211 finished with value: 0.7604259582554302 and parameters: {'learning_rate': 0.05376571728475858, 'num_leaves': 120, 'max_depth': 5, 'min_child_samples': 55, 'subsample': 0.9361510664367257, 'colsample_bytree': 0.8546642965602419, 'reg_alpha': 7.4142583373777455, 'reg_lambda': 0.2114246756843093}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:18,314] Trial 212 finished with value: 0.7604789144009828 and parameters: {'learning_rate': 0.07036648652460605, 'num_leaves': 110, 'max_depth': 5, 'min_child_samples': 54, 'subsample': 0.9735119503099258, 'colsample_bytree': 0.8808699236481259, 'reg_alpha': 7.593941133078317, 'reg_lambda': 0.5706243084641174}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:23,018] Trial 213 finished with value: 0.7604700279022684 and parameters: {'learning_rate': 0.05275050685544424, 'num_leaves': 101, 'max_depth': 5, 'min_child_samples': 53, 'subsample': 0.9480713572160018, 'colsample_bytree': 0.8991627917892175, 'reg_alpha': 7.227541518863406, 'reg_lambda': 0.162224674542864}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:27,620] Trial 214 finished with value: 0.7604821036585969 and parameters: {'learning_rate': 0.06260711707481038, 'num_leaves': 113, 'max_depth': 5, 'min_child_samples': 57, 'subsample': 0.9559719501208224, 'colsample_bytree': 0.9156784280038778, 'reg_alpha': 8.328794162369354, 'reg_lambda': 0.502081941490172}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:32,107] Trial 215 finished with value: 0.7604892910729305 and parameters: {'learning_rate': 0.07364753004119094, 'num_leaves': 127, 'max_depth': 5, 'min_child_samples': 50, 'subsample': 0.7050206553452998, 'colsample_bytree': 0.8412835876924127, 'reg_alpha': 7.825177376777527, 'reg_lambda': 0.3823763072445243}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:36,617] Trial 216 finished with value: 0.7604648993767751 and parameters: {'learning_rate': 0.07400273112991626, 'num_leaves': 127, 'max_depth': 5, 'min_child_samples': 50, 'subsample': 0.7044120682743106, 'colsample_bytree': 0.8340874600204736, 'reg_alpha': 7.921295406663212, 'reg_lambda': 0.3255087339890933}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:42,471] Trial 217 finished with value: 0.7603580076733958 and parameters: {'learning_rate': 0.06754964410043272, 'num_leaves': 122, 'max_depth': 8, 'min_child_samples': 49, 'subsample': 0.7252145759053615, 'colsample_bytree': 0.8480081797524188, 'reg_alpha': 8.587624888273252, 'reg_lambda': 0.045018966459055565}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:47,015] Trial 218 finished with value: 0.7604496487363132 and parameters: {'learning_rate': 0.07810477428748544, 'num_leaves': 117, 'max_depth': 5, 'min_child_samples': 41, 'subsample': 0.7157181905464599, 'colsample_bytree': 0.8691612750253629, 'reg_alpha': 7.836654223707532, 'reg_lambda': 5.4366601008279}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:53,309] Trial 219 finished with value: 0.7602824235906788 and parameters: {'learning_rate': 0.005164914438658336, 'num_leaves': 104, 'max_depth': 9, 'min_child_samples': 48, 'subsample': 0.693371733447447, 'colsample_bytree': 0.8877745861432825, 'reg_alpha': 1.1960914929150301, 'reg_lambda': 0.7974284857133294}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:32:57,801] Trial 220 finished with value: 0.7604971177509391 and parameters: {'learning_rate': 0.0867603955358326, 'num_leaves': 93, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.7407959299680965, 'colsample_bytree': 0.8606604799512577, 'reg_alpha': 8.213249725948994, 'reg_lambda': 3.2397608813492957}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:33:02,171] Trial 221 finished with value: 0.7604488798901959 and parameters: {'learning_rate': 0.09191287934760221, 'num_leaves': 88, 'max_depth': 5, 'min_child_samples': 35, 'subsample': 0.7467106375618117, 'colsample_bytree': 0.8415088850466369, 'reg_alpha': 8.114076215273638, 'reg_lambda': 3.399753538791652}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:33:06,568] Trial 222 finished with value: 0.7604522549534778 and parameters: {'learning_rate': 0.08672886772382062, 'num_leaves': 98, 'max_depth': 5, 'min_child_samples': 37, 'subsample': 0.7604101260782322, 'colsample_bytree': 0.8522080121104316, 'reg_alpha': 8.457232208408215, 'reg_lambda': 3.031642376508546}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:33:11,100] Trial 223 finished with value: 0.7604546158338149 and parameters: {'learning_rate': 0.07611322185487275, 'num_leaves': 94, 'max_depth': 5, 'min_child_samples': 32, 'subsample': 0.734563369438732, 'colsample_bytree': 0.8741774981034145, 'reg_alpha': 8.235140424975308, 'reg_lambda': 3.1633120224557056}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:33:15,608] Trial 224 finished with value: 0.7604807968267567 and parameters: {'learning_rate': 0.06738680834122149, 'num_leaves': 109, 'max_depth': 5, 'min_child_samples': 34, 'subsample': 0.7083441458330366, 'colsample_bytree': 0.862487963607017, 'reg_alpha': 7.77417195057596, 'reg_lambda': 3.2736764317017846}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:33:20,205] Trial 225 finished with value: 0.7604955029756554 and parameters: {'learning_rate': 0.05931238263901622, 'num_leaves': 134, 'max_depth': 5, 'min_child_samples': 39, 'subsample': 0.7386151754076655, 'colsample_bytree': 0.8914324150474631, 'reg_alpha': 8.801927439074555, 'reg_lambda': 5.097202838445559}. Best is trial 145 with value: 0.7605087447829039.\n",
      "[I 2025-04-04 20:33:24,931] Trial 226 finished with value: 0.7604414249911159 and parameters: {'learning_rate': 0.058566919731129145, 'num_leaves': 131, 'max_depth': 5, 'min_child_samples': 39, 'subsample': 0.7233462994494564, 'colsample_bytree': 0.8965040724102151, 'reg_alpha': 8.037094819249123, 'reg_lambda': 5.0417018321960185}. Best is trial 145 with value: 0.7605087447829039.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stacking_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1342</td>\n",
       "      <td>0.138443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9852</td>\n",
       "      <td>0.741076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10206</td>\n",
       "      <td>0.152573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11317</td>\n",
       "      <td>0.179707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13289</td>\n",
       "      <td>0.682962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068419</th>\n",
       "      <td>11157283</td>\n",
       "      <td>0.164151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068420</th>\n",
       "      <td>11160395</td>\n",
       "      <td>0.126374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068421</th>\n",
       "      <td>11165052</td>\n",
       "      <td>0.588383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068422</th>\n",
       "      <td>11168218</td>\n",
       "      <td>0.553448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068423</th>\n",
       "      <td>11172313</td>\n",
       "      <td>0.501891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2068424 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  stacking_predict\n",
       "0            1342          0.138443\n",
       "1            9852          0.741076\n",
       "2           10206          0.152573\n",
       "3           11317          0.179707\n",
       "4           13289          0.682962\n",
       "...           ...               ...\n",
       "2068419  11157283          0.164151\n",
       "2068420  11160395          0.126374\n",
       "2068421  11165052          0.588383\n",
       "2068422  11168218          0.553448\n",
       "2068423  11172313          0.501891\n",
       "\n",
       "[2068424 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import joblib\n",
    "\n",
    "best_model = None\n",
    "best_score = -1\n",
    "\n",
    "def objective(trial):\n",
    "    global best_model, best_score\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
    "        'random_state': 39,\n",
    "    }\n",
    "    \n",
    "    # Используем 3-fold CV для оценки\n",
    "    scores = []\n",
    "    models = []\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=39)\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(meta_train):\n",
    "        X_tr, X_val = meta_train[train_idx], meta_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        models.append(model)\n",
    "        \n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        score = roc_auc_score(y_val, preds)\n",
    "        scores.append(score)\n",
    "    \n",
    "    mean_score = np.mean(scores)\n",
    "    \n",
    "    # Сохраняем лучшую модель\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        # Обучаем модель на всех данных с этими параметрами\n",
    "        best_model = lgb.LGBMClassifier(**params, n_estimators=900)\n",
    "        best_model.fit(meta_train, y_train)\n",
    "        \n",
    "        # Сохраняем модель на диск\n",
    "        joblib.dump(best_model, 'best_meta_model.pkl')\n",
    "    \n",
    "    return mean_score\n",
    "\n",
    "# Оптимизация\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, timeout=60*20)  # 20 минут\n",
    "\n",
    "# Если нужно загрузить сохраненную модель\n",
    "best_model = joblib.load('best_meta_model.pkl')\n",
    "\n",
    "# Используем сохраненную лучшую модель\n",
    "test_df_pd['stacking_predict'] = best_model.predict_proba(meta_test)[:, 1]\n",
    "result = test_df_pd[['user_id', 'stacking_predict']]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f8fe256",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_pd[['user_id', 'stacking_predict']].to_csv('stacking_opt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b0aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799eac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd442fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84bb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01caf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158393f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde93000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0356128b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd936fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de560b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887c896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a460c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e32b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
      "[LightGBM] [Info] Number of positive: 647575, number of negative: 1227381\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1785\n",
      "[LightGBM] [Info] Number of data points in the train set: 1874956, number of used features: 7\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345381 -> initscore=-0.639403\n",
      "[LightGBM] [Info] Start training from score -0.639403\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.16636209, 0.76811173, 0.20386324, ..., 0.58945946, 0.53982926,\n",
       "       0.50582742])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Параметры для LightGBM (упрощенные, без early stopping)\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 120,\n",
    "    'num_leaves': 75,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'n_estimators': 900,\n",
    "    'random_state': 42,\n",
    "    'verbose': 1,\n",
    "    'random_state':39\n",
    "}\n",
    "\n",
    "# Создаем и обучаем мета-модель на всех данных\n",
    "meta_model = lgb.LGBMClassifier(**lgb_params)\n",
    "meta_model.fit(meta_train, y_train)  # Просто fit без валидации\n",
    "\n",
    "test_df_pd['stacking_predict'] = meta_model.predict_proba(meta_test)[:, 1]\n",
    "test_df_pd[['user_id', 'stacking_predict']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8f12074",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_pd[['user_id', 'stacking_predict']].to_csv('stacking.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e161fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696220a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90cc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be10bdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e7dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63e4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04021779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7acfc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b4c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training lgbm_native...\n",
      "lgbm_native trained successfully with 3 base models!\n",
      "\n",
      "Training xgb_sklearn...\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[18:05:18] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 94\u001b[0m\n\u001b[0;32m     89\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_tr, y_tr, \n\u001b[0;32m     90\u001b[0m              eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)], \n\u001b[0;32m     91\u001b[0m              eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     92\u001b[0m              callbacks\u001b[38;5;241m=\u001b[39m[lgb\u001b[38;5;241m.\u001b[39mlog_evaluation(period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)])  \u001b[38;5;66;03m# Отключаем вывод\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb_native\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb_sklearn\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 94\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_tr, y_tr, \n\u001b[0;32m     95\u001b[0m              eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)],\n\u001b[0;32m     96\u001b[0m              verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Используем verbose=False для XGBoost\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_tr, y_tr)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1580\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1577\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[0;32m   1579\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m-> 1580\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1581\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1582\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   1583\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   1584\u001b[0m     group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1585\u001b[0m     qid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1586\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1587\u001b[0m     base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[0;32m   1588\u001b[0m     feature_weights\u001b[38;5;241m=\u001b[39mfeature_weights,\n\u001b[0;32m   1589\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39meval_set,\n\u001b[0;32m   1590\u001b[0m     sample_weight_eval_set\u001b[38;5;241m=\u001b[39msample_weight_eval_set,\n\u001b[0;32m   1591\u001b[0m     base_margin_eval_set\u001b[38;5;241m=\u001b[39mbase_margin_eval_set,\n\u001b[0;32m   1592\u001b[0m     eval_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1593\u001b[0m     eval_qid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1594\u001b[0m     create_dmatrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dmatrix,\n\u001b[0;32m   1595\u001b[0m     enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_categorical,\n\u001b[0;32m   1596\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1597\u001b[0m )\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1600\u001b[0m     params,\n\u001b[0;32m   1601\u001b[0m     train_dmatrix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1610\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1611\u001b[0m )\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:603\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[0;32m    584\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m    585\u001b[0m     X: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    599\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    600\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m    601\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m create_dmatrix(\n\u001b[0;32m    604\u001b[0m         data\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    605\u001b[0m         label\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    606\u001b[0m         group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[0;32m    607\u001b[0m         qid\u001b[38;5;241m=\u001b[39mqid,\n\u001b[0;32m    608\u001b[0m         weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    609\u001b[0m         base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[0;32m    610\u001b[0m         feature_weights\u001b[38;5;241m=\u001b[39mfeature_weights,\n\u001b[0;32m    611\u001b[0m         missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    612\u001b[0m         enable_categorical\u001b[38;5;241m=\u001b[39menable_categorical,\n\u001b[0;32m    613\u001b[0m         feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[0;32m    614\u001b[0m         ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    615\u001b[0m     )\n\u001b[0;32m    617\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1065\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1065\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m QuantileDMatrix(\n\u001b[0;32m   1066\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, ref\u001b[38;5;241m=\u001b[39mref, nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[0;32m   1067\u001b[0m         )\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:1573\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1554\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m         )\n\u001b[0;32m   1567\u001b[0m     ):\n\u001b[0;32m   1568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1571\u001b[0m         )\n\u001b[1;32m-> 1573\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(\n\u001b[0;32m   1574\u001b[0m     data,\n\u001b[0;32m   1575\u001b[0m     ref\u001b[38;5;241m=\u001b[39mref,\n\u001b[0;32m   1576\u001b[0m     label\u001b[38;5;241m=\u001b[39mlabel,\n\u001b[0;32m   1577\u001b[0m     weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[0;32m   1578\u001b[0m     base_margin\u001b[38;5;241m=\u001b[39mbase_margin,\n\u001b[0;32m   1579\u001b[0m     group\u001b[38;5;241m=\u001b[39mgroup,\n\u001b[0;32m   1580\u001b[0m     qid\u001b[38;5;241m=\u001b[39mqid,\n\u001b[0;32m   1581\u001b[0m     label_lower_bound\u001b[38;5;241m=\u001b[39mlabel_lower_bound,\n\u001b[0;32m   1582\u001b[0m     label_upper_bound\u001b[38;5;241m=\u001b[39mlabel_upper_bound,\n\u001b[0;32m   1583\u001b[0m     feature_weights\u001b[38;5;241m=\u001b[39mfeature_weights,\n\u001b[0;32m   1584\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m   1585\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[0;32m   1586\u001b[0m     enable_categorical\u001b[38;5;241m=\u001b[39menable_categorical,\n\u001b[0;32m   1587\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:1634\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[1;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[0;32m   1632\u001b[0m it\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[1;32m-> 1634\u001b[0m _check_call(ret)\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m handle\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:284\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [18:05:18] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\common\\../data/gradient_index.h:94: Check failed: valid: Input data contains `inf` or a value too large, while `missing` is not set to `inf`"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Функция для обработки пропусков\n",
    "def handle_missing_values(df):\n",
    "    # Создаем копию DataFrame\n",
    "    processed = df.copy()\n",
    "    \n",
    "    # Определяем группы столбцов\n",
    "    columns = processed.columns\n",
    "    zero_fill_cols = [col for col in columns if col.startswith(('num_', 'sum_', 'max_', 'days_', 'log_', 'avg_', 'main_', 'search_', 'product_', 'recent_', 'knn'))]\n",
    "    zero_fill_cols_end = [col for col in columns if col.endswith(('_30d'))]\n",
    "    time_fill_cols = [col for col in columns if col.endswith('_time')]\n",
    "    \n",
    "    # Заполняем пропуски\n",
    "    processed[zero_fill_cols] = processed[zero_fill_cols].fillna(0)\n",
    "    processed[zero_fill_cols_end] = processed[zero_fill_cols_end].fillna(0)\n",
    "    # Проверяем, есть ли временные столбцы перед заполнением\n",
    "    if time_fill_cols:\n",
    "        processed[time_fill_cols] = processed[time_fill_cols].fillna(pd.Timestamp('2024-01-01 00:00:00'))\n",
    "\n",
    "    # Для остальных числовых признаков\n",
    "    numeric_cols = processed.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col not in zero_fill_cols and col not in zero_fill_cols_end and col not in time_fill_cols:\n",
    "            processed[col] = processed[col].fillna(-999)\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# Обработка данных (предполагаем, что df_pd и test_df_pd уже определены)\n",
    "df_processed = handle_missing_values(df_pd)\n",
    "test_processed = handle_missing_values(test_df_pd)\n",
    "\n",
    "# Выбор фичей (исключаем user_id и target)\n",
    "cols_to_exclude = {'user_id', 'target', 'last_click_time', 'first_click_time', 'last_favorite_time', \n",
    "                   'first_favorite_time', 'last_order_time', 'first_order_time', 'last_to_cart_time', \n",
    "                   'first_to_cart_time', 'last_search_time', 'first_search_time', 'top3_search_clusters', \n",
    "                   'top3_search_counts', 'search_cluster_entropy', 'top3_product_counts', \n",
    "                   'product_cluster_entropy', 'top3_product_clusters'}\n",
    "\n",
    "cols_2 = [col for col in df_pd.columns if col not in cols_to_exclude]\n",
    "\n",
    "# Подготовка данных\n",
    "X_train_2 = df_processed[cols_2].values\n",
    "y_train_2 = df_processed['target'].values\n",
    "X_test_2 = test_processed[cols_2].values\n",
    "\n",
    "# Базовые модели с индивидуальными параметрами\n",
    "base_models = {\n",
    "    \"lgbm_native\": LGBMClassifier(\n",
    "        n_estimators=150, learning_rate=0.05, max_depth=5, random_state=42,\n",
    "        verbose=-1  # Используем -1 для silence вместо параметра в fit\n",
    "    ),\n",
    "    \"xgb_sklearn\": XGBClassifier(\n",
    "        n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42,\n",
    "        tree_method='hist', enable_categorical=True, missing=-999,\n",
    "        verbosity=0  # Используем verbosity вместо verbose\n",
    "    ),\n",
    "    \"xgb_native\": XGBClassifier(\n",
    "        n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42,\n",
    "        tree_method='hist', enable_categorical=True, missing=-999,\n",
    "        verbosity=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Подготовка OOF-прогнозов\n",
    "n_models_2 = len(base_models)\n",
    "n_train_2 = len(X_train_2)\n",
    "n_test_2 = len(X_test_2)\n",
    "\n",
    "meta_train_2 = np.zeros((n_train_2, n_models_2))\n",
    "meta_test_2 = np.zeros((n_test_2, n_models_2))\n",
    "\n",
    "kf_2 = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (name, model) in enumerate(base_models.items()):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_2.split(X_train_2, y_train_2)):\n",
    "        X_tr, X_val = X_train_2[train_idx], X_train_2[val_idx]\n",
    "        y_tr, y_val = y_train_2[train_idx], y_train_2[val_idx]\n",
    "        \n",
    "        if name == 'lgbm_native':\n",
    "            model.fit(X_tr, y_tr, \n",
    "                     eval_set=[(X_val, y_val)], \n",
    "                     eval_metric='auc',\n",
    "                     callbacks=[lgb.log_evaluation(period=0)])  # Отключаем вывод\n",
    "        elif name in ['xgb_native', 'xgb_sklearn']:\n",
    "            model.fit(X_tr, y_tr, \n",
    "                     eval_set=[(X_val, y_val)],\n",
    "                     verbose=False)  # Используем verbose=False для XGBoost\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "        \n",
    "        meta_train_2[val_idx, i] = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Делаем предсказания для тестовых данных\n",
    "    meta_test_2[:, i] = model.predict_proba(X_test_2)[:, 1]\n",
    "    print(f\"{name} trained successfully with {n_models_2} base models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# 1. Полностью очищаем данные от временных столбцов и нечисловых типов\n",
    "cols = list(set(df_pd.columns) - {'user_id', 'target', 'last_click_time', 'first_click_time', 'last_favorite_time', 'first_favorite_time', \n",
    "                                  'last_order_time', 'first_order_time', 'last_to_cart_time', 'first_to_cart_time', 'last_search_time', 'first_search_time',\n",
    "                                  'top3_search_clusters', 'top3_search_counts', 'search_cluster_entropy', 'top3_product_counts', 'product_cluster_entropy', 'top3_product_clusters'})\n",
    "\n",
    "# 2. Явно преобразуем все данные в float и заменяем оставшиеся NaT/NaN\n",
    "X_train = df_pd[cols].astype(float).fillna(-999).values\n",
    "y_train = df_pd['target'].values\n",
    "\n",
    "X_test = test_df_pd[cols].astype(float).fillna(-999).values\n",
    "\n",
    "# Базовые модели с поддержкой NaN\n",
    "base_models = {\n",
    "    \"lgbm_sklearn\": LGBMClassifier(\n",
    "        n_estimators=150, learning_rate=0.05, max_depth=5, random_state=42\n",
    "    ),\n",
    "    \"catboost\": CatBoostClassifier(\n",
    "        iterations=150, learning_rate=0.05, depth=5, random_state=42,\n",
    "        verbose=0, allow_writing_files=False\n",
    "    )\n",
    "}   \n",
    "\n",
    "# Подготовка OOF-прогнозов\n",
    "n_models = len(base_models)\n",
    "n_train = len(X_train)\n",
    "n_test = len(X_test)\n",
    "\n",
    "meta_train = np.zeros((n_train, n_models))\n",
    "meta_test = np.zeros((n_test, n_models))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (name, model) in enumerate(base_models.items()):\n",
    "    print(f\"Training {name}...\")\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        try:\n",
    "            if name == 'lgbm_native':\n",
    "                model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=0)\n",
    "            elif name == 'xgb_native':\n",
    "                model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=0)\n",
    "            elif name == 'catboost':\n",
    "                model.fit(X_tr, y_tr, eval_set=(X_val, y_val), verbose=0)\n",
    "            else:\n",
    "                model.fit(X_tr, y_tr)\n",
    "            \n",
    "            meta_train[val_idx, i] = model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {name}: {str(e)}\")\n",
    "            meta_train[val_idx, i] = 0.5\n",
    "    \n",
    "    try:\n",
    "        meta_test[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {name} predict: {str(e)}\")\n",
    "        meta_test[:, i] = 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
