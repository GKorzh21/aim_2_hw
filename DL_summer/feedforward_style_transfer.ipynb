{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c11ce48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from PIL import Image\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "543d5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_img(content_img_path, style_img_path, target_size=(512, 512), return_tensor = True):\n",
    "    \"\"\"\n",
    "    Загружает и подготавливает изображения\n",
    "    \n",
    "    Параметры:\n",
    "        content_img_path: путь к контентному изображению\n",
    "        style_img_path: путь к изображению стиля\n",
    "        target_size: желаемый размер (ширина, высота)\n",
    "        return_tensor: если True - возвращает тензоры, если False - PIL Images\n",
    "    \n",
    "    Возвращает:\n",
    "        кортеж (content_image, style_image)\n",
    "    \"\"\"\n",
    "    \n",
    "    common_transforms = [\n",
    "        transforms.Resize(target_size)\n",
    "    ]\n",
    "    \n",
    "    if return_tensor:\n",
    "        tensor_transforms = [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "        transform = transforms.Compose(common_transforms + tensor_transforms)\n",
    "    else:\n",
    "        transform = transforms.Compose(common_transforms)\n",
    "\n",
    "    def load_img(img_path):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        transformed = transform(img)\n",
    "        return transformed.unsqueeze(0) if return_tensor else transformed\n",
    "        \n",
    "    content_image = load_img(content_img_path)\n",
    "    style_image = load_img(style_img_path)\n",
    "    \n",
    "    return content_image, style_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "46e4281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont, style = load_and_prepare_img(\n",
    "    \"C:\\\\Users\\\\Admin\\\\Desktop\\\\AIM\\\\aim_2_hw\\\\DL_summer\\\\feedforward_data\\\\cat.jpg\",\n",
    "    \"C:\\\\Users\\\\Admin\\\\Desktop\\\\AIM\\\\aim_2_hw\\\\DL_summer\\\\feedforward_data\\\\fff.jpg\",\n",
    "    (800,680),\n",
    "    True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "0561a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = cont.to(device)\n",
    "style = style.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ec70c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg19\n",
    "model = vgg19(pretrained=True).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "6f6d5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, features, layers_list):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.layers = layers_list\n",
    "        self.layer_map = {\n",
    "            1: 'relu1_1',\n",
    "            6: 'relu2_1',\n",
    "            11: 'relu3_1',\n",
    "            20: 'relu4_1',\n",
    "            22: 'relu4_2',\n",
    "            29: 'relu5_1'\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        features_dict = {}\n",
    "        for i, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            if i in self.layers:\n",
    "                name = self.layer_map[i]\n",
    "                features_dict[name] = x\n",
    "\n",
    "        return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7c5f8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = FeatureExtractor(model.features, [1,6,11,20,22,29])\n",
    "content_features = extractor(cont)\n",
    "style_features = extractor(style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "af2a0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    B, C, H, W = tensor.size()\n",
    "    features = tensor.view(B, C, H*W)\n",
    "    G = torch.bmm(features, features.transpose(1, 2))\n",
    "    return G / (C*H*W)\n",
    "\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "f33c0b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Step 1\n",
      "Step 2\n",
      "Step 3\n",
      "Step 4\n",
      "Step 5\n",
      "Step 6\n",
      "Step 7\n",
      "Step 8\n",
      "Step 9\n",
      "Step 10\n",
      "Step 11\n",
      "Step 12\n",
      "Step 13\n",
      "Step 14\n",
      "Step 15\n",
      "Step 16\n",
      "Step 17\n",
      "Step 18\n",
      "Step 19\n",
      "Step 20\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 24\n",
      "Step 25\n",
      "Step 26\n",
      "Step 27\n",
      "Step 28\n",
      "Step 29\n",
      "Step 30\n",
      "Step 31\n",
      "Step 32\n",
      "Step 33\n",
      "Step 34\n",
      "Step 35\n",
      "Step 36\n",
      "Step 37\n",
      "Step 38\n",
      "Step 39\n",
      "Step 40\n",
      "Step 41\n",
      "Step 42\n",
      "Step 43\n",
      "Step 44\n",
      "Step 45\n",
      "Step 46\n",
      "Step 47\n",
      "Step 48\n",
      "Step 49\n"
     ]
    }
   ],
   "source": [
    "target = cont.clone().requires_grad_(True).to(device)\n",
    "optimizer = torch.optim.LBFGS([target])\n",
    "style_layers = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1']\n",
    "\n",
    "content_weight = 1\n",
    "style_weight = 1e4\n",
    "\n",
    "for step in range(50):\n",
    "    print(f\"Step {step}\")\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        target_features = extractor(target)\n",
    "\n",
    "        content_loss = mse(target_features['relu4_2'], content_features['relu4_2'])\n",
    "\n",
    "        style_loss = 0\n",
    "        for layer in style_layers:\n",
    "            G_t = gram_matrix(target_features[layer])\n",
    "            G_s = style_grams[layer]\n",
    "            style_loss += mse(G_t, G_s)\n",
    "\n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        total_loss.backward()\n",
    "        return total_loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2d6ef7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "# Обратная нормализация\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).to(device).view(1,3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).to(device).view(1,3,1,1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "result = denormalize(target.clone().detach())\n",
    "save_image(result, \"cat_fff.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ee0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
