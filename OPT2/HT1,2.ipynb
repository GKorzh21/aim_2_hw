{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAGYpWSWbCZq"
      },
      "source": [
        "# 1.Multi Armed Bandits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7rJmBtCeU9D"
      },
      "source": [
        "### 1.1 Теоретическая задача\n",
        "\n",
        "Epsilon-greedy algorithm with exploration probabilities $\\varepsilon = t^{-1/3} \\cdot (K \\log t)^{1/3}$\n",
        "\n",
        "achieves regret bound $\\mathbb{E} R(t) \\leq t^{2/3} \\cdot O(K \\log t)^{1/3}$ for each round $t$.\n",
        "\n",
        "[см.](https://arxiv.org/pdf/1904.07272) Theorem 1.6 & Algorithm 1.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Регрет растет не только тогда, когда мы выбираем при исследовании рандомную ручку, но и еще если при использовании мы дергаем не лучшую ручку в силу недостаточной уверенности в ней(те мы недостаточно много исследовали). Тогда ожидаемое количество исследовательных шагов: \n",
        "\n",
        "$$E(N_{explore}(t)) = \\sum_{s = 1}^{t} \\varepsilon = \\int_1^t t^{-1/3} \\cdot (K \\log t)^{1/3} dt = (*)$$\n",
        "\n",
        "Первый множетель интегрируется сразу, а второй(логарифм) растет достаточно медленно, так что его можно вынести в приближение, туда же константа 3/2 от первого множетеля. \n",
        "\n",
        "$$ (*) \\approx (K \\log t)^{1/3} \\cdot \\int_1^t t^{-1/3} dt \\approx O(t^{2/3} \\cdot (K \\log t)^{1/3})$$\n",
        "\n",
        "То есть среднее число исследовательских шагов можно оценить как $E(N_{explore}) \\approx O(t^{2/3} \\cdot (K \\log t)^{1/3})$, при этом зазор на каждом из таких шагов тоже можно оценить средним и средний зазор не зависит от t, потому что это среднее арифметическое зазоров на каждой ручке, а они просто разность выигрыша от данной ручки и лучшей. \n",
        "\n",
        "Так что $E(R_{explore}(t)) = \\bar{\\Delta} E(N_{explore}(t)) = O(1) О(\\cdot t^{2/3} \\cdot (K \\log t)^{1/3}) = О(t^{2/3} \\cdot (K \\log t)^{1/3})$\n",
        "\n",
        "$E(R_{exploit}(t)) << E(R_{explore}(t))$ так что можно пренебреч.\n",
        "\n",
        "Итого $E(R(t)) = E(R_{explore}(t)) + E(R_{exploit}(t)) \\approx E(R_{explore}(t)) \\approx О(t^{2/3} \\cdot (K \\log t)^{1/3})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6iCw14gbkKS"
      },
      "source": [
        "### 1.2 Теоретическая задача\n",
        "\n",
        "Take UCB bandit algorithm $A$ for fixed time horizon $T$ . Convert it to an algorithm $A_{\\infty}$ which runs forever, in phases $i = 1, 2, 3, ...$ of $2^i$ rounds each.\n",
        "\n",
        "In each phase $i$ algorithm $A$ is restarted and run with time horizon $2^i$.\n",
        "\n",
        "State and prove a theorem which converts an instance-independent upper bound on regret for $A$ into similar bound for $A_{\\infty}$.\n",
        "\n",
        "[см.](https://arxiv.org/pdf/1904.07272) Chapter 1 & Algorithm 1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh36qcQ8tina"
      },
      "source": [
        "Нам предлагают рассмотреть UCB, но не с фиксированым горизонтом действий, а с бесконечным, но разбитым на эпохи длиной $2^i$. Для UCB с фиксированым горизонтом сожаление $R(T)$ ограничено сверху некоторой функией от Т, те $R(T) \\leq f(T)$. Тогда если рассмотреть эпоху $T = 2^i$, то сожаление на ней будет ограничено вот так: $R(T) \\leq f(2^i)$. \n",
        "\n",
        "Тогда общее сожаление $R_{\\infty} \\leq \\sum_{i = 1}^{\\infty} f(2^i)$\n",
        "\n",
        "Рассмотрим ограничение на регрет, которое фигурировало в статье: $R(T) \\leq O(T^{2/3} (logT)^{1/3})$\n",
        "\n",
        "Для $T = 2^i$ получаем: $R(2^i) \\leq O(2^{2i/3} \\cdot i^{1/3}) = O( (4^{i} \\cdot i)^{1/3})$\n",
        "\n",
        "$R(A_{\\infty}) = \\sum_{i = 1}^{\\infty} R(2^i) = \\sum_{i = 1}^{\\infty} O(4^{i/3} \\cdot i^{1/3})$\n",
        "\n",
        "Воспользуемся признаком Даламбера: \n",
        "$$ \\lim_{i \\to \\infty} \\frac{a_{i+1}}{a_i} = \\lim_{i \\to \\infty} \\frac{4^{(i+1)/3} \\cdot (i+1)^{1/3}}{4^{i/3} \\cdot i^{1/3}} = \\lim_{i \\to \\infty} 4^{1/3} \\cdot \\left(\\frac{i+1}{i}\\right)^{1/3} = 4^{1/3} \\cdot 1 > 1$$\n",
        "Значит сумма расходится "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_F3AhG9gGUM"
      },
      "source": [
        "### 1.3 Практическая задача\n",
        "\n",
        "На первом семинаре разбирали метод $\\varepsilon$ - greedy.\n",
        "\n",
        "В этом задании предстоит дополнить MAB методом UCB в рамках того же класса MAB, что и рассматривали в семинаре.\n",
        "\n",
        "Для определенности алгоритм можно посмотреть здесь **[см.](https://arxiv.org/pdf/1904.07272) Chapter 1 & Algorithm 1.5**\n",
        "\n",
        "$UCB_i(t) = \\sqrt{\\frac{\\delta \\ln (T)}{t_i}}$\n",
        "\n",
        "$\\delta$ - параметр исследования, подберите самостоятельно\n",
        "\n",
        "$T$ - временной горизонт\n",
        "\n",
        "$t_i$ - сколько раз дергали ручку $i$ до шага $t$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7kJaXehgKEp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "В ЭТОЙ ЯЧЕЙКЕ НИЧЕГО МЕНЯТЬ НЕ НУЖНО\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class MAB(ABC):\n",
        "    \"\"\"Base class for multi-armed bandit (MAB)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_arms : int\n",
        "        Number of arms.\n",
        "    \"\"\"\n",
        "    # initialise\n",
        "    def __init__(self, n_arms):\n",
        "        self.n_arms = n_arms\n",
        "\n",
        "    @abstractmethod\n",
        "    def play(self):\n",
        "        \"\"\"Play a round\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        arm : int\n",
        "            Integer index of the arm played this round. Should be in the set\n",
        "            {0, ..., n_arms - 1}.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def update(self, arm, reward):\n",
        "        \"\"\"Update the internal state of the MAB after a play\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        arm : int\n",
        "            Integer index of the played arm in the set {0, ..., n_arms - 1}.\n",
        "\n",
        "        reward : float\n",
        "            Reward received from the arm.\n",
        "\n",
        "        \"\"\"\n",
        "        self.arm = arm\n",
        "        self.reward = reward\n",
        "\n",
        "\n",
        "def offlineEvaluate(mab, true_bandit_probs, n_arms, n_rounds= 100000):\n",
        "    \"\"\"Offline evaluation of a multi-armed bandit\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mab : instance of MAB\n",
        "        MAB to evaluate.\n",
        "\n",
        "    n_arms : int\n",
        "        Number of arms.\n",
        "\n",
        "    true_bandit_probs : float numpy.ndarray, shape (n_events,)\n",
        "        Array containing the history of rewards.\n",
        "\n",
        "\n",
        "    n_rounds : int, default=None\n",
        "        Number of matching events to evaluate the MAB on.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    R : float numpy.ndarray\n",
        "        Rewards for the matching events.\n",
        "\n",
        "    H : int numpy.ndarray\n",
        "        Historical events\n",
        "    \"\"\"\n",
        "\n",
        "    R = []          # save the total payoff\n",
        "    H = np.zeros((n_rounds, n_arms)) # save used historical events\n",
        "\n",
        "    for i in range(n_rounds):\n",
        "        arm = mab.play()\n",
        "        reward = np.random.rand() < true_bandit_probs[arm]\n",
        "\n",
        "        R.append(reward)  # append the new rewards\n",
        "        H[i, arm] = 1     # append the used events\n",
        "\n",
        "        mab.update(arm, reward) # update the information\n",
        "\n",
        "    return np.array(R), np.array(H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dVHSJtFgkzA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "реализуем класс UCB по аналогии с EpsGreedy(MAB) первого семинара\n",
        "\n",
        "необходимые каунтеры можете использовать такие же как в EpsGreedy или завести свои\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class UCB(MAB):\n",
        "    \"\"\"UCB multi-armed bandit\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    \"\"\"\n",
        "    # initialise values\n",
        "    def __init__(self, n_arms, param):\n",
        "        super().__init__(n_arms)\n",
        "        self.param = param\n",
        "        ...\n",
        "\n",
        "\n",
        "    # select a random arm to explore or a arm with best rewards to exploit, then return the arm\n",
        "    def play(self): #, context=None):\n",
        "        super().play()\n",
        "        ...\n",
        "\n",
        "\n",
        "    # update values\n",
        "    def update(self, arm, reward):\n",
        "        super().update(arm, reward)\n",
        "\n",
        "        ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aalbshcg7Hz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Заведем бандита с 4 ручками и распределением выигрышей ручек [0.45, 0.35, 0.85, 0.62]\n",
        "\n",
        "На временном горизонте 100000\n",
        "\n",
        "Подбираем параметр исследования (delta), который согласуется с теоретическим в оценке регрета\n",
        "\n",
        "\"\"\"\n",
        "n_arms =\n",
        "n_iterations =\n",
        "true_bandit_probs =\n",
        "param =\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1oHX617mBCQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "В ЭТОЙ ЯЧЕЙКЕ НИЧЕГО МЕНЯТЬ НЕ НУЖНО\n",
        "\n",
        "запускаем процесс\n",
        "\n",
        "\"\"\"\n",
        "mab = UCB(n_arms, param)\n",
        "results_UCB, history_UCB = offlineEvaluate(mab, true_bandit_probs, n_arms, n_iterations)\n",
        "print('UCB average reward', np.mean(results_UCB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSh1BPsCmuMn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "В ЭТОЙ ЯЧЕЙКЕ НИЧЕГО МЕНЯТЬ НЕ НУЖНО\n",
        "\n",
        "посмотрим на динамику выбора ручек\n",
        "\"\"\"\n",
        "\n",
        "selections_percentage = np.cumsum(history_UCB, axis=0) / np.arange(1, n_iterations + 1).reshape(-1, 1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "for arm in range(n_arms):\n",
        "    plt.plot(selections_percentage[:, arm], label=f'Bandit #{arm+1}')\n",
        "plt.xscale('log')\n",
        "plt.title('Bandit Action Choices Over Time')\n",
        "plt.xlabel('Episode Number')\n",
        "plt.ylabel('Percentage of Bandit Selections (%)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "for i, prob in enumerate(true_bandit_probs, 1):\n",
        "    print(f\"Bandit #{i} -> {prob:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-8zOMXJZtsh"
      },
      "source": [
        "# 2.Expert Advice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpAYSxp0PGv1"
      },
      "source": [
        "###2.1 Теоретическая задача\n",
        "\n",
        "Sequences of outcomes on which many experts suffer a small loss are\n",
        "intuitively easier to predict. Show that the **exponentially weighted forecaster** satistifies the following property: for every $n$, for every outcome sequence $y_n$ , and for all $L > 0$,\n",
        "$$\\hat L_n ≤ L + \\frac{1}{\\mu}\\ln\\frac{N}{N_L} + \\frac{\\mu}{8}n$$\n",
        "where $N_L$ is the cardinality of the set $\\{1 \\leq i \\leq N : L_{i,n} \\leq L\\}$\n",
        "### --------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "Exponentially Weighted Forecaster based on the\n",
        "potential $\\Phi_{\\mu}(\\mathbf{u})= \\frac{1}{\\mu} \\ln(\\sum_{i=1}^N e^{\\mu u_i}) $ where $\\mu>0$.\n",
        "\n",
        "$N$ - number of experts.\n",
        "\n",
        "$n$ - number of times.\n",
        "\n",
        "$L_{i, n} = \\sum_{t=1}^n\\ell(f_{i, t}, y_t)$ - cumulative loss of expert $i$\n",
        "\n",
        " $\\hat L_{n} = \\sum_{t=1}^n \\ell(\\hat p_t, y_t)$ - cumulative loss of forecaster.\n",
        "\n",
        "**[см ](https://ece.iisc.ac.in/~aditya/Prediction_Learning_and_Games.pdf)Corollary 2.2.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C52MgpYGtkrC"
      },
      "source": [
        "###.\n",
        "###.\n",
        "###.\n",
        "Ячейка для ответа на задачу\n",
        "###.\n",
        "###.\n",
        "###."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bek_wJ_FcJQN"
      },
      "source": [
        "### 2.2 Практическая задача\n",
        "\n",
        "В этом задании предстоит реализовать exponentially weighted forecaster  по аналогии с polynomially-weighted forecaster второго семинара. И сравнить теоретический и реализованный регреты.\n",
        "\n",
        "Нужно определить потенциальную функцию и лосс, подсчитать реализованный и теоретический регреты.\n",
        "\n",
        "**param** выступает в роли параметра метода. Для polynomially-weighted forecaster параметр отвечал за значение степени нормы $p$. Для exponentially weighted forecaster параметр отвечает за $\\mu$.\n",
        "\n",
        "Параметр $\\mu$ подберите самостоятельно.\n",
        "\n",
        "В res функции **step** записываются необходимые результаты для подсчета реализованного регрета\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEwjW6TPYRzH"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "В ЭТОЙ ЯЧЕЙКЕ НИЧЕГО МЕНЯТЬ НЕ НУЖНО\n",
        "\n",
        "\"\"\"\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "@partial(jax.vmap, in_axes=(0, 0, None), out_axes=-1)\n",
        "def _create_mistakes(key, p, T):\n",
        "    mistakes = jax.random.bernoulli(key, p=p, shape=(T,))\n",
        "    return mistakes\n",
        "\n",
        "\n",
        "def bern_oracle_beta_forecasters(key, n_experts, n_timesteps, a_fcst=2, b_fsct=7, p_oracle=0.5):\n",
        "    \"\"\"\n",
        "    Create a set of forecasters whose probability of being correct is\n",
        "    distributed according to a Beta(a_fcst, b_fcst) distribution.\n",
        "    The oracle is a Bernoulli random variable with p=p_oracle.\n",
        "    \"\"\"\n",
        "    key_oracle, key_noise, key_errs = jax.random.split(key, 3)\n",
        "    keys_noise = jax.random.split(key_noise, n_experts)\n",
        "\n",
        "    oracle = jax.random.bernoulli(key_oracle, p=p_oracle, shape=(n_timesteps,))\n",
        "    ps_mistakes = jax.random.beta(key_errs, a=a_fcst, b=b_fsct, shape=(n_experts,))\n",
        "\n",
        "    mistakes = _create_mistakes(keys_noise, ps_mistakes, n_timesteps)\n",
        "    experts = (mistakes ^ oracle[:, None]).astype(float)\n",
        "\n",
        "    return oracle, experts\n",
        "\n",
        "\n",
        "\n",
        "def forecast_step(experts_curr, regret_prev, param):\n",
        "    weights = gradp(regret_prev, param)\n",
        "    forecast = weights @ experts_curr / weights.sum()\n",
        "    return weights, forecast\n",
        "\n",
        "\n",
        "def step(regret, xs, param):\n",
        "    \"\"\"\n",
        "    At time t, we make a prediction based on the\n",
        "    regret (as defined by the potential) obtained at time\n",
        "    t-1 and the prediction of each forecaster at time t.\n",
        "\n",
        "    After making a new prediction, we update the regret.\n",
        "    \"\"\"\n",
        "    experts, oracle = xs\n",
        "    weights, forecast = forecast_step(experts, regret, param)\n",
        "\n",
        "    loss_forecaster = loss(forecast, oracle)\n",
        "    loss_experts = loss(experts, oracle)\n",
        "\n",
        "    iregret_experts = loss_forecaster - loss_experts\n",
        "\n",
        "    regret_new = regret + iregret_experts\n",
        "\n",
        "    res = {\n",
        "        \"weights\": weights,\n",
        "        \"forecast\": forecast,\n",
        "        \"step-loss-experts\": loss_experts,\n",
        "        \"step-loss-forecaster\": loss_forecaster,\n",
        "    }\n",
        "\n",
        "\n",
        "    return regret_new, res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC3earM9XPkY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Определим лосс и потенциал для экспотенциального взвешивания\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def loss(yhat, y):\n",
        "    ...\n",
        "\n",
        "def potential(x, param):\n",
        "   ...\n",
        "\n",
        "gradp = jax.grad(potential) #функция градиента\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUZb32YyXxjt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "генерим ответы природы (oracle) ~ Bernoulli(0.7) и ошибки 10 экспертов (experts) ~  Beta(3,3)\n",
        "c помощью bern_oracle_beta_forecasters()\n",
        "\n",
        "подбираем временной горизонт\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "key = jax.random.PRNGKey(314)\n",
        "\n",
        "param = ...\n",
        "\n",
        "n_experts = ...\n",
        "n_timesteps = ...\n",
        "a, b =  ... #параметры распределения Beta\n",
        "p_oracle = ... #параметры распределения  Bernoulli\n",
        "\n",
        "\n",
        "oracle, experts = bern_oracle_beta_forecasters(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wFmBCgRX39l"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "В ЭТОЙ ЯЧЕЙКЕ НИЧЕГО МЕНЯТЬ НЕ НУЖНО\n",
        "\n",
        "запускаем процесс\n",
        "\"\"\"\n",
        "\n",
        "regret = jnp.ones(n_experts) #инициализация весов(соответственно регрета) экспертов\n",
        "\n",
        "xs = (experts, oracle)\n",
        "\n",
        "part_step = partial(step, param=param)\n",
        "_, res = jax.lax.scan(part_step, regret, xs) #скользим по xs и получаем res для каждого шага (количество шагов - длина oracle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X73dQNFgX8v9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "построим  график реализованного и теоретического регретов\n",
        "\n",
        "необходимые значения можно получить из результатов res\n",
        "\"\"\"\n",
        "regret_forecaster = ...\n",
        "regret_ubound = ...\n",
        "\n",
        "plt.plot(regret_forecaster, label=\"Regret\")\n",
        "plt.plot(regret_ubound, label=\"Upper bound regret\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
